{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:03:07.545163Z",
     "start_time": "2025-10-22T02:02:56.605062Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 54380 tracks, 11107 albums, 3517 artists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIdxJREFUeJzt3XtwVOXh//HP5ioCm4WQZJMSMAy3hAIFUmK4TDuS4eagIiXCRGEsDS0lXkA6kKkItB3ipXSmWCWNWuAPEcGKhYyAgWhACAECVLk0hggmmNtozG4CEgJ7fn/wc+sK+mUhIU/S92vmzJhznj37nGcy7nvO7gabZVmWAAAADBLQ1hMAAAD4LgIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHGC2noCN8Pj8aiyslJdu3aVzWZr6+kAAIAbYFmWGhoaFBMTo4CAH75H0i4DpbKyUrGxsW09DQAAcBMqKirUs2fPHxzTLgOla9eukq5eoN1ub+PZAACAG+F2uxUbG+t9Hf8h7TJQvnlbx263EygAALQzN/LxDD4kCwAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADj+B0on3/+uR5++GGFh4erU6dOGjx4sA4fPixJam5u1uLFizV48GB17txZMTExmjVrliorK33OUVdXp7S0NNntdjkcDs2ZM0eNjY0tc0UAAKDd8ytQvvrqK40ePVrBwcHavn27Tp48qVWrVqlbt26SpAsXLujIkSNaunSpjhw5orffflslJSW67777fM6TlpamEydOKC8vT7m5udqzZ4/mzp3bclcFAADaNZtlWdaNDl6yZIn27dunvXv33vATHDp0SCNHjtRnn32mXr166dSpU0pISNChQ4eUmJgoSdqxY4cmT56sc+fOKSYm5v88p9vtVlhYmFwul+x2+w3PBQAAtB1/Xr/9uoOydetWJSYmavr06YqMjNSwYcP0yiuv/OBjXC6XbDabHA6HJKmwsFAOh8MbJ5KUkpKigIAAFRUVXfccTU1NcrvdPhsAAOi4/AqUTz/9VGvWrFG/fv20c+dOzZs3T48//rjWr19/3fEXL17U4sWLNXPmTG8pVVdXKzIy0mdcUFCQunfvrurq6uueJysrS2FhYd4tNjbWn2kDAIB2xq9A8Xg8Gj58uFauXKlhw4Zp7ty5Sk9PV3Z29jVjm5ublZqaKsuytGbNmluaZGZmplwul3erqKi4pfMBAACz+RUo0dHRSkhI8NkXHx+v8vJyn33fxMlnn32mvLw8n/eZnE6namtrfcZfvnxZdXV1cjqd133e0NBQ2e12nw0AAHRcfgXK6NGjVVJS4rPvk08+Ue/evb0/fxMnpaWl2rVrl8LDw33GJycnq76+XsXFxd59+fn58ng8SkpKuplrAAAAHUyQP4MXLFigUaNGaeXKlUpNTdXBgweVk5OjnJwcSVfj5Be/+IWOHDmi3NxcXblyxfu5ku7duyskJETx8fGaOHGi962h5uZmZWRkaMaMGTf0DR4AANDx+fU1Y0nKzc1VZmamSktLFRcXp4ULFyo9PV2SdPbsWcXFxV33ce+//75+/vOfS7r6h9oyMjK0bds2BQQEaNq0aVq9erW6dOlyQ3Pga8YAALQ//rx++x0oJiBQAABof1rt76AAAADcDgQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMI7fgfL555/r4YcfVnh4uDp16qTBgwfr8OHD3uNvv/22xo8fr/DwcNlsNh07duyac1y8eFHz589XeHi4unTpomnTpqmmpuaWLgQAAHQcfgXKV199pdGjRys4OFjbt2/XyZMntWrVKnXr1s075vz58xozZoyee+657z3PggULtG3bNm3evFkFBQWqrKzUgw8+ePNXAQAAOpQgfwY/99xzio2N1dq1a7374uLifMY88sgjkqSzZ89e9xwul0uvvfaaNmzYoHvuuUeStHbtWsXHx+vAgQO6++67/ZkSAADogPy6g7J161YlJiZq+vTpioyM1LBhw/TKK6/49YTFxcVqbm5WSkqKd9/AgQPVq1cvFRYWXvcxTU1NcrvdPhsAAOi4/AqUTz/9VGvWrFG/fv20c+dOzZs3T48//rjWr19/w+eorq5WSEiIHA6Hz/6oqChVV1df9zFZWVkKCwvzbrGxsf5MGwAAtDN+BYrH49Hw4cO1cuVKDRs2THPnzlV6erqys7Nba36SpMzMTLlcLu9WUVHRqs8HAADall+BEh0drYSEBJ998fHxKi8vv+FzOJ1OXbp0SfX19T77a2pq5HQ6r/uY0NBQ2e12nw0AAHRcfgXK6NGjVVJS4rPvk08+Ue/evW/4HCNGjFBwcLB2797t3VdSUqLy8nIlJyf7Mx0AANBB+fUtngULFmjUqFFauXKlUlNTdfDgQeXk5CgnJ8c7pq6uTuXl5aqsrJQkb9A4nU45nU6FhYVpzpw5Wrhwobp37y673a7HHntMycnJfIMHAABIkmyWZVn+PCA3N1eZmZkqLS1VXFycFi5cqPT0dO/xdevW6dFHH73mccuWLdPy5cslXf1DbU899ZTeeOMNNTU1acKECXr55Ze/9y2e73K73QoLC5PL5eLtHgAA2gl/Xr/9DhQTECgAALQ//rx+82/xAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOP4HSiff/65Hn74YYWHh6tTp04aPHiwDh8+7D1uWZaeeeYZRUdHq1OnTkpJSVFpaanPOerq6pSWlia73S6Hw6E5c+aosbHx1q8GAAB0CH4FyldffaXRo0crODhY27dv18mTJ7Vq1Sp169bNO+b555/X6tWrlZ2draKiInXu3FkTJkzQxYsXvWPS0tJ04sQJ5eXlKTc3V3v27NHcuXNb7qoAAEC7ZrMsy7rRwUuWLNG+ffu0d+/e6x63LEsxMTF66qmntGjRIkmSy+VSVFSU1q1bpxkzZujUqVNKSEjQoUOHlJiYKEnasWOHJk+erHPnzikmJub/nIfb7VZYWJhcLpfsdvuNTh8AALQhf16//bqDsnXrViUmJmr69OmKjIzUsGHD9Morr3iPnzlzRtXV1UpJSfHuCwsLU1JSkgoLCyVJhYWFcjgc3jiRpJSUFAUEBKioqMif6QAAgA7Kr0D59NNPtWbNGvXr1087d+7UvHnz9Pjjj2v9+vWSpOrqaklSVFSUz+OioqK8x6qrqxUZGelzPCgoSN27d/eO+a6mpia53W6fDQAAdFxB/gz2eDxKTEzUypUrJUnDhg3T8ePHlZ2drdmzZ7fKBCUpKytLK1asaLXzAwAAs/h1ByU6OloJCQk+++Lj41VeXi5JcjqdkqSamhqfMTU1Nd5jTqdTtbW1PscvX76suro675jvyszMlMvl8m4VFRX+TBsAALQzfgXK6NGjVVJS4rPvk08+Ue/evSVJcXFxcjqd2r17t/e42+1WUVGRkpOTJUnJycmqr69XcXGxd0x+fr48Ho+SkpKu+7yhoaGy2+0+GwAA6Lj8eotnwYIFGjVqlFauXKnU1FQdPHhQOTk5ysnJkSTZbDY9+eST+tOf/qR+/fopLi5OS5cuVUxMjB544AFJV++4TJw4Uenp6crOzlZzc7MyMjI0Y8aMG/oGDwAA6Pj8+pqxJOXm5iozM1OlpaWKi4vTwoULlZ6e7j1uWZaWLVumnJwc1dfXa8yYMXr55ZfVv39/75i6ujplZGRo27ZtCggI0LRp07R69Wp16dLlhubA14wBAGh//Hn99jtQTECgAADQ/rTa30EBAAC4HQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcYLaegImsSxLXzdfaetpAABghE7BgbLZbG3y3ATKt3zdfEUJz+xs62kAAGCEk3+YoDtD2iYVeIsHAAAYhzso39IpOFAn/zChracBAIAROgUHttlzEyjfYrPZ2uxWFgAA+C/e4gEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGMevQFm+fLlsNpvPNnDgQO/xsrIyTZ06VREREbLb7UpNTVVNTY3POerq6pSWlia73S6Hw6E5c+aosbGxZa4GAAB0CH7fQRk0aJCqqqq824cffihJOn/+vMaPHy+bzab8/Hzt27dPly5d0pQpU+TxeLyPT0tL04kTJ5SXl6fc3Fzt2bNHc+fObbkrAgAA7V6Q3w8ICpLT6bxm/759+3T27FkdPXpUdrtdkrR+/Xp169ZN+fn5SklJ0alTp7Rjxw4dOnRIiYmJkqQXX3xRkydP1p///GfFxMTc4uUAAICOwO87KKWlpYqJiVGfPn2Ulpam8vJySVJTU5NsNptCQ0O9Y++44w4FBAR477IUFhbK4XB440SSUlJSFBAQoKKiou99zqamJrndbp8NAAB0XH4FSlJSktatW6cdO3ZozZo1OnPmjMaOHauGhgbdfffd6ty5sxYvXqwLFy7o/PnzWrRoka5cuaKqqipJUnV1tSIjI33OGRQUpO7du6u6uvp7nzcrK0thYWHeLTY29iYuFQAAtBd+BcqkSZM0ffp0DRkyRBMmTNC7776r+vp6bdq0SREREdq8ebO2bdumLl26KCwsTPX19Ro+fLgCAm7ty0KZmZlyuVzeraKi4pbOBwAAzOb3Z1C+zeFwqH///jp9+rQkafz48SorK9MXX3yhoKAgORwOOZ1O9enTR5LkdDpVW1vrc47Lly+rrq7uup9r+UZoaKjPW0cAAKBju6VbG42NjSorK1N0dLTP/h49esjhcCg/P1+1tbW67777JEnJycmqr69XcXGxd2x+fr48Ho+SkpJuZSoAAKAD8esOyqJFizRlyhT17t1blZWVWrZsmQIDAzVz5kxJ0tq1axUfH6+IiAgVFhbqiSee0IIFCzRgwABJUnx8vCZOnKj09HRlZ2erublZGRkZmjFjBt/gAQAAXn4Fyrlz5zRz5kx9+eWXioiI0JgxY3TgwAFFRERIkkpKSpSZmam6ujrddddd+v3vf68FCxb4nOP1119XRkaGxo0bp4CAAE2bNk2rV69uuSsCAADtns2yLKutJ+Evt9utsLAwuVwu799cAQAAZvPn9Zt/iwcAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYx69AWb58uWw2m882cOBA7/Hq6mo98sgjcjqd6ty5s4YPH65//vOfPueoq6tTWlqa7Ha7HA6H5syZo8bGxpa5GgAA0CEE+fuAQYMGadeuXf89QdB/TzFr1izV19dr69at6tGjhzZs2KDU1FQdPnxYw4YNkySlpaWpqqpKeXl5am5u1qOPPqq5c+dqw4YNLXA5AACgI/D7LZ6goCA5nU7v1qNHD++x/fv367HHHtPIkSPVp08fPf3003I4HCouLpYknTp1Sjt27NCrr76qpKQkjRkzRi+++KI2btyoysrKlrsqAADQrvkdKKWlpYqJiVGfPn2Ulpam8vJy77FRo0bpzTffVF1dnTwejzZu3KiLFy/q5z//uSSpsLBQDodDiYmJ3sekpKQoICBARUVFt341AACgQ/DrLZ6kpCStW7dOAwYMUFVVlVasWKGxY8fq+PHj6tq1qzZt2qSHHnpI4eHhCgoK0p133qktW7aob9++kq5+RiUyMtJ3AkFB6t69u6qrq7/3eZuamtTU1OT92e12+zNtAADQzvgVKJMmTfL+95AhQ5SUlKTevXtr06ZNmjNnjpYuXar6+nrt2rVLPXr00DvvvKPU1FTt3btXgwcPvulJZmVlacWKFTf9eAAA0L74/SHZb3M4HOrfv79Onz6tsrIy/e1vf9Px48c1aNAgSdLQoUO1d+9evfTSS8rOzpbT6VRtba3POS5fvqy6ujo5nc7vfZ7MzEwtXLjQ+7Pb7VZsbOytTB0AABjslv4OSmNjo8rKyhQdHa0LFy5cPWGA7ykDAwPl8XgkScnJyaqvr/d+aFaS8vPz5fF4lJSU9L3PExoaKrvd7rMBAICOy69AWbRokQoKCnT27Fnt379fU6dOVWBgoGbOnKmBAweqb9+++vWvf62DBw+qrKxMq1atUl5enh544AFJUnx8vCZOnKj09HQdPHhQ+/btU0ZGhmbMmKGYmJjWuD4AANAO+fUWz7lz5zRz5kx9+eWXioiI0JgxY3TgwAFFRERIkt59910tWbJEU6ZMUWNjo/r27av169dr8uTJ3nO8/vrrysjI0Lhx4xQQEKBp06Zp9erVLXtVAACgXbNZlmW19ST85Xa7FRYWJpfLxds9AAC0E/68fvNv8QAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4fgXK8uXLZbPZfLaBAwdKks6ePXvNsW+2zZs3e89RXl6ue++9V3feeaciIyP1u9/9TpcvX27ZqwIAAO1akL8PGDRokHbt2vXfEwRdPUVsbKyqqqp8xubk5OiFF17QpEmTJElXrlzRvffeK6fTqf3796uqqkqzZs1ScHCwVq5ceSvXAQAAOhC/AyUoKEhOp/Oa/YGBgdfs37Jli1JTU9WlSxdJ0nvvvaeTJ09q165dioqK0k9+8hP98Y9/1OLFi7V8+XKFhITc5GUAAICOxO/PoJSWliomJkZ9+vRRWlqaysvLrzuuuLhYx44d05w5c7z7CgsLNXjwYEVFRXn3TZgwQW63WydOnPje52xqapLb7fbZAABAx+VXoCQlJWndunXasWOH1qxZozNnzmjs2LFqaGi4Zuxrr72m+Ph4jRo1yruvurraJ04keX+urq7+3ufNyspSWFiYd4uNjfVn2gAAoJ3xK1AmTZqk6dOna8iQIZowYYLeffdd1dfXa9OmTT7jvv76a23YsMHn7smtyMzMlMvl8m4VFRUtcl4AAGAmvz+D8m0Oh0P9+/fX6dOnffa/9dZbunDhgmbNmuWz3+l06uDBgz77ampqvMe+T2hoqEJDQ29lqgAAoB25pb+D0tjYqLKyMkVHR/vsf+2113TfffcpIiLCZ39ycrI+/vhj1dbWevfl5eXJbrcrISHhVqYCAAA6EL8CZdGiRSooKNDZs2e1f/9+TZ06VYGBgZo5c6Z3zOnTp7Vnzx796le/uubx48ePV0JCgh555BH9+9//1s6dO/X0009r/vz53CEBAABefr3Fc+7cOc2cOVNffvmlIiIiNGbMGB04cMDnTsk//vEP9ezZU+PHj7/m8YGBgcrNzdW8efOUnJyszp07a/bs2frDH/5w61cCAAA6DJtlWVZbT8JfbrdbYWFhcrlcstvtbT0dAABwA/x5/ebf4gEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxglq6wncDMuyJElut7uNZwIAAG7UN6/b37yO/5B2GSgNDQ2SpNjY2DaeCQAA8FdDQ4PCwsJ+cIzNupGMMYzH41FlZaW6du0qm83Woud2u92KjY1VRUWF7HZ7i54b12K9by/W+/ZivW8v1vv2upn1tixLDQ0NiomJUUDAD3/KpF3eQQkICFDPnj1b9Tnsdju/4LcR6317sd63F+t9e7Het5e/6/1/3Tn5Bh+SBQAAxiFQAACAcQiU7wgNDdWyZcsUGhra1lP5n8B6316s9+3Fet9erPft1drr3S4/JAsAADo27qAAAADjECgAAMA4BAoAADAOgQIAAIxDoHzLSy+9pLvuukt33HGHkpKSdPDgwbaeUoexZ88eTZkyRTExMbLZbHrnnXd8jluWpWeeeUbR0dHq1KmTUlJSVFpa2jaTbeeysrL005/+VF27dlVkZKQeeOABlZSU+Iy5ePGi5s+fr/DwcHXp0kXTpk1TTU1NG824fVuzZo2GDBni/WNVycnJ2r59u/c4a926nn32WdlsNj355JPefax5y1m+fLlsNpvPNnDgQO/x1lxrAuX/e/PNN7Vw4UItW7ZMR44c0dChQzVhwgTV1ta29dQ6hPPnz2vo0KF66aWXrnv8+eef1+rVq5Wdna2ioiJ17txZEyZM0MWLF2/zTNu/goICzZ8/XwcOHFBeXp6am5s1fvx4nT9/3jtmwYIF2rZtmzZv3qyCggJVVlbqwQcfbMNZt189e/bUs88+q+LiYh0+fFj33HOP7r//fp04cUISa92aDh06pL///e8aMmSIz37WvGUNGjRIVVVV3u3DDz/0HmvVtbZgWZZljRw50po/f7735ytXrlgxMTFWVlZWG86qY5Jkbdmyxfuzx+OxnE6n9cILL3j31dfXW6GhodYbb7zRBjPsWGpray1JVkFBgWVZV9c2ODjY2rx5s3fMqVOnLElWYWFhW02zQ+nWrZv16quvstatqKGhwerXr5+Vl5dn/exnP7OeeOIJy7L4/W5py5Yts4YOHXrdY6291txBkXTp0iUVFxcrJSXFuy8gIEApKSkqLCxsw5n9bzhz5oyqq6t91j8sLExJSUmsfwtwuVySpO7du0uSiouL1dzc7LPeAwcOVK9evVjvW3TlyhVt3LhR58+fV3JyMmvdiubPn697773XZ20lfr9bQ2lpqWJiYtSnTx+lpaWpvLxcUuuvdbv8xwJb2hdffKErV64oKirKZ39UVJT+85//tNGs/ndUV1dL0nXX/5tjuDkej0dPPvmkRo8erR//+MeSrq53SEiIHA6Hz1jW++Z9/PHHSk5O1sWLF9WlSxdt2bJFCQkJOnbsGGvdCjZu3KgjR47o0KFD1xzj97tlJSUlad26dRowYICqqqq0YsUKjR07VsePH2/1tSZQgA5s/vz5On78uM97xmh5AwYM0LFjx+RyufTWW29p9uzZKigoaOtpdUgVFRV64oknlJeXpzvuuKOtp9PhTZo0yfvfQ4YMUVJSknr37q1NmzapU6dOrfrcvMUjqUePHgoMDLzmk8c1NTVyOp1tNKv/Hd+sMevfsjIyMpSbm6v3339fPXv29O53Op26dOmS6uvrfcaz3jcvJCREffv21YgRI5SVlaWhQ4fqr3/9K2vdCoqLi1VbW6vhw4crKChIQUFBKigo0OrVqxUUFKSoqCjWvBU5HA71799fp0+fbvXfbwJFV//nMmLECO3evdu7z+PxaPfu3UpOTm7Dmf1viIuLk9Pp9Fl/t9utoqIi1v8mWJaljIwMbdmyRfn5+YqLi/M5PmLECAUHB/usd0lJicrLy1nvFuLxeNTU1MRat4Jx48bp448/1rFjx7xbYmKi0tLSvP/NmreexsZGlZWVKTo6uvV/v2/5Y7YdxMaNG63Q0FBr3bp11smTJ625c+daDofDqq6ubuupdQgNDQ3W0aNHraNHj1qSrL/85S/W0aNHrc8++8yyLMt69tlnLYfDYf3rX/+yPvroI+v++++34uLirK+//rqNZ97+zJs3zwoLC7M++OADq6qqyrtduHDBO+Y3v/mN1atXLys/P986fPiwlZycbCUnJ7fhrNuvJUuWWAUFBdaZM2esjz76yFqyZIlls9ms9957z7Is1vp2+Pa3eCyLNW9JTz31lPXBBx9YZ86csfbt22elpKRYPXr0sGpray3Lat21JlC+5cUXX7R69eplhYSEWCNHjrQOHDjQ1lPqMN5//31L0jXb7NmzLcu6+lXjpUuXWlFRUVZoaKg1btw4q6SkpG0n3U5db50lWWvXrvWO+frrr63f/va3Vrdu3aw777zTmjp1qlVVVdV2k27HfvnLX1q9e/e2QkJCrIiICGvcuHHeOLEs1vp2+G6gsOYt56GHHrKio6OtkJAQ60c/+pH10EMPWadPn/Yeb821tlmWZd36fRgAAICWw2dQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxvl/kcHiKyry/FwAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11057/11057 [00:00<00:00, 502578.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Samples per Genre: 594\n",
      "Max Samples per Genre: 594\n",
      " Standard Deviation: 0.0\n",
      " Mean: 594.0\n",
      "Tracks in Total: 11057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11057/11057 [00:00<00:00, 13131.84it/s]\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "from data.processing import ParseBalanced\n",
    "\n",
    "subset_name = \"autotagging_top50tags\"\n",
    "subset_file = f\"E:/mtg-jamendo-dataset/data/{subset_name}.tsv\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "output_directory = f\"D:/SongsDataset/melspec-mtg-jamendo\"\n",
    "subset_data = f'E:/mtg-jamendo-dataset/stats/{subset_name}/all.tsv'\n",
    "\n",
    "ParseBalanced(subset_file, subset_data, data_directory, output_directory)"
   ],
   "id": "60706242edea8ffb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T06:16:39.871067Z",
     "start_time": "2025-10-23T06:16:30.608125Z"
    }
   },
   "source": [
    "from utils.Config import Config\n",
    "from data.data_utils import *\n",
    "\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "prefetch_factor = 1\n",
    "\n",
    "model_name = \"Myna-CLS-2D-ALIBI\"\n",
    "config = Config(\n",
    "        save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "        num_epochs=8,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=1e-4,\n",
    "        num_workers=2,\n",
    "        batch_size= 32,\n",
    "        eval_batch_size=32,\n",
    "        dtype=torch.float32\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "25b92472f73db5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T06:16:39.907472Z",
     "start_time": "2025-10-23T06:16:39.876065Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"D:\\\\SongsDataset\\\\melspec-mtg-jamendo\\\\train_set\\\\\", chunk_size=256, views=2)\n",
    "test_dataset  = StreamViewDataset(f\"D:\\\\SongsDataset\\\\melspec-mtg-jamendo\\\\test_set\\\\\", chunk_size=256, views=2)\n",
    "\n",
    "random_indicies = np.random.permutation(len(train_dataset))\n",
    "n_fold = random_indicies[:1024]\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, n_fold)\n",
    "\n",
    "random_indicies = np.random.permutation(len(test_dataset))\n",
    "n_fold = random_indicies[:256]\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, n_fold)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=config.num_workers,\n",
    "    # prefetch_factor=prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=config.num_workers,\n",
    "    # prefetch_factor=prefetch_factor,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2167d4f0a8538e6f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-23T06:16:40.275472Z"
    }
   },
   "source": [
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, 256),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"2D-ALIBI\"\n",
    ")\n",
    "\n",
    "model_name = \"Myna-CLS-2D-ALIBI\"\n",
    "config = Config(\n",
    "        save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "        num_epochs=64,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=1e-4,\n",
    "        num_workers=config.num_workers,\n",
    "        batch_size=config.batch_size,\n",
    "        eval_batch_size=config.batch_size,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, convex=False, start_epoch=0, views=2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21425536 Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d0aa324874146fdb7e8a0ad9c635d41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, 256),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"sinusoidal\"\n",
    ")\n",
    "\n",
    "model_name = \"Myna-CLS-Sinusoidal\"\n",
    "config = Config(\n",
    "        save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "        num_epochs=64,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=1e-4,\n",
    "        num_workers=2,\n",
    "        batch_size= 256,\n",
    "        eval_batch_size=484,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, convex=False, start_epoch=0, views=2)"
   ],
   "id": "af973eddce362141"
  },
  {
   "cell_type": "code",
   "id": "727b985fcedeefba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T21:46:57.326367Z",
     "start_time": "2025-10-20T21:46:11.394546Z"
    }
   },
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-ALIBI\\\\Epoch-103.pt\", weights_only=False)\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc773067b0164b0e905ba00d0451f935"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8604929447174072\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a36b40ab1336637b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T21:47:29.887878Z",
     "start_time": "2025-10-20T21:46:57.385617Z"
    }
   },
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-Sinusoidal\\\\Epoch-96.pt\", weights_only=False)\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3bd0ae1c6ab4339988b58fe47f63b84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.715553879737854\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd03050-a728-462a-9e90-0df61af7ea96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21425536 Parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0346790190d64cf583557c0744665b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 21\u001B[0m\n\u001B[0;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m Myna(\n\u001B[0;32m      6\u001B[0m     image_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m256\u001B[39m),\n\u001B[0;32m      7\u001B[0m     channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m     alibi\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     17\u001B[0m )\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmisc\u001B[38;5;241m.\u001B[39mmodel_size(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m \u001B[43mtrain_contrastive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariational\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_masked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_masked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malbum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m14\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mviews\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\contrastive_training.py:114\u001B[0m, in \u001B[0;36mtrain_contrastive\u001B[1;34m(model, test_dataloader, train_dataloader, config, variational, train_masked, test_masked, album, convex, start_epoch, views)\u001B[0m\n\u001B[0;32m    110\u001B[0m     B, _, T, F \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    112\u001B[0m stacked \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(inputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m--> 114\u001B[0m z_stacked \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstacked\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m z_list \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(z_stacked, B, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    118\u001B[0m contrastive_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:245\u001B[0m, in \u001B[0;36mMyna.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    243\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x, coords\u001B[38;5;241m=\u001B[39mcoordinates, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 245\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    248\u001B[0m     x \u001B[38;5;241m=\u001B[39m x[:, \u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:161\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, coords, cls)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, coords\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m coords \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 161\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    162\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:63\u001B[0m, in \u001B[0;36mAlibi2DBias.forward\u001B[1;34m(self, coords)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, coords):\n\u001B[0;32m     60\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;124;03m    coords: (B, N, 2) tensor with (x, y) integer positions of kept patches\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m     B, N, _ \u001B[38;5;241m=\u001B[39m \u001B[43mcoords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\n\u001B[0;32m     64\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m coords[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mfloat(), coords[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     66\u001B[0m     dx \u001B[38;5;241m=\u001B[39m (x[:, :, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m-\u001B[39m x[:, \u001B[38;5;28;01mNone\u001B[39;00m, :])\u001B[38;5;241m.\u001B[39mabs()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, 256),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"Sinusoidal\"\n",
    ")\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, convex=False, start_epoch=14, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea07b222919e84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:30:14.751780Z",
     "start_time": "2025-10-15T21:29:41.844616Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model.mask_ratio = 0.25\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed9b902789fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:30:51.957111Z",
     "start_time": "2025-10-15T21:30:14.775777Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6af62a7e1524ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.autoencoding_training import train_autoencode\n",
    "train_autoencode(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
