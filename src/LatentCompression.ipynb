{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60706242edea8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.processing import ParseBalanced\n",
    "\n",
    "directory = \"melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "ParseBalanced(subset_file_name, f\"{data_directory}\", f\"D:/SongsDataset/{directory}\", convert=True, target_per_genre=1300, chunk_size=256, chunks_per_batch=1, write_individually=True)"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T21:45:20.559860Z",
     "start_time": "2025-10-20T21:45:13.121139Z"
    }
   },
   "source": [
    "from info_nce import InfoNCE\n",
    "from data.data_utils import *\n",
    "\n",
    "from libauc.losses.contrastive import GCLoss_v1\n",
    "\n",
    "directory = \"melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "\n",
    "# augmentations = Compose([\n",
    "#     AddGaussianNoise(std=0.25),\n",
    "# ])\n",
    "\n",
    "class Config:\n",
    "    # === General ===\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    model_name = \"Myna-CLS-Sinusoidal\"\n",
    "    save_path = f\"trained_models\\\\{model_name}\\\\\"\n",
    "    seed = 42\n",
    "\n",
    "    # === Training ===\n",
    "    num_epochs = 128\n",
    "    batch_size = 484\n",
    "    learning_rate = 2e-4\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    coef = 1\n",
    "    cycles = 42\n",
    "    warmup = 0\n",
    "    gamma = 2.0\n",
    "\n",
    "    sogclr_tau = 0.1\n",
    "    sogclr_gamma = 0.9\n",
    "    gamma_schedule = 'constant' #'cosine'\n",
    "    epochs = 0\n",
    "    sogclr_eps = 1e-8\n",
    "    isogclr = False\n",
    "    rank = 0\n",
    "    lr_schedule = 'constant'\n",
    "\n",
    "    # === Dataset ===\n",
    "    transforms = None\n",
    "    use_masks = True\n",
    "    num_workers = 1\n",
    "    prefetch_factor = 1\n",
    "    val_split = 0.1\n",
    "    #pos_weight = (torch.ones(num_classes) * 50).to(\"cuda\")\n",
    "    criterion = InfoNCE()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "25b92472f73db5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T21:45:20.712851Z",
     "start_time": "2025-10-20T21:45:20.565861Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "large_directory = directory\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\data\", \n",
    "                                  f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\genre_labels\", pair_album=False, views=2)\n",
    "\n",
    "test_dataset  = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\data\", \n",
    "                                  f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\genre_labels\", pair_album=False, views=2)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f15ad34c069a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:31:37.758363300Z",
     "start_time": "2025-10-14T14:31:13.623019Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import train_contrastive\n",
    "from models.AudioViTEncoder import AudioViTEncoder\n",
    "from utils import misc\n",
    "\n",
    "Config.model_name = \"ViT-Contrastive-Embeddings-Masking-0.9\"\n",
    "Config.save_path = f\"trained_models\\\\{Config.model_name}\\\\\"\n",
    "\n",
    "model = AudioViTEncoder(patch_size=8, input_dim=128, num_heads=8, encoder_layers=8, length=256, d_model=256, dim_feedforward=512, dropout=0.1, latent_space=128, use_alibi=True, use_pooling=False, CLS=True, use_rope=False, masking_percent=0.0, variational=False)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=False, train_masked=True, test_masked=False, album=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2167d4f0a8538e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T13:12:45.414436800Z",
     "start_time": "2025-10-19T12:54:47.312198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21425536 Parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733e6e64af0141c0904e2cba0f99a473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0d55e5335a4cc4b769081c841dee32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 79] Train: Same Song Contrastive Loss = 4.4652\n",
      "Test: Same Song Contrastive Loss = 3.7963\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac556e37b27542eb9f9a979e1edd8928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98795c7763b4f33bfffe5b615e542d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 80] Train: Same Song Contrastive Loss = 4.4396\n",
      "Test: Same Song Contrastive Loss = 3.8100\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f51c06e7b6418cbe277f96458c3ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7da2a787f7493589b6f48a433d57dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 81] Train: Same Song Contrastive Loss = 4.3740\n",
      "Test: Same Song Contrastive Loss = 3.9107\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faceeb42105e4176bc4f3878f93eef97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9cec286f534b11842bd333da5ee959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 82] Train: Same Song Contrastive Loss = 4.3619\n",
      "Test: Same Song Contrastive Loss = 3.8902\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a90629ea134d3db05c6395b4ae995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a23cef4c324c93b0d6e5a045663dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 83] Train: Same Song Contrastive Loss = 4.3991\n",
      "Test: Same Song Contrastive Loss = 3.7609\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf4026b00ab4c3ca552ede05260ac42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c665f2984a9427f858ed1d20e779721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 84] Train: Same Song Contrastive Loss = 4.3876\n",
      "Test: Same Song Contrastive Loss = 3.9849\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedeeb303a7e4233a20a31e7475e2ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6b94ce477e48459c4869ea2fccfe6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 85] Train: Same Song Contrastive Loss = 4.3579\n",
      "Test: Same Song Contrastive Loss = 3.7658\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de808363ed2c4e24990ea502ce84a5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:24<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e443dc64eae54d1f87f45ef288bfdafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 86] Train: Same Song Contrastive Loss = 4.3792\n",
      "Test: Same Song Contrastive Loss = 3.9610\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b45bf77f9f54ab09eec2ca5918a03f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:33<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94a36a622e446d0baab4a9453caf9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 87] Train: Same Song Contrastive Loss = 4.3512\n",
      "Test: Same Song Contrastive Loss = 3.9475\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0a876835914589af41e330616ad689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:25<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b98e71b2bdf44d698d2c35213d3ae27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 88] Train: Same Song Contrastive Loss = 4.3783\n",
      "Test: Same Song Contrastive Loss = 3.9077\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e571cf9a8d044afea8be66324dd53252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9863b4a46374fc79a30bd4282771df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 89] Train: Same Song Contrastive Loss = 4.3353\n",
      "Test: Same Song Contrastive Loss = 3.8489\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30939a56c86441c48f4a2e4c09c0a1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:06<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab7b8f3c9d64fa094ffa308844562f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 90] Train: Same Song Contrastive Loss = 4.3505\n",
      "Test: Same Song Contrastive Loss = 3.8058\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234b2c84098c48e6bc0ca28c94211c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a742701028f84b8e8c3430bb6192eeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 91] Train: Same Song Contrastive Loss = 4.3384\n",
      "Test: Same Song Contrastive Loss = 3.9670\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14cd404f94442bb968a87c610125ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:08<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376c0c8d0f944e21b701141a986bfe8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 92] Train: Same Song Contrastive Loss = 4.3096\n",
      "Test: Same Song Contrastive Loss = 4.0213\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605d79c002f5455688770e3d0c6ae62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:06<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd99ed0ecc842dabec8cb267fef8a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 93] Train: Same Song Contrastive Loss = 4.3525\n",
      "Test: Same Song Contrastive Loss = 3.8019\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2a2521246944539e98714f24b605be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:08<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb50a5f5d2b048adac81238c16bb71a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 94] Train: Same Song Contrastive Loss = 4.3138\n",
      "Test: Same Song Contrastive Loss = 3.6927\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70cd1a9b0d840efa408913a459066d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:28<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a62249d9ad408b94ea69348671b4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 95] Train: Same Song Contrastive Loss = 4.3256\n",
      "Test: Same Song Contrastive Loss = 3.7835\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b233808123f40929cf963a62e7fc176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:25<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e35d7d9117d4d08a0d638198758eb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96] Train: Same Song Contrastive Loss = 4.3305\n",
      "Test: Same Song Contrastive Loss = 3.8045\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b80ba324e84e3bb5f3e1fd4dbef28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:22<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 223, in collate\n    clone[i] = collate(samples, collate_fn_map=collate_fn_map)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\storage.py\", line 1180, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\storage.py\", line 400, in _new_shared\n    return cls._new_using_filename_cpu(size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn't open shared file mapping: <torch_132156_2014720276_16>, error code: <1455>\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mCoding\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongAnalyzer\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mAnalyzer\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtrained_models\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mMyna-CLS-Sinusoidal\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mEpoch-78.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m, weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmisc\u001B[38;5;241m.\u001B[39mmodel_size(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 22\u001B[0m \u001B[43mtrain_contrastive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariational\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malbum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m79\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mviews\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\contrastive_training.py:44\u001B[0m, in \u001B[0;36mtrain_contrastive\u001B[1;34m(model, test_dataloader, train_dataloader, config, variational, test_masked, album, convex, start_epoch, views)\u001B[0m\n\u001B[0;32m     40\u001B[0m epoch_convex_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     42\u001B[0m batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n\u001B[1;32m---> 44\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindicies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mview\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\tqdm\\notebook.py:250\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    249\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[1;32m--> 250\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mit\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   1182\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[0;32m   1183\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[0;32m   1184\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1463\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1464\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1489\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1490\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1491\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1492\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\_utils.py:715\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    711\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    712\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    713\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    714\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 715\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 223, in collate\n    clone[i] = collate(samples, collate_fn_map=collate_fn_map)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\storage.py\", line 1180, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\storage.py\", line 400, in _new_shared\n    return cls._new_using_filename_cpu(size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn't open shared file mapping: <torch_132156_2014720276_16>, error code: <1455>\n"
     ]
    }
   ],
   "source": [
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "# model = Myna(\n",
    "#     image_size=(128, 256),\n",
    "#     channels=1,\n",
    "#     patch_size=(16, 16),\n",
    "#     latent_space=128,\n",
    "#     d_model=384,\n",
    "#     depth=12,\n",
    "#     heads=6,\n",
    "#     mlp_dim=1536,\n",
    "#     mask_ratio=0.9,\n",
    "#     use_cls=True,\n",
    "#     alibi=False\n",
    "# )\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=False, album=False, convex=False, start_epoch=79, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "id": "727b985fcedeefba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T21:46:57.326367Z",
     "start_time": "2025-10-20T21:46:11.394546Z"
    }
   },
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-ALIBI\\\\Epoch-103.pt\", weights_only=False)\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc773067b0164b0e905ba00d0451f935"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8604929447174072\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a36b40ab1336637b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T21:47:29.887878Z",
     "start_time": "2025-10-20T21:46:57.385617Z"
    }
   },
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-Sinusoidal\\\\Epoch-96.pt\", weights_only=False)\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3bd0ae1c6ab4339988b58fe47f63b84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.715553879737854\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd03050-a728-462a-9e90-0df61af7ea96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21425536 Parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0346790190d64cf583557c0744665b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 21\u001B[0m\n\u001B[0;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m Myna(\n\u001B[0;32m      6\u001B[0m     image_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m256\u001B[39m),\n\u001B[0;32m      7\u001B[0m     channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m     alibi\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     17\u001B[0m )\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmisc\u001B[38;5;241m.\u001B[39mmodel_size(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m \u001B[43mtrain_contrastive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariational\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_masked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_masked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malbum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m14\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mviews\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\contrastive_training.py:114\u001B[0m, in \u001B[0;36mtrain_contrastive\u001B[1;34m(model, test_dataloader, train_dataloader, config, variational, train_masked, test_masked, album, convex, start_epoch, views)\u001B[0m\n\u001B[0;32m    110\u001B[0m     B, _, T, F \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    112\u001B[0m stacked \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(inputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m--> 114\u001B[0m z_stacked \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstacked\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m z_list \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(z_stacked, B, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    118\u001B[0m contrastive_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:245\u001B[0m, in \u001B[0;36mMyna.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    243\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x, coords\u001B[38;5;241m=\u001B[39mcoordinates, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 245\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    248\u001B[0m     x \u001B[38;5;241m=\u001B[39m x[:, \u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:161\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, coords, cls)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, coords\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m coords \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 161\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    162\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:63\u001B[0m, in \u001B[0;36mAlibi2DBias.forward\u001B[1;34m(self, coords)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, coords):\n\u001B[0;32m     60\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;124;03m    coords: (B, N, 2) tensor with (x, y) integer positions of kept patches\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m     B, N, _ \u001B[38;5;241m=\u001B[39m \u001B[43mcoords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\n\u001B[0;32m     64\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m coords[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mfloat(), coords[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     66\u001B[0m     dx \u001B[38;5;241m=\u001B[39m (x[:, :, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m-\u001B[39m x[:, \u001B[38;5;28;01mNone\u001B[39;00m, :])\u001B[38;5;241m.\u001B[39mabs()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, 256),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    alibi=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=False, train_masked=True, test_masked=False, album=False, convex=False, start_epoch=14, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea07b222919e84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:30:14.751780Z",
     "start_time": "2025-10-15T21:29:41.844616Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model.mask_ratio = 0.25\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed9b902789fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:30:51.957111Z",
     "start_time": "2025-10-15T21:30:14.775777Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6af62a7e1524ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.autoencoding_training import train_autoencode\n",
    "train_autoencode(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82afdf885f8438",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import IPython\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from datasets import tqdm\n",
    "from training.inference import load_and_parse_audio\n",
    "\n",
    "def test(model):\n",
    "    path = \"E:\\\\SongsDataset\\\\songs\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for each_song in tqdm(all_folders[100:110]):\n",
    "            song_path = os.path.join(path, each_song)\n",
    "            chunks = load_and_parse_audio(song_path, convert=True, chunk_size=1024).to(\"cuda\")\n",
    "            permuted_chunks = torch.stack([c for c in chunks])\n",
    "\n",
    "            mean = permuted_chunks.mean(dim=[1, 2], keepdim=True)\n",
    "            std = permuted_chunks.std(dim=[1, 2], keepdim=True)\n",
    "\n",
    "            permuted_chunks = (permuted_chunks - mean) / (std + 1e-6)\n",
    "\n",
    "            reconstructed, latent = model(permuted_chunks)\n",
    "\n",
    "            input_tensor = np.concatenate(permuted_chunks.cpu().detach().numpy(), axis=1)\n",
    "            reconstructed = np.concatenate(reconstructed.cpu().detach().numpy(), axis=1)\n",
    "\n",
    "            input_tensor = input_tensor[:, :512]\n",
    "            reconstructed = reconstructed[:, :512]\n",
    "\n",
    "            graph(input_tensor, reconstructed)\n",
    "\n",
    "            S_recon = librosa.feature.inverse.mel_to_stft(reconstructed)\n",
    "            Y_recon = librosa.griffinlim(S_recon)\n",
    "\n",
    "            S_orig = librosa.feature.inverse.mel_to_stft(input_tensor)\n",
    "            Y_orig = librosa.griffinlim(S_orig)\n",
    "\n",
    "            IPython.display.display(IPython.display.Audio(Y_orig, rate=44100))\n",
    "            IPython.display.display(IPython.display.Audio(Y_recon, rate=44100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
