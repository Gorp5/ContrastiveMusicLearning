{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60706242edea8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.processing import ParseBalanced\n",
    "\n",
    "directory = \"melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "ParseBalanced(subset_file_name, f\"{data_directory}\", f\"D:/SongsDataset/{directory}\", convert=True, target_per_genre=1300, chunk_size=256, chunks_per_batch=1, write_individually=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:54:47.082182Z",
     "start_time": "2025-10-19T12:54:40.560208Z"
    }
   },
   "outputs": [],
   "source": [
    "from info_nce import InfoNCE\n",
    "from data.data_utils import *\n",
    "\n",
    "from libauc.losses.contrastive import GCLoss_v1\n",
    "\n",
    "directory = \"melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "\n",
    "# augmentations = Compose([\n",
    "#     AddGaussianNoise(std=0.25),\n",
    "# ])\n",
    "\n",
    "class Config:\n",
    "    # === General ===\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    model_name = \"Myna-CLS-Sinusoidal\"\n",
    "    save_path = f\"trained_models\\\\{model_name}\\\\\"\n",
    "    seed = 42\n",
    "\n",
    "    # === Training ===\n",
    "    num_epochs = 128\n",
    "    batch_size = 484\n",
    "    learning_rate = 2e-4\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    coef = 1\n",
    "    cycles = 42\n",
    "    warmup = 0\n",
    "    gamma = 2.0\n",
    "\n",
    "    sogclr_tau = 0.1\n",
    "    sogclr_gamma = 0.9\n",
    "    gamma_schedule = 'constant' #'cosine'\n",
    "    epochs = 0\n",
    "    sogclr_eps = 1e-8\n",
    "    isogclr = False\n",
    "    rank = 0\n",
    "    lr_schedule = 'constant'\n",
    "\n",
    "    # === Dataset ===\n",
    "    transforms = None\n",
    "    use_masks = True\n",
    "    num_workers = 1\n",
    "    prefetch_factor = 1\n",
    "    val_split = 0.1\n",
    "    #pos_weight = (torch.ones(num_classes) * 50).to(\"cuda\")\n",
    "    criterion = InfoNCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b92472f73db5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:54:47.099690Z",
     "start_time": "2025-10-19T12:54:47.086180Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "large_directory = directory\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\data\", \n",
    "                                  f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\genre_labels\", pair_album=False, views=2)\n",
    "\n",
    "test_dataset  = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\data\", \n",
    "                                  f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\genre_labels\", pair_album=False, views=2)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f15ad34c069a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:31:37.758363300Z",
     "start_time": "2025-10-14T14:31:13.623019Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import train_contrastive\n",
    "from models.AudioViTEncoder import AudioViTEncoder\n",
    "from utils import misc\n",
    "\n",
    "Config.model_name = \"ViT-Contrastive-Embeddings-Masking-0.9\"\n",
    "Config.save_path = f\"trained_models\\\\{Config.model_name}\\\\\"\n",
    "\n",
    "model = AudioViTEncoder(patch_size=8, input_dim=128, num_heads=8, encoder_layers=8, length=256, d_model=256, dim_feedforward=512, dropout=0.1, latent_space=128, use_alibi=True, use_pooling=False, CLS=True, use_rope=False, masking_percent=0.0, variational=False)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=False, train_masked=True, test_masked=False, album=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167d4f0a8538e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T13:12:45.414436800Z",
     "start_time": "2025-10-19T12:54:47.312198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21425536 Parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c3ebfd6a3d48fbb64cdc2d3ae75e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:03<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d4b4e3bdf14f248653511989281da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train: Same Song Contrastive Loss = 6.1726\n",
      "Test: Same Song Contrastive Loss = 5.5328\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d3f89a0e4547b7b4cd99ff96ef97d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9985304be04d069dceb3afd842e450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train: Same Song Contrastive Loss = 6.1222\n",
      "Test: Same Song Contrastive Loss = 5.4503\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4553c7cac48e4dd799a9d5c3d9b4b908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, 256),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    alibi=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=False, train_masked=True, test_masked=False, album=False, convex=False, start_epoch=0, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727b985fcedeefba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:29:13.705871Z",
     "start_time": "2025-10-15T21:28:42.509919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfebf65727044d26926aea312779453a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3003698190053306\n"
     ]
    }
   ],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-ALIBI-2\\\\Epoch-64.pt\", weights_only=False)\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36b40ab1336637b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:29:41.828590Z",
     "start_time": "2025-10-15T21:29:13.860902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12665e3959a84f5db62c609e00855282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2113026777903237\n"
     ]
    }
   ],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS\\\\Epoch-64.pt\", weights_only=False)\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd03050-a728-462a-9e90-0df61af7ea96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21425536 Parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0346790190d64cf583557c0744665b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Myna(\n\u001b[0;32m      6\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m      7\u001b[0m     channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     alibi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmisc\u001b[38;5;241m.\u001b[39mmodel_size(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrain_contrastive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariational\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malbum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviews\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\contrastive_training.py:114\u001b[0m, in \u001b[0;36mtrain_contrastive\u001b[1;34m(model, test_dataloader, train_dataloader, config, variational, train_masked, test_masked, album, convex, start_epoch, views)\u001b[0m\n\u001b[0;32m    110\u001b[0m     B, _, T, F \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    112\u001b[0m stacked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m z_stacked \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m z_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(z_stacked, B, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    118\u001b[0m contrastive_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:245\u001b[0m, in \u001b[0;36mMyna.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    243\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x, coords\u001b[38;5;241m=\u001b[39mcoordinates, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:161\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, coords, cls)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, coords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         alibi_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malibi_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m         alibi_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:63\u001b[0m, in \u001b[0;36mAlibi2DBias.forward\u001b[1;34m(self, coords)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, coords):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    coords: (B, N, 2) tensor with (x, y) integer positions of kept patches\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     B, N, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     64\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m coords[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat(), coords[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     66\u001b[0m     dx \u001b[38;5;241m=\u001b[39m (x[:, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m-\u001b[39m x[:, \u001b[38;5;28;01mNone\u001b[39;00m, :])\u001b[38;5;241m.\u001b[39mabs()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, 256),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    alibi=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=False, train_masked=True, test_masked=False, album=False, convex=False, start_epoch=14, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea07b222919e84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:30:14.751780Z",
     "start_time": "2025-10-15T21:29:41.844616Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model.mask_ratio = 0.25\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed9b902789fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:30:51.957111Z",
     "start_time": "2025-10-15T21:30:14.775777Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.contrastive_training import evaluate_contrastive\n",
    "\n",
    "model.mask_ratio = 0.0\n",
    "same_song_contrastive_loss = evaluate_contrastive(model, test_dataloader, Config, test_masked=False)\n",
    "print(same_song_contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6af62a7e1524ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from training.autoencoding_training import train_autoencode\n",
    "train_autoencode(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82afdf885f8438",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import IPython\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from datasets import tqdm\n",
    "from training.inference import load_and_parse_audio\n",
    "\n",
    "def test(model):\n",
    "    path = \"E:\\\\SongsDataset\\\\songs\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for each_song in tqdm(all_folders[100:110]):\n",
    "            song_path = os.path.join(path, each_song)\n",
    "            chunks = load_and_parse_audio(song_path, convert=True, chunk_size=1024).to(\"cuda\")\n",
    "            permuted_chunks = torch.stack([c for c in chunks])\n",
    "\n",
    "            mean = permuted_chunks.mean(dim=[1, 2], keepdim=True)\n",
    "            std = permuted_chunks.std(dim=[1, 2], keepdim=True)\n",
    "\n",
    "            permuted_chunks = (permuted_chunks - mean) / (std + 1e-6)\n",
    "\n",
    "            reconstructed, latent = model(permuted_chunks)\n",
    "\n",
    "            input_tensor = np.concatenate(permuted_chunks.cpu().detach().numpy(), axis=1)\n",
    "            reconstructed = np.concatenate(reconstructed.cpu().detach().numpy(), axis=1)\n",
    "\n",
    "            input_tensor = input_tensor[:, :512]\n",
    "            reconstructed = reconstructed[:, :512]\n",
    "\n",
    "            graph(input_tensor, reconstructed)\n",
    "\n",
    "            S_recon = librosa.feature.inverse.mel_to_stft(reconstructed)\n",
    "            Y_recon = librosa.griffinlim(S_recon)\n",
    "\n",
    "            S_orig = librosa.feature.inverse.mel_to_stft(input_tensor)\n",
    "            Y_orig = librosa.griffinlim(S_orig)\n",
    "\n",
    "            IPython.display.display(IPython.display.Audio(Y_orig, rate=44100))\n",
    "            IPython.display.display(IPython.display.Audio(Y_recon, rate=44100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
