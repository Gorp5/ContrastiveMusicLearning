{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T02:15:20.392334600Z",
     "start_time": "2025-10-28T02:14:56.662216Z"
    }
   },
   "source": [
    "from models.PositionalEmbeddings import AttentionClamping\n",
    "from utils.Config import Config\n",
    "from data.data_utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import misc\n",
    "from models.Myna import Myna\n",
    "from training.contrastive_training import train_contrastive\n",
    "\n",
    "model_name = \"Myna-CLS-TEST\"\n",
    "config = Config(\n",
    "        save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "        num_epochs=16,\n",
    "        learning_rate=3e-4,\n",
    "        weight_decay=1e-4,\n",
    "        num_workers=4,\n",
    "        batch_size= 8,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "chunk_size = 256\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"D:\\\\SongsDataset\\\\melspec-mtg-jamendo\\\\train_set\\\\\", chunk_size=chunk_size)\n",
    "test_dataset  = StreamViewDataset(f\"D:\\\\SongsDataset\\\\melspec-mtg-jamendo\\\\test_set\\\\\", chunk_size=chunk_size)\n",
    "\n",
    "prefetch_factor = 1\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=config.num_workers,\n",
    "    # prefetch_factor=prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=config.num_workers,\n",
    "    # prefetch_factor=prefetch_factor,\n",
    ")\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, chunk_size),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    predict_tempo=\"CNN\",\n",
    "    use_sinusoidal=True,\n",
    "    use_y_emb=True,\n",
    "    use_rope_x=True,\n",
    "    use_rope_y=True,\n",
    "    rope_base=512,\n",
    "    use_alibi_x=True,\n",
    "    use_alibi_y=True\n",
    "    #clamping=AttentionClamping(method=\"cap\", cap_type=\"tanh\", cap_value=16, learnable=False)\n",
    ")\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, start_epoch=0, views=2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23277964 Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1242 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d19260f9ac354aa4be666843785677ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa6296-a692-48e4-9157-e21347df09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Myna-CLS-Sinusoid-Stochastic-Length\"\n",
    "config = Config(\n",
    "        save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "        num_epochs=16,\n",
    "        learning_rate=3e-4,\n",
    "        weight_decay=1e-4,\n",
    "        num_workers=2,\n",
    "        batch_size= 256,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "chunk_size = 256\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"D:\\\\SongsDataset\\\\melspec-mtg-jamendo\\\\train_set\\\\\", chunk_size=chunk_size, views=2)\n",
    "test_dataset  = StreamViewDataset(f\"D:\\\\SongsDataset\\\\melspec-mtg-jamendo\\\\test_set\\\\\", chunk_size=chunk_size, views=2)\n",
    "\n",
    "prefetch_factor = 1\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    prefetch_factor=prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    prefetch_factor=prefetch_factor,\n",
    ")\n",
    "\n",
    "model = Myna(\n",
    "    image_size=(128, chunk_size),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"sinusoidal\",\n",
    "    #rope_base=8192\n",
    "    #clamping=AttentionClamping(method=\"cap\", cap_type=\"tanh\", cap_value=16, learnable=False)\n",
    ")\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, start_epoch=0, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83b578fb8b2cf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T08:30:28.711185Z",
     "start_time": "2025-10-23T08:30:28.464395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21428608 Parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc516b0bddd64e388fb349ac2584510b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:10<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd871640e40343b299a5aa9337bc6935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:35<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train: Same Song Contrastive Loss = 5.4035\n",
      "Test: Same Song Contrastive Loss = 5.1766\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a81c69fe019437d8057c48c8c5fba06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:48<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead92cda7ffb4bb991b02b99cb3b04cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:35<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train: Same Song Contrastive Loss = 5.2613\n",
      "Test: Same Song Contrastive Loss = 5.0854\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75055d0138834a53964ca06556fbbffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:35<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75d220e2ff7448f85d349ed04828fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:35<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train: Same Song Contrastive Loss = 5.1631\n",
      "Test: Same Song Contrastive Loss = 4.9340\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb3a5fb5f2e438ab3a3fde860c86518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:37<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b9222cc4324320976d1bbb8c3b07a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:33<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train: Same Song Contrastive Loss = 5.0303\n",
      "Test: Same Song Contrastive Loss = 4.8351\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9880f00aa4df4462ba823fed89d56e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:35<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58095cfadf5487785a7a8e7ca8fac8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:34<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train: Same Song Contrastive Loss = 4.9564\n",
      "Test: Same Song Contrastive Loss = 4.8074\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33008bf11754fb8a94182dd4bae184d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:34<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8894c5c000f34859b525430053691901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:34<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train: Same Song Contrastive Loss = 4.9150\n",
      "Test: Same Song Contrastive Loss = 4.8076\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28dfa672522410a9f4f17f68631924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:35<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Myna(\n",
    "    image_size=(128, chunk_size),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"1D-ALIBI\",\n",
    "    #rope_base=8192\n",
    "    #clamping=AttentionClamping(method=\"cap\", cap_type=\"tanh\", cap_value=16, learnable=False)\n",
    ")\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, start_epoch=0, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3af92c-65c8-4506-9502-b5977301fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Myna(\n",
    "    image_size=(128, chunk_size),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"1D-ALIBI\",\n",
    "    #rope_base=8192\n",
    "    #clamping=AttentionClamping(method=\"cap\", cap_type=\"tanh\", cap_value=16, learnable=False)\n",
    ")\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, start_epoch=0, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5615e18-4c42-4e15-98f3-3f15cf934e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Myna(\n",
    "    image_size=(128, chunk_size),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"2D-ALIBI\",\n",
    "    rope_base=8192\n",
    "    #clamping=AttentionClamping(method=\"cap\", cap_type=\"tanh\", cap_value=16, learnable=False)\n",
    ")\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, start_epoch=0, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b621b-b64d-4a0a-9a63-c4f356c3935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Myna(\n",
    "    image_size=(128, chunk_size),\n",
    "    channels=1,\n",
    "    patch_size=(16, 16),\n",
    "    latent_space=128,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    heads=6,\n",
    "    mlp_dim=1536,\n",
    "    mask_ratio=0.9,\n",
    "    use_cls=True,\n",
    "    positional_encoding=\"2D-ALIBI\",\n",
    "    #rope_base=8192\n",
    "    #clamping=AttentionClamping(method=\"cap\", cap_type=\"tanh\", cap_value=16, learnable=False)\n",
    ")\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, config, start_epoch=0, views=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf574e-8c96-4b7c-bab8-bb48140a32fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
