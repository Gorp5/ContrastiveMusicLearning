{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T05:59:02.509309Z",
     "start_time": "2025-09-21T05:58:54.363975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.processing import ParseBalanced\n",
    "\n",
    "directory = \"large-melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "ParseBalanced(subset_file_name, f\"{data_directory}\", f\"E:/SongsDataset/{directory}\", convert=True, target_per_genre=1300, chunk_size=256, chunks_per_batch=1, write_individually=True)"
   ],
   "id": "60706242edea8ffb",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdata\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ParseBalanced\n\u001B[32m      3\u001B[39m directory = \u001B[33m\"\u001B[39m\u001B[33mlarge-melspec-dataset-top-50-LIBROSA-256-Triplet\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      4\u001B[39m data_directory = \u001B[33m\"\u001B[39m\u001B[33mE:/mtg-jamendo/\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\data\\processing.py:11\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtqdm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmtgjamendodataset\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mscripts\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m commons\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlibrosa\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfeature\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m melspectrogram\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpulp\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LpProblem, LpVariable, lpSum, LpMaximize, LpBinary, LpStatus, value, PULP_CBC_CMD, LpMinimize\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mReadStats\u001B[39m(subset_file_name):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1412\u001B[39m, in \u001B[36m_handle_fromlist\u001B[39m\u001B[34m(module, fromlist, import_, recursive)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\lazy_loader\\__init__.py:82\u001B[39m, in \u001B[36mattach.<locals>.__getattr__\u001B[39m\u001B[34m(name)\u001B[39m\n\u001B[32m     80\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m attr_to_modules:\n\u001B[32m     81\u001B[39m     submod_path = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpackage_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_to_modules[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     submod = \u001B[43mimportlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubmod_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     83\u001B[39m     attr = \u001B[38;5;28mgetattr\u001B[39m(submod, name)\n\u001B[32m     85\u001B[39m     \u001B[38;5;66;03m# If the attribute lives in a file (module) with the same\u001B[39;00m\n\u001B[32m     86\u001B[39m     \u001B[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001B[39;00m\n\u001B[32m     87\u001B[39m     \u001B[38;5;66;03m# the module is accessible on the package.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\librosa\\feature\\spectral.py:7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msignal\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m util\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m filters\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\scipy\\signal\\__init__.py:301\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[33;03m=======================================\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[33;03mSignal processing (:mod:`scipy.signal`)\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    295\u001B[39m \n\u001B[32m    296\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    297\u001B[39m \u001B[38;5;66;03m# bring in the public functionality from private namespaces\u001B[39;00m\n\u001B[32m    298\u001B[39m \n\u001B[32m    299\u001B[39m \u001B[38;5;66;03m# mypy: ignore-errors\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m301\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_support_alternative_backends\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _support_alternative_backends\n\u001B[32m    303\u001B[39m __all__ = _support_alternative_backends.__all__\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\scipy\\signal\\_support_alternative_backends.py:6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfunctools\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_lib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_array_api\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      3\u001B[39m     is_cupy, is_jax, scipy_namespace_for, SCIPY_ARRAY_API\n\u001B[32m      4\u001B[39m )\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_signal_api\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *   \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _signal_api\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _delegators\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\scipy\\signal\\_signal_api.py:9\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"This is the 'bare' scipy.signal API.\u001B[39;00m\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m \u001B[33;03mThis --- private! --- module only collects implementations of public  API\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m      6\u001B[39m \u001B[33;03mre-exports decorated names to __init__.py\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _sigtools, windows         \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_waveforms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *        \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_max_len_seq\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m max_len_seq       \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\scipy\\signal\\windows\\__init__.py:42\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[33;03mWindow functions (:mod:`scipy.signal.windows`)\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[33;03m==============================================\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     39\u001B[39m \n\u001B[32m     40\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_windows\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m     44\u001B[39m \u001B[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m windows\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\scipy\\signal\\windows\\_windows.py:8\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwarnings\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_lib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m doccer\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m linalg, special, fft \u001B[38;5;28;01mas\u001B[39;00m sp_fft\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_lib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01marray_api_compat\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m numpy \u001B[38;5;28;01mas\u001B[39;00m np_compat\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_lib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_array_api\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m array_namespace, xp_device\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1412\u001B[39m, in \u001B[36m_handle_fromlist\u001B[39m\u001B[34m(module, fromlist, import_, recursive)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\scipy\\__init__.py:143\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(name)\u001B[39m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getattr__\u001B[39m(name):\n\u001B[32m    142\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m submodules:\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_importlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mscipy.\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    145\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1387\u001B[39m, in \u001B[36m_gcd_import\u001B[39m\u001B[34m(name, package, level)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1360\u001B[39m, in \u001B[36m_find_and_load\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1331\u001B[39m, in \u001B[36m_find_and_load_unlocked\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:935\u001B[39m, in \u001B[36m_load_unlocked\u001B[39m\u001B[34m(spec)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:991\u001B[39m, in \u001B[36mexec_module\u001B[39m\u001B[34m(self, module)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:1087\u001B[39m, in \u001B[36mget_code\u001B[39m\u001B[34m(self, fullname)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:1187\u001B[39m, in \u001B[36mget_data\u001B[39m\u001B[34m(self, path)\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-21T07:18:16.703254Z",
     "start_time": "2025-09-21T07:18:13.252730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchaudio.transforms import TimeMasking, FrequencyMasking\n",
    "#from loss.ConstrastiveLoss import InfoCNELoss\n",
    "from info_nce import InfoNCE\n",
    "from data.data_utils import *\n",
    "\n",
    "directory = \"large-melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "\n",
    "augmentations = Compose([\n",
    "    AddGaussianNoise(std=0.25),\n",
    "    TimeMasking(time_mask_param=int(0.05* 256)),\n",
    "    FrequencyMasking(freq_mask_param=int(0.05 * 128)),\n",
    "])\n",
    "\n",
    "class Config:\n",
    "    # === General ===\n",
    "    model_name = \"ViT-Contrastive-Custom-Slope\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    save_path = f\"trained_models\\\\{model_name}\\\\\"\n",
    "    seed = 42\n",
    "\n",
    "    # === Training ===\n",
    "    num_classes = 50\n",
    "    num_epochs = 10\n",
    "    batch_size = 64\n",
    "    max_batch_size = 64\n",
    "    learning_rate = 1e-4\n",
    "    min_learning_rate = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    warmup_threshold = 1.0 / 100.0\n",
    "    step_coefficient = 25.0 / 100.0\n",
    "\n",
    "    gamma = 2.0\n",
    "    save_checkpoints = True\n",
    "\n",
    "    # === Dataset ===\n",
    "    transforms = None\n",
    "    use_masks = True\n",
    "    num_workers = 1\n",
    "    prefetch_factor = 3\n",
    "    val_split = 0.1\n",
    "    shuffle = True\n",
    "    pos_weight = (torch.ones(num_classes) * 50).to(\"cuda\")\n",
    "    criterion = InfoNCE()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T07:18:17.247647Z",
     "start_time": "2025-09-21T07:18:17.235261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "large_directory = directory\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"E:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\data\", f\"E:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\genre_labels\")\n",
    "test_dataset  = StreamViewDataset(f\"E:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\data\", f\"E:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\genre_labels\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")"
   ],
   "id": "25b92472f73db5a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T18:57:45.174304Z",
     "start_time": "2025-09-13T02:25:19.494259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.AudioTransformer import AudioTransformer\n",
    "from utils import misc\n",
    "\n",
    "model = AudioTransformer(latent_space=32, input_dim=128, d_model=256, dim_feedforward=512, length=256, num_heads=8, encoder_layers=16, decoder_layers=16, dropout=0.1, use_alibi=True, custom_slopes=True)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "b699c79369ea9dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25462368 Parameters\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T18:57:45.175302100Z",
     "start_time": "2025-09-13T07:48:26.318845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.AudioViT import AudioViT\n",
    "from utils import misc\n",
    "\n",
    "model = AudioViT(latent_space=32, input_dim=128, d_model=256, length=256, num_heads=8, encoder_layers=8, decoder_layers=8, dropout=0.1, use_alibi=True, use_rope=True)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "e6022a934522412a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23554720 Parameters\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.AudioPreCNNTransformer import AudioPreCNNTransformer\n",
    "from utils import misc\n",
    "\n",
    "model = AudioPreCNNTransformer(latent_space=512, input_dim=128, length=1024, num_heads=8, transformer_layers=8, d_model=256, dropout=0.1)\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "3cc0b5351b3b35fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.AudioTransformerWeaved import AudioTransformerWeaved\n",
    "from utils import misc\n",
    "\n",
    "model = AudioTransformerWeaved(latent_space=128, input_dim=128, d_model=256, dim_feedforward=512, length=1024, num_heads=8, encoder_layers=8, decoder_layers=8, dropout=0.1, use_alibi=True)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "92dabd0320a8ecaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T07:12:39.989128Z",
     "start_time": "2025-09-21T07:12:39.845147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.AudioViTEncoder import AudioViTEncoder\n",
    "from utils import misc\n",
    "\n",
    "model = AudioViTEncoder(latent_space=32, input_dim=128, d_model=256, length=256, num_heads=8, encoder_layers=8, decoder_layers=8, dropout=0.1, use_alibi=True, use_rope=True)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "4affc8e27387dce9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13792256 Parameters\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T07:13:10.352377Z",
     "start_time": "2025-09-21T07:12:40.114619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training.contrastive_training import train_contrastive\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ],
   "id": "3fca8edb9b282363",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "919343df81b14bb18ad3a33c64262636"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x128 and 256x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtraining\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcontrastive_training\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_contrastive\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mtrain_contrastive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_graph\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\contrastive_training.py:37\u001B[39m, in \u001B[36mtrain_contrastive\u001B[39m\u001B[34m(model, test_dataloader, train_dataloader, config, show_graph)\u001B[39m\n\u001B[32m     34\u001B[39m a = a.to(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m, config.dtype)\n\u001B[32m     35\u001B[39m b = b.to(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m, config.dtype)\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m za = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     38\u001B[39m zb = model(b)\n\u001B[32m     40\u001B[39m loss = criterion(za, zb)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\AudioTransformerWeavedEmbedding.py:72\u001B[39m, in \u001B[36mAudioTransformerWeavedEmbedding.forward\u001B[39m\u001B[34m(self, x, mask)\u001B[39m\n\u001B[32m     69\u001B[39m memory = \u001B[38;5;28mself\u001B[39m.encode_to_latent_gelu(memory)\n\u001B[32m     71\u001B[39m \u001B[38;5;66;03m# Project to Output Dimension\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m memory = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfc_out\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m memory = \u001B[38;5;28mself\u001B[39m.fc_gelu(memory)\n\u001B[32m     75\u001B[39m memory = memory.permute(\u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m, \u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: mat1 and mat2 shapes cannot be multiplied (64x128 and 256x128)"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T07:18:24.060033Z",
     "start_time": "2025-09-21T07:18:23.622033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.AudioTransformerEmbeddings import AudioTransformerEmbeddings\n",
    "from utils import misc\n",
    "\n",
    "model = AudioTransformerEmbeddings(latent_space=128, input_dim=128, d_model=256, dim_feedforward=512, length=256, num_heads=8, encoder_layers=8, dropout=0.1, use_alibi=True, use_rope=True, custom_slope=1)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "cb45f085270d259a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24468914 Parameters\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T11:41:52.022857Z",
     "start_time": "2025-09-21T07:18:24.138033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training.contrastive_training import train_contrastive\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ],
   "id": "7dd2c33162f0f100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9aa1e546ccc545eba308422b8bd83edc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 4.1884\n",
      "\n",
      "Batch Loss [2/155]: 4.1526\n",
      "\n",
      "Batch Loss [3/155]: 4.1499\n",
      "\n",
      "Batch Loss [4/155]: 4.1457\n",
      "\n",
      "Batch Loss [5/155]: 4.1451\n",
      "\n",
      "Batch Loss [6/155]: 4.1244\n",
      "\n",
      "Batch Loss [7/155]: 4.0774\n",
      "\n",
      "Batch Loss [8/155]: 3.9222\n",
      "\n",
      "Batch Loss [9/155]: 4.3615\n",
      "\n",
      "Batch Loss [10/155]: 4.0819\n",
      "\n",
      "Batch Loss [11/155]: 4.1155\n",
      "\n",
      "Batch Loss [12/155]: 4.1398\n",
      "\n",
      "Batch Loss [13/155]: 4.1480\n",
      "\n",
      "Batch Loss [14/155]: 4.1465\n",
      "\n",
      "Batch Loss [15/155]: 4.1500\n",
      "\n",
      "Batch Loss [16/155]: 4.1527\n",
      "\n",
      "Batch Loss [17/155]: 4.1539\n",
      "\n",
      "Batch Loss [18/155]: 4.1544\n",
      "\n",
      "Batch Loss [19/155]: 4.1559\n",
      "\n",
      "Batch Loss [20/155]: 4.1561\n",
      "\n",
      "Batch Loss [21/155]: 4.1558\n",
      "\n",
      "Batch Loss [22/155]: 4.1562\n",
      "\n",
      "Batch Loss [23/155]: 4.1552\n",
      "\n",
      "Batch Loss [24/155]: 4.1561\n",
      "\n",
      "Batch Loss [25/155]: 4.1577\n",
      "\n",
      "Batch Loss [26/155]: 4.1569\n",
      "\n",
      "Batch Loss [27/155]: 4.1568\n",
      "\n",
      "Batch Loss [28/155]: 4.1564\n",
      "\n",
      "Batch Loss [29/155]: 4.1559\n",
      "\n",
      "Batch Loss [30/155]: 4.1561\n",
      "\n",
      "Batch Loss [31/155]: 4.1563\n",
      "\n",
      "Batch Loss [32/155]: 4.1561\n",
      "\n",
      "Batch Loss [33/155]: 4.1565\n",
      "\n",
      "Batch Loss [34/155]: 4.1563\n",
      "\n",
      "Batch Loss [35/155]: 4.1563\n",
      "\n",
      "Batch Loss [36/155]: 4.1568\n",
      "\n",
      "Batch Loss [37/155]: 4.1569\n",
      "\n",
      "Batch Loss [38/155]: 4.1558\n",
      "\n",
      "Batch Loss [39/155]: 4.1562\n",
      "\n",
      "Batch Loss [40/155]: 4.1553\n",
      "\n",
      "Batch Loss [41/155]: 4.1555\n",
      "\n",
      "Batch Loss [42/155]: 4.1550\n",
      "\n",
      "Batch Loss [43/155]: 4.1555\n",
      "\n",
      "Batch Loss [44/155]: 4.1554\n",
      "\n",
      "Batch Loss [45/155]: 4.1562\n",
      "\n",
      "Batch Loss [46/155]: 4.1550\n",
      "\n",
      "Batch Loss [47/155]: 4.1543\n",
      "\n",
      "Batch Loss [48/155]: 4.1539\n",
      "\n",
      "Batch Loss [49/155]: 4.1539\n",
      "\n",
      "Batch Loss [50/155]: 4.1550\n",
      "\n",
      "Batch Loss [51/155]: 4.1537\n",
      "\n",
      "Batch Loss [52/155]: 4.1539\n",
      "\n",
      "Batch Loss [53/155]: 4.1541\n",
      "\n",
      "Batch Loss [54/155]: 4.1539\n",
      "\n",
      "Batch Loss [55/155]: 4.1526\n",
      "\n",
      "Batch Loss [56/155]: 4.1509\n",
      "\n",
      "Batch Loss [57/155]: 4.1465\n",
      "\n",
      "Batch Loss [58/155]: 4.1467\n",
      "\n",
      "Batch Loss [59/155]: 4.1425\n",
      "\n",
      "Batch Loss [60/155]: 4.1491\n",
      "\n",
      "Batch Loss [61/155]: 4.1332\n",
      "\n",
      "Batch Loss [62/155]: 4.1272\n",
      "\n",
      "Batch Loss [63/155]: 4.0746\n",
      "\n",
      "Batch Loss [64/155]: 3.9863\n",
      "\n",
      "Batch Loss [65/155]: 3.9929\n",
      "\n",
      "Batch Loss [66/155]: 3.9440\n",
      "\n",
      "Batch Loss [67/155]: 3.8707\n",
      "\n",
      "Batch Loss [68/155]: 4.0553\n",
      "\n",
      "Batch Loss [69/155]: 4.0343\n",
      "\n",
      "Batch Loss [70/155]: 3.9486\n",
      "\n",
      "Batch Loss [71/155]: 3.8750\n",
      "\n",
      "Batch Loss [72/155]: 4.0263\n",
      "\n",
      "Batch Loss [73/155]: 4.0214\n",
      "\n",
      "Batch Loss [74/155]: 4.0312\n",
      "\n",
      "Batch Loss [75/155]: 3.9467\n",
      "\n",
      "Batch Loss [76/155]: 3.9814\n",
      "\n",
      "Batch Loss [77/155]: 3.7383\n",
      "\n",
      "Batch Loss [78/155]: 3.7977\n",
      "\n",
      "Batch Loss [79/155]: 4.0046\n",
      "\n",
      "Batch Loss [80/155]: 3.9948\n",
      "\n",
      "Batch Loss [81/155]: 4.0513\n",
      "\n",
      "Batch Loss [82/155]: 4.1202\n",
      "\n",
      "Batch Loss [83/155]: 4.0745\n",
      "\n",
      "Batch Loss [84/155]: 4.0703\n",
      "\n",
      "Batch Loss [85/155]: 4.0658\n",
      "\n",
      "Batch Loss [86/155]: 4.0850\n",
      "\n",
      "Batch Loss [87/155]: 3.9978\n",
      "\n",
      "Batch Loss [88/155]: 4.0658\n",
      "\n",
      "Batch Loss [89/155]: 4.0168\n",
      "\n",
      "Batch Loss [90/155]: 3.8858\n",
      "\n",
      "Batch Loss [91/155]: 3.9800\n",
      "\n",
      "Batch Loss [92/155]: 4.0764\n",
      "\n",
      "Batch Loss [93/155]: 3.9107\n",
      "\n",
      "Batch Loss [94/155]: 3.9839\n",
      "\n",
      "Batch Loss [95/155]: 3.9541\n",
      "\n",
      "Batch Loss [96/155]: 3.9571\n",
      "\n",
      "Batch Loss [97/155]: 3.9110\n",
      "\n",
      "Batch Loss [98/155]: 3.8347\n",
      "\n",
      "Batch Loss [99/155]: 3.7527\n",
      "\n",
      "Batch Loss [100/155]: 3.4408\n",
      "\n",
      "Batch Loss [101/155]: 4.0810\n",
      "\n",
      "Batch Loss [102/155]: 4.1859\n",
      "\n",
      "Batch Loss [103/155]: 3.9457\n",
      "\n",
      "Batch Loss [104/155]: 3.9882\n",
      "\n",
      "Batch Loss [105/155]: 3.9420\n",
      "\n",
      "Batch Loss [106/155]: 4.0608\n",
      "\n",
      "Batch Loss [107/155]: 4.0328\n",
      "\n",
      "Batch Loss [108/155]: 4.0525\n",
      "\n",
      "Batch Loss [109/155]: 4.0374\n",
      "\n",
      "Batch Loss [110/155]: 4.0215\n",
      "\n",
      "Batch Loss [111/155]: 3.9854\n",
      "\n",
      "Batch Loss [112/155]: 3.9474\n",
      "\n",
      "Batch Loss [113/155]: 3.9874\n",
      "\n",
      "Batch Loss [114/155]: 3.8409\n",
      "\n",
      "Batch Loss [115/155]: 3.8489\n",
      "\n",
      "Batch Loss [116/155]: 3.7948\n",
      "\n",
      "Batch Loss [117/155]: 3.8000\n",
      "\n",
      "Batch Loss [118/155]: 3.7650\n",
      "\n",
      "Batch Loss [119/155]: 3.8730\n",
      "\n",
      "Batch Loss [120/155]: 3.9338\n",
      "\n",
      "Batch Loss [121/155]: 3.9658\n",
      "\n",
      "Batch Loss [122/155]: 4.0522\n",
      "\n",
      "Batch Loss [123/155]: 3.9474\n",
      "\n",
      "Batch Loss [124/155]: 4.0442\n",
      "\n",
      "Batch Loss [125/155]: 3.9858\n",
      "\n",
      "Batch Loss [126/155]: 4.0524\n",
      "\n",
      "Batch Loss [127/155]: 3.9595\n",
      "\n",
      "Batch Loss [128/155]: 3.8818\n",
      "\n",
      "Batch Loss [129/155]: 3.9462\n",
      "\n",
      "Batch Loss [130/155]: 3.9088\n",
      "\n",
      "Batch Loss [131/155]: 3.9313\n",
      "\n",
      "Batch Loss [132/155]: 3.9601\n",
      "\n",
      "Batch Loss [133/155]: 4.0767\n",
      "\n",
      "Batch Loss [134/155]: 3.8393\n",
      "\n",
      "Batch Loss [135/155]: 4.0059\n",
      "\n",
      "Batch Loss [136/155]: 3.9251\n",
      "\n",
      "Batch Loss [137/155]: 3.9653\n",
      "\n",
      "Batch Loss [138/155]: 3.9600\n",
      "\n",
      "Batch Loss [139/155]: 3.7971\n",
      "\n",
      "Batch Loss [140/155]: 3.8096\n",
      "\n",
      "Batch Loss [141/155]: 3.7529\n",
      "\n",
      "Batch Loss [142/155]: 3.7373\n",
      "\n",
      "Batch Loss [143/155]: 3.9825\n",
      "\n",
      "Batch Loss [144/155]: 3.8728\n",
      "\n",
      "Batch Loss [145/155]: 3.8514\n",
      "\n",
      "Batch Loss [146/155]: 4.0008\n",
      "\n",
      "Batch Loss [147/155]: 4.0767\n",
      "\n",
      "Batch Loss [148/155]: 3.9196\n",
      "\n",
      "Batch Loss [149/155]: 3.8896\n",
      "\n",
      "Batch Loss [150/155]: 3.8485\n",
      "\n",
      "Batch Loss [151/155]: 3.6658\n",
      "\n",
      "Batch Loss [152/155]: 3.6839\n",
      "\n",
      "Batch Loss [153/155]: 3.6684\n",
      "\n",
      "Batch Loss [154/155]: 3.9374\n",
      "\n",
      "Batch Loss [155/155]: 3.7462\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6343421a289f43abaa981b71f4892283"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train:  4.0232\n",
      "Test:  Contrastive Loss: 3.7577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5b3d2867cb447bc88741f182f39edd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 3.8405\n",
      "\n",
      "Batch Loss [2/155]: 3.7926\n",
      "\n",
      "Batch Loss [3/155]: 3.7873\n",
      "\n",
      "Batch Loss [4/155]: 3.8034\n",
      "\n",
      "Batch Loss [5/155]: 3.8721\n",
      "\n",
      "Batch Loss [6/155]: 4.0102\n",
      "\n",
      "Batch Loss [7/155]: 4.0158\n",
      "\n",
      "Batch Loss [8/155]: 3.7368\n",
      "\n",
      "Batch Loss [9/155]: 3.8911\n",
      "\n",
      "Batch Loss [10/155]: 3.8592\n",
      "\n",
      "Batch Loss [11/155]: 3.7474\n",
      "\n",
      "Batch Loss [12/155]: 3.5344\n",
      "\n",
      "Batch Loss [13/155]: 3.5241\n",
      "\n",
      "Batch Loss [14/155]: 3.6664\n",
      "\n",
      "Batch Loss [15/155]: 3.3324\n",
      "\n",
      "Batch Loss [16/155]: 3.7630\n",
      "\n",
      "Batch Loss [17/155]: 3.6764\n",
      "\n",
      "Batch Loss [18/155]: 3.5998\n",
      "\n",
      "Batch Loss [19/155]: 3.8721\n",
      "\n",
      "Batch Loss [20/155]: 3.8764\n",
      "\n",
      "Batch Loss [21/155]: 3.8514\n",
      "\n",
      "Batch Loss [22/155]: 3.6784\n",
      "\n",
      "Batch Loss [23/155]: 3.7626\n",
      "\n",
      "Batch Loss [24/155]: 3.5968\n",
      "\n",
      "Batch Loss [25/155]: 3.7355\n",
      "\n",
      "Batch Loss [26/155]: 3.9297\n",
      "\n",
      "Batch Loss [27/155]: 3.5766\n",
      "\n",
      "Batch Loss [28/155]: 3.8647\n",
      "\n",
      "Batch Loss [29/155]: 3.5471\n",
      "\n",
      "Batch Loss [30/155]: 3.5490\n",
      "\n",
      "Batch Loss [31/155]: 3.6468\n",
      "\n",
      "Batch Loss [32/155]: 3.5345\n",
      "\n",
      "Batch Loss [33/155]: 3.7559\n",
      "\n",
      "Batch Loss [34/155]: 3.6981\n",
      "\n",
      "Batch Loss [35/155]: 4.0647\n",
      "\n",
      "Batch Loss [36/155]: 3.7793\n",
      "\n",
      "Batch Loss [37/155]: 3.6890\n",
      "\n",
      "Batch Loss [38/155]: 3.6224\n",
      "\n",
      "Batch Loss [39/155]: 3.7178\n",
      "\n",
      "Batch Loss [40/155]: 3.7934\n",
      "\n",
      "Batch Loss [41/155]: 3.4907\n",
      "\n",
      "Batch Loss [42/155]: 3.7832\n",
      "\n",
      "Batch Loss [43/155]: 3.6909\n",
      "\n",
      "Batch Loss [44/155]: 3.5885\n",
      "\n",
      "Batch Loss [45/155]: 3.6406\n",
      "\n",
      "Batch Loss [46/155]: 3.9315\n",
      "\n",
      "Batch Loss [47/155]: 3.6820\n",
      "\n",
      "Batch Loss [48/155]: 3.7475\n",
      "\n",
      "Batch Loss [49/155]: 3.7584\n",
      "\n",
      "Batch Loss [50/155]: 3.6005\n",
      "\n",
      "Batch Loss [51/155]: 3.6934\n",
      "\n",
      "Batch Loss [52/155]: 3.4721\n",
      "\n",
      "Batch Loss [53/155]: 3.6507\n",
      "\n",
      "Batch Loss [54/155]: 3.7277\n",
      "\n",
      "Batch Loss [55/155]: 3.6477\n",
      "\n",
      "Batch Loss [56/155]: 3.8354\n",
      "\n",
      "Batch Loss [57/155]: 3.6338\n",
      "\n",
      "Batch Loss [58/155]: 3.6490\n",
      "\n",
      "Batch Loss [59/155]: 3.5684\n",
      "\n",
      "Batch Loss [60/155]: 3.4085\n",
      "\n",
      "Batch Loss [61/155]: 3.5853\n",
      "\n",
      "Batch Loss [62/155]: 3.2763\n",
      "\n",
      "Batch Loss [63/155]: 3.5417\n",
      "\n",
      "Batch Loss [64/155]: 3.5460\n",
      "\n",
      "Batch Loss [65/155]: 3.1080\n",
      "\n",
      "Batch Loss [66/155]: 3.2103\n",
      "\n",
      "Batch Loss [67/155]: 3.7387\n",
      "\n",
      "Batch Loss [68/155]: 3.9368\n",
      "\n",
      "Batch Loss [69/155]: 3.4036\n",
      "\n",
      "Batch Loss [70/155]: 3.4740\n",
      "\n",
      "Batch Loss [71/155]: 3.6767\n",
      "\n",
      "Batch Loss [72/155]: 3.7187\n",
      "\n",
      "Batch Loss [73/155]: 3.3269\n",
      "\n",
      "Batch Loss [74/155]: 3.3780\n",
      "\n",
      "Batch Loss [75/155]: 3.3574\n",
      "\n",
      "Batch Loss [76/155]: 3.4027\n",
      "\n",
      "Batch Loss [77/155]: 3.2026\n",
      "\n",
      "Batch Loss [78/155]: 3.4303\n",
      "\n",
      "Batch Loss [79/155]: 3.4988\n",
      "\n",
      "Batch Loss [80/155]: 3.3723\n",
      "\n",
      "Batch Loss [81/155]: 3.1750\n",
      "\n",
      "Batch Loss [82/155]: 3.2709\n",
      "\n",
      "Batch Loss [83/155]: 3.1098\n",
      "\n",
      "Batch Loss [84/155]: 3.4578\n",
      "\n",
      "Batch Loss [85/155]: 3.2102\n",
      "\n",
      "Batch Loss [86/155]: 3.3647\n",
      "\n",
      "Batch Loss [87/155]: 3.5845\n",
      "\n",
      "Batch Loss [88/155]: 3.5692\n",
      "\n",
      "Batch Loss [89/155]: 3.3620\n",
      "\n",
      "Batch Loss [90/155]: 3.5830\n",
      "\n",
      "Batch Loss [91/155]: 3.3881\n",
      "\n",
      "Batch Loss [92/155]: 3.6218\n",
      "\n",
      "Batch Loss [93/155]: 3.3686\n",
      "\n",
      "Batch Loss [94/155]: 3.6865\n",
      "\n",
      "Batch Loss [95/155]: 3.2398\n",
      "\n",
      "Batch Loss [96/155]: 3.4324\n",
      "\n",
      "Batch Loss [97/155]: 3.1884\n",
      "\n",
      "Batch Loss [98/155]: 3.8176\n",
      "\n",
      "Batch Loss [99/155]: 3.3934\n",
      "\n",
      "Batch Loss [100/155]: 2.8666\n",
      "\n",
      "Batch Loss [101/155]: 3.2712\n",
      "\n",
      "Batch Loss [102/155]: 3.1659\n",
      "\n",
      "Batch Loss [103/155]: 3.6295\n",
      "\n",
      "Batch Loss [104/155]: 3.6632\n",
      "\n",
      "Batch Loss [105/155]: 3.2405\n",
      "\n",
      "Batch Loss [106/155]: 3.5190\n",
      "\n",
      "Batch Loss [107/155]: 3.4405\n",
      "\n",
      "Batch Loss [108/155]: 3.3013\n",
      "\n",
      "Batch Loss [109/155]: 3.3832\n",
      "\n",
      "Batch Loss [110/155]: 3.4661\n",
      "\n",
      "Batch Loss [111/155]: 3.7072\n",
      "\n",
      "Batch Loss [112/155]: 3.3487\n",
      "\n",
      "Batch Loss [113/155]: 3.1176\n",
      "\n",
      "Batch Loss [114/155]: 3.4666\n",
      "\n",
      "Batch Loss [115/155]: 3.4523\n",
      "\n",
      "Batch Loss [116/155]: 3.2298\n",
      "\n",
      "Batch Loss [117/155]: 3.2019\n",
      "\n",
      "Batch Loss [118/155]: 3.3314\n",
      "\n",
      "Batch Loss [119/155]: 3.4808\n",
      "\n",
      "Batch Loss [120/155]: 3.2123\n",
      "\n",
      "Batch Loss [121/155]: 3.2914\n",
      "\n",
      "Batch Loss [122/155]: 3.6793\n",
      "\n",
      "Batch Loss [123/155]: 3.0981\n",
      "\n",
      "Batch Loss [124/155]: 3.3360\n",
      "\n",
      "Batch Loss [125/155]: 3.2974\n",
      "\n",
      "Batch Loss [126/155]: 3.3808\n",
      "\n",
      "Batch Loss [127/155]: 3.1628\n",
      "\n",
      "Batch Loss [128/155]: 2.9855\n",
      "\n",
      "Batch Loss [129/155]: 3.4864\n",
      "\n",
      "Batch Loss [130/155]: 3.3102\n",
      "\n",
      "Batch Loss [131/155]: 3.0771\n",
      "\n",
      "Batch Loss [132/155]: 3.1786\n",
      "\n",
      "Batch Loss [133/155]: 3.0972\n",
      "\n",
      "Batch Loss [134/155]: 3.2650\n",
      "\n",
      "Batch Loss [135/155]: 3.0601\n",
      "\n",
      "Batch Loss [136/155]: 3.3043\n",
      "\n",
      "Batch Loss [137/155]: 3.0702\n",
      "\n",
      "Batch Loss [138/155]: 3.2727\n",
      "\n",
      "Batch Loss [139/155]: 3.1825\n",
      "\n",
      "Batch Loss [140/155]: 3.3088\n",
      "\n",
      "Batch Loss [141/155]: 3.3669\n",
      "\n",
      "Batch Loss [142/155]: 3.2111\n",
      "\n",
      "Batch Loss [143/155]: 3.1938\n",
      "\n",
      "Batch Loss [144/155]: 2.9341\n",
      "\n",
      "Batch Loss [145/155]: 3.2708\n",
      "\n",
      "Batch Loss [146/155]: 2.9624\n",
      "\n",
      "Batch Loss [147/155]: 3.0739\n",
      "\n",
      "Batch Loss [148/155]: 3.6167\n",
      "\n",
      "Batch Loss [149/155]: 3.3058\n",
      "\n",
      "Batch Loss [150/155]: 3.0066\n",
      "\n",
      "Batch Loss [151/155]: 3.3644\n",
      "\n",
      "Batch Loss [152/155]: 2.9852\n",
      "\n",
      "Batch Loss [153/155]: 3.1606\n",
      "\n",
      "Batch Loss [154/155]: 3.3890\n",
      "\n",
      "Batch Loss [155/155]: 2.9728\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af5ea666791641d6a4825dfc610029c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train:  3.4836\n",
      "Test:  Contrastive Loss: 3.1360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf63859a6d8144e28f27eed1be0a9b43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 3.2981\n",
      "\n",
      "Batch Loss [2/155]: 3.5625\n",
      "\n",
      "Batch Loss [3/155]: 3.6153\n",
      "\n",
      "Batch Loss [4/155]: 3.3741\n",
      "\n",
      "Batch Loss [5/155]: 3.1808\n",
      "\n",
      "Batch Loss [6/155]: 3.4592\n",
      "\n",
      "Batch Loss [7/155]: 2.8864\n",
      "\n",
      "Batch Loss [8/155]: 3.4723\n",
      "\n",
      "Batch Loss [9/155]: 3.4055\n",
      "\n",
      "Batch Loss [10/155]: 3.0117\n",
      "\n",
      "Batch Loss [11/155]: 3.3070\n",
      "\n",
      "Batch Loss [12/155]: 3.2370\n",
      "\n",
      "Batch Loss [13/155]: 3.1811\n",
      "\n",
      "Batch Loss [14/155]: 3.1600\n",
      "\n",
      "Batch Loss [15/155]: 3.1813\n",
      "\n",
      "Batch Loss [16/155]: 3.0550\n",
      "\n",
      "Batch Loss [17/155]: 3.2667\n",
      "\n",
      "Batch Loss [18/155]: 2.9552\n",
      "\n",
      "Batch Loss [19/155]: 3.3671\n",
      "\n",
      "Batch Loss [20/155]: 3.1685\n",
      "\n",
      "Batch Loss [21/155]: 3.5511\n",
      "\n",
      "Batch Loss [22/155]: 3.0719\n",
      "\n",
      "Batch Loss [23/155]: 3.1001\n",
      "\n",
      "Batch Loss [24/155]: 2.8897\n",
      "\n",
      "Batch Loss [25/155]: 2.9194\n",
      "\n",
      "Batch Loss [26/155]: 3.0195\n",
      "\n",
      "Batch Loss [27/155]: 3.5323\n",
      "\n",
      "Batch Loss [28/155]: 3.1495\n",
      "\n",
      "Batch Loss [29/155]: 2.9395\n",
      "\n",
      "Batch Loss [30/155]: 3.2017\n",
      "\n",
      "Batch Loss [31/155]: 2.7772\n",
      "\n",
      "Batch Loss [32/155]: 3.0174\n",
      "\n",
      "Batch Loss [33/155]: 3.1835\n",
      "\n",
      "Batch Loss [34/155]: 3.0413\n",
      "\n",
      "Batch Loss [35/155]: 3.2876\n",
      "\n",
      "Batch Loss [36/155]: 3.1938\n",
      "\n",
      "Batch Loss [37/155]: 3.1401\n",
      "\n",
      "Batch Loss [38/155]: 3.0980\n",
      "\n",
      "Batch Loss [39/155]: 2.9782\n",
      "\n",
      "Batch Loss [40/155]: 2.7336\n",
      "\n",
      "Batch Loss [41/155]: 3.3673\n",
      "\n",
      "Batch Loss [42/155]: 3.2936\n",
      "\n",
      "Batch Loss [43/155]: 3.2331\n",
      "\n",
      "Batch Loss [44/155]: 3.2964\n",
      "\n",
      "Batch Loss [45/155]: 2.9206\n",
      "\n",
      "Batch Loss [46/155]: 3.2337\n",
      "\n",
      "Batch Loss [47/155]: 3.0212\n",
      "\n",
      "Batch Loss [48/155]: 3.2799\n",
      "\n",
      "Batch Loss [49/155]: 3.1634\n",
      "\n",
      "Batch Loss [50/155]: 3.5913\n",
      "\n",
      "Batch Loss [51/155]: 3.1762\n",
      "\n",
      "Batch Loss [52/155]: 3.6919\n",
      "\n",
      "Batch Loss [53/155]: 3.1531\n",
      "\n",
      "Batch Loss [54/155]: 3.0961\n",
      "\n",
      "Batch Loss [55/155]: 3.5815\n",
      "\n",
      "Batch Loss [56/155]: 3.4021\n",
      "\n",
      "Batch Loss [57/155]: 3.6006\n",
      "\n",
      "Batch Loss [58/155]: 3.1365\n",
      "\n",
      "Batch Loss [59/155]: 3.0174\n",
      "\n",
      "Batch Loss [60/155]: 3.1214\n",
      "\n",
      "Batch Loss [61/155]: 3.4548\n",
      "\n",
      "Batch Loss [62/155]: 3.1038\n",
      "\n",
      "Batch Loss [63/155]: 3.2877\n",
      "\n",
      "Batch Loss [64/155]: 2.8440\n",
      "\n",
      "Batch Loss [65/155]: 2.9309\n",
      "\n",
      "Batch Loss [66/155]: 3.2763\n",
      "\n",
      "Batch Loss [67/155]: 3.4944\n",
      "\n",
      "Batch Loss [68/155]: 3.4272\n",
      "\n",
      "Batch Loss [69/155]: 3.5388\n",
      "\n",
      "Batch Loss [70/155]: 3.0841\n",
      "\n",
      "Batch Loss [71/155]: 3.4046\n",
      "\n",
      "Batch Loss [72/155]: 3.3335\n",
      "\n",
      "Batch Loss [73/155]: 3.0851\n",
      "\n",
      "Batch Loss [74/155]: 3.3517\n",
      "\n",
      "Batch Loss [75/155]: 3.2668\n",
      "\n",
      "Batch Loss [76/155]: 3.2572\n",
      "\n",
      "Batch Loss [77/155]: 2.7621\n",
      "\n",
      "Batch Loss [78/155]: 2.7548\n",
      "\n",
      "Batch Loss [79/155]: 3.3319\n",
      "\n",
      "Batch Loss [80/155]: 3.4291\n",
      "\n",
      "Batch Loss [81/155]: 3.1366\n",
      "\n",
      "Batch Loss [82/155]: 3.2973\n",
      "\n",
      "Batch Loss [83/155]: 3.1740\n",
      "\n",
      "Batch Loss [84/155]: 3.0433\n",
      "\n",
      "Batch Loss [85/155]: 3.0280\n",
      "\n",
      "Batch Loss [86/155]: 2.8314\n",
      "\n",
      "Batch Loss [87/155]: 3.3334\n",
      "\n",
      "Batch Loss [88/155]: 3.1777\n",
      "\n",
      "Batch Loss [89/155]: 3.2585\n",
      "\n",
      "Batch Loss [90/155]: 3.4458\n",
      "\n",
      "Batch Loss [91/155]: 3.2056\n",
      "\n",
      "Batch Loss [92/155]: 2.9531\n",
      "\n",
      "Batch Loss [93/155]: 3.1225\n",
      "\n",
      "Batch Loss [94/155]: 3.1614\n",
      "\n",
      "Batch Loss [95/155]: 3.0616\n",
      "\n",
      "Batch Loss [96/155]: 3.1621\n",
      "\n",
      "Batch Loss [97/155]: 2.9829\n",
      "\n",
      "Batch Loss [98/155]: 3.2459\n",
      "\n",
      "Batch Loss [99/155]: 3.1581\n",
      "\n",
      "Batch Loss [100/155]: 3.1748\n",
      "\n",
      "Batch Loss [101/155]: 2.9519\n",
      "\n",
      "Batch Loss [102/155]: 3.1539\n",
      "\n",
      "Batch Loss [103/155]: 2.9844\n",
      "\n",
      "Batch Loss [104/155]: 2.9693\n",
      "\n",
      "Batch Loss [105/155]: 3.5211\n",
      "\n",
      "Batch Loss [106/155]: 3.2048\n",
      "\n",
      "Batch Loss [107/155]: 3.4197\n",
      "\n",
      "Batch Loss [108/155]: 3.3968\n",
      "\n",
      "Batch Loss [109/155]: 3.1958\n",
      "\n",
      "Batch Loss [110/155]: 3.1520\n",
      "\n",
      "Batch Loss [111/155]: 2.9512\n",
      "\n",
      "Batch Loss [112/155]: 2.5596\n",
      "\n",
      "Batch Loss [113/155]: 2.7844\n",
      "\n",
      "Batch Loss [114/155]: 2.9314\n",
      "\n",
      "Batch Loss [115/155]: 3.3839\n",
      "\n",
      "Batch Loss [116/155]: 3.3133\n",
      "\n",
      "Batch Loss [117/155]: 3.2197\n",
      "\n",
      "Batch Loss [118/155]: 3.2418\n",
      "\n",
      "Batch Loss [119/155]: 3.0936\n",
      "\n",
      "Batch Loss [120/155]: 3.4394\n",
      "\n",
      "Batch Loss [121/155]: 2.9573\n",
      "\n",
      "Batch Loss [122/155]: 3.0547\n",
      "\n",
      "Batch Loss [123/155]: 3.4339\n",
      "\n",
      "Batch Loss [124/155]: 3.3868\n",
      "\n",
      "Batch Loss [125/155]: 3.3560\n",
      "\n",
      "Batch Loss [126/155]: 3.2852\n",
      "\n",
      "Batch Loss [127/155]: 2.9606\n",
      "\n",
      "Batch Loss [128/155]: 3.2791\n",
      "\n",
      "Batch Loss [129/155]: 2.8544\n",
      "\n",
      "Batch Loss [130/155]: 3.0353\n",
      "\n",
      "Batch Loss [131/155]: 3.3236\n",
      "\n",
      "Batch Loss [132/155]: 3.3097\n",
      "\n",
      "Batch Loss [133/155]: 3.1171\n",
      "\n",
      "Batch Loss [134/155]: 2.8828\n",
      "\n",
      "Batch Loss [135/155]: 3.1191\n",
      "\n",
      "Batch Loss [136/155]: 3.5413\n",
      "\n",
      "Batch Loss [137/155]: 3.4433\n",
      "\n",
      "Batch Loss [138/155]: 3.1798\n",
      "\n",
      "Batch Loss [139/155]: 3.0834\n",
      "\n",
      "Batch Loss [140/155]: 3.4915\n",
      "\n",
      "Batch Loss [141/155]: 3.2886\n",
      "\n",
      "Batch Loss [142/155]: 3.0800\n",
      "\n",
      "Batch Loss [143/155]: 3.2358\n",
      "\n",
      "Batch Loss [144/155]: 3.1841\n",
      "\n",
      "Batch Loss [145/155]: 3.1192\n",
      "\n",
      "Batch Loss [146/155]: 2.9474\n",
      "\n",
      "Batch Loss [147/155]: 3.4960\n",
      "\n",
      "Batch Loss [148/155]: 3.3035\n",
      "\n",
      "Batch Loss [149/155]: 3.2432\n",
      "\n",
      "Batch Loss [150/155]: 3.3894\n",
      "\n",
      "Batch Loss [151/155]: 3.0586\n",
      "\n",
      "Batch Loss [152/155]: 2.9926\n",
      "\n",
      "Batch Loss [153/155]: 3.3489\n",
      "\n",
      "Batch Loss [154/155]: 3.5768\n",
      "\n",
      "Batch Loss [155/155]: 3.3662\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5809a6cc7f0847afa129676050b53313"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train:  3.1956\n",
      "Test:  Contrastive Loss: 3.1084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0f129f8cf28493a88c35d156eac5dd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 3.0421\n",
      "\n",
      "Batch Loss [2/155]: 2.9511\n",
      "\n",
      "Batch Loss [3/155]: 3.1770\n",
      "\n",
      "Batch Loss [4/155]: 3.3029\n",
      "\n",
      "Batch Loss [5/155]: 2.8730\n",
      "\n",
      "Batch Loss [6/155]: 3.1153\n",
      "\n",
      "Batch Loss [7/155]: 3.6709\n",
      "\n",
      "Batch Loss [8/155]: 3.0855\n",
      "\n",
      "Batch Loss [9/155]: 2.7987\n",
      "\n",
      "Batch Loss [10/155]: 2.7534\n",
      "\n",
      "Batch Loss [11/155]: 3.2721\n",
      "\n",
      "Batch Loss [12/155]: 3.2517\n",
      "\n",
      "Batch Loss [13/155]: 3.3331\n",
      "\n",
      "Batch Loss [14/155]: 2.9804\n",
      "\n",
      "Batch Loss [15/155]: 3.0275\n",
      "\n",
      "Batch Loss [16/155]: 3.4674\n",
      "\n",
      "Batch Loss [17/155]: 3.2641\n",
      "\n",
      "Batch Loss [18/155]: 3.0097\n",
      "\n",
      "Batch Loss [19/155]: 3.0534\n",
      "\n",
      "Batch Loss [20/155]: 3.4083\n",
      "\n",
      "Batch Loss [21/155]: 3.3051\n",
      "\n",
      "Batch Loss [22/155]: 3.3070\n",
      "\n",
      "Batch Loss [23/155]: 2.8847\n",
      "\n",
      "Batch Loss [24/155]: 3.3194\n",
      "\n",
      "Batch Loss [25/155]: 2.9499\n",
      "\n",
      "Batch Loss [26/155]: 3.3059\n",
      "\n",
      "Batch Loss [27/155]: 3.2021\n",
      "\n",
      "Batch Loss [28/155]: 3.2367\n",
      "\n",
      "Batch Loss [29/155]: 3.3345\n",
      "\n",
      "Batch Loss [30/155]: 2.8073\n",
      "\n",
      "Batch Loss [31/155]: 2.6892\n",
      "\n",
      "Batch Loss [32/155]: 3.1819\n",
      "\n",
      "Batch Loss [33/155]: 2.7545\n",
      "\n",
      "Batch Loss [34/155]: 3.2491\n",
      "\n",
      "Batch Loss [35/155]: 3.2912\n",
      "\n",
      "Batch Loss [36/155]: 3.3138\n",
      "\n",
      "Batch Loss [37/155]: 3.4128\n",
      "\n",
      "Batch Loss [38/155]: 2.8918\n",
      "\n",
      "Batch Loss [39/155]: 3.0840\n",
      "\n",
      "Batch Loss [40/155]: 3.0680\n",
      "\n",
      "Batch Loss [41/155]: 3.0224\n",
      "\n",
      "Batch Loss [42/155]: 3.0231\n",
      "\n",
      "Batch Loss [43/155]: 2.9992\n",
      "\n",
      "Batch Loss [44/155]: 3.2776\n",
      "\n",
      "Batch Loss [45/155]: 2.8145\n",
      "\n",
      "Batch Loss [46/155]: 3.1170\n",
      "\n",
      "Batch Loss [47/155]: 3.4506\n",
      "\n",
      "Batch Loss [48/155]: 2.8168\n",
      "\n",
      "Batch Loss [49/155]: 3.1516\n",
      "\n",
      "Batch Loss [50/155]: 3.0513\n",
      "\n",
      "Batch Loss [51/155]: 3.1118\n",
      "\n",
      "Batch Loss [52/155]: 2.8753\n",
      "\n",
      "Batch Loss [53/155]: 3.4508\n",
      "\n",
      "Batch Loss [54/155]: 2.9645\n",
      "\n",
      "Batch Loss [55/155]: 3.0680\n",
      "\n",
      "Batch Loss [56/155]: 3.0994\n",
      "\n",
      "Batch Loss [57/155]: 3.0946\n",
      "\n",
      "Batch Loss [58/155]: 3.0754\n",
      "\n",
      "Batch Loss [59/155]: 3.2131\n",
      "\n",
      "Batch Loss [60/155]: 3.4537\n",
      "\n",
      "Batch Loss [61/155]: 3.2945\n",
      "\n",
      "Batch Loss [62/155]: 2.8357\n",
      "\n",
      "Batch Loss [63/155]: 3.0922\n",
      "\n",
      "Batch Loss [64/155]: 3.0538\n",
      "\n",
      "Batch Loss [65/155]: 3.2543\n",
      "\n",
      "Batch Loss [66/155]: 2.9011\n",
      "\n",
      "Batch Loss [67/155]: 2.9999\n",
      "\n",
      "Batch Loss [68/155]: 3.3094\n",
      "\n",
      "Batch Loss [69/155]: 3.0233\n",
      "\n",
      "Batch Loss [70/155]: 3.0056\n",
      "\n",
      "Batch Loss [71/155]: 3.1643\n",
      "\n",
      "Batch Loss [72/155]: 3.2856\n",
      "\n",
      "Batch Loss [73/155]: 2.9462\n",
      "\n",
      "Batch Loss [74/155]: 3.2911\n",
      "\n",
      "Batch Loss [75/155]: 3.2319\n",
      "\n",
      "Batch Loss [76/155]: 3.1581\n",
      "\n",
      "Batch Loss [77/155]: 3.2800\n",
      "\n",
      "Batch Loss [78/155]: 3.2632\n",
      "\n",
      "Batch Loss [79/155]: 3.0474\n",
      "\n",
      "Batch Loss [80/155]: 3.1262\n",
      "\n",
      "Batch Loss [81/155]: 2.9453\n",
      "\n",
      "Batch Loss [82/155]: 2.9139\n",
      "\n",
      "Batch Loss [83/155]: 3.2071\n",
      "\n",
      "Batch Loss [84/155]: 2.8554\n",
      "\n",
      "Batch Loss [85/155]: 3.1534\n",
      "\n",
      "Batch Loss [86/155]: 3.0015\n",
      "\n",
      "Batch Loss [87/155]: 2.8002\n",
      "\n",
      "Batch Loss [88/155]: 3.2139\n",
      "\n",
      "Batch Loss [89/155]: 3.3381\n",
      "\n",
      "Batch Loss [90/155]: 3.2587\n",
      "\n",
      "Batch Loss [91/155]: 2.9004\n",
      "\n",
      "Batch Loss [92/155]: 3.1173\n",
      "\n",
      "Batch Loss [93/155]: 2.8973\n",
      "\n",
      "Batch Loss [94/155]: 2.8174\n",
      "\n",
      "Batch Loss [95/155]: 3.1918\n",
      "\n",
      "Batch Loss [96/155]: 2.9354\n",
      "\n",
      "Batch Loss [97/155]: 3.6196\n",
      "\n",
      "Batch Loss [98/155]: 2.9409\n",
      "\n",
      "Batch Loss [99/155]: 3.0723\n",
      "\n",
      "Batch Loss [100/155]: 3.0772\n",
      "\n",
      "Batch Loss [101/155]: 3.2643\n",
      "\n",
      "Batch Loss [102/155]: 3.0089\n",
      "\n",
      "Batch Loss [103/155]: 3.4990\n",
      "\n",
      "Batch Loss [104/155]: 2.7891\n",
      "\n",
      "Batch Loss [105/155]: 2.9666\n",
      "\n",
      "Batch Loss [106/155]: 3.0910\n",
      "\n",
      "Batch Loss [107/155]: 3.3982\n",
      "\n",
      "Batch Loss [108/155]: 3.3197\n",
      "\n",
      "Batch Loss [109/155]: 2.9561\n",
      "\n",
      "Batch Loss [110/155]: 2.5021\n",
      "\n",
      "Batch Loss [111/155]: 3.2264\n",
      "\n",
      "Batch Loss [112/155]: 3.1554\n",
      "\n",
      "Batch Loss [113/155]: 3.2690\n",
      "\n",
      "Batch Loss [114/155]: 3.1441\n",
      "\n",
      "Batch Loss [115/155]: 3.0933\n",
      "\n",
      "Batch Loss [116/155]: 3.0989\n",
      "\n",
      "Batch Loss [117/155]: 2.7538\n",
      "\n",
      "Batch Loss [118/155]: 3.3272\n",
      "\n",
      "Batch Loss [119/155]: 3.1210\n",
      "\n",
      "Batch Loss [120/155]: 2.9000\n",
      "\n",
      "Batch Loss [121/155]: 2.8902\n",
      "\n",
      "Batch Loss [122/155]: 3.0224\n",
      "\n",
      "Batch Loss [123/155]: 2.9818\n",
      "\n",
      "Batch Loss [124/155]: 3.2830\n",
      "\n",
      "Batch Loss [125/155]: 2.9333\n",
      "\n",
      "Batch Loss [126/155]: 2.7774\n",
      "\n",
      "Batch Loss [127/155]: 3.3434\n",
      "\n",
      "Batch Loss [128/155]: 3.1926\n",
      "\n",
      "Batch Loss [129/155]: 3.6374\n",
      "\n",
      "Batch Loss [130/155]: 3.4061\n",
      "\n",
      "Batch Loss [131/155]: 3.0844\n",
      "\n",
      "Batch Loss [132/155]: 3.0691\n",
      "\n",
      "Batch Loss [133/155]: 3.1876\n",
      "\n",
      "Batch Loss [134/155]: 3.0008\n",
      "\n",
      "Batch Loss [135/155]: 3.0133\n",
      "\n",
      "Batch Loss [136/155]: 3.0720\n",
      "\n",
      "Batch Loss [137/155]: 3.2153\n",
      "\n",
      "Batch Loss [138/155]: 2.6911\n",
      "\n",
      "Batch Loss [139/155]: 3.0282\n",
      "\n",
      "Batch Loss [140/155]: 3.1800\n",
      "\n",
      "Batch Loss [141/155]: 3.2501\n",
      "\n",
      "Batch Loss [142/155]: 3.1970\n",
      "\n",
      "Batch Loss [143/155]: 3.2596\n",
      "\n",
      "Batch Loss [144/155]: 2.9274\n",
      "\n",
      "Batch Loss [145/155]: 3.0838\n",
      "\n",
      "Batch Loss [146/155]: 3.0862\n",
      "\n",
      "Batch Loss [147/155]: 2.7859\n",
      "\n",
      "Batch Loss [148/155]: 3.4822\n",
      "\n",
      "Batch Loss [149/155]: 3.2711\n",
      "\n",
      "Batch Loss [150/155]: 2.8670\n",
      "\n",
      "Batch Loss [151/155]: 2.9796\n",
      "\n",
      "Batch Loss [152/155]: 3.3119\n",
      "\n",
      "Batch Loss [153/155]: 3.0159\n",
      "\n",
      "Batch Loss [154/155]: 3.1176\n",
      "\n",
      "Batch Loss [155/155]: 3.1544\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf5a81a3f36343db89817ae1e84bfabf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train:  3.1101\n",
      "Test:  Contrastive Loss: 3.0885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3748c5c643914486a98fde1c054ee949"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 2.6913\n",
      "\n",
      "Batch Loss [2/155]: 3.1025\n",
      "\n",
      "Batch Loss [3/155]: 2.8624\n",
      "\n",
      "Batch Loss [4/155]: 2.9823\n",
      "\n",
      "Batch Loss [5/155]: 3.2944\n",
      "\n",
      "Batch Loss [6/155]: 2.8856\n",
      "\n",
      "Batch Loss [7/155]: 3.0882\n",
      "\n",
      "Batch Loss [8/155]: 3.4618\n",
      "\n",
      "Batch Loss [9/155]: 2.8896\n",
      "\n",
      "Batch Loss [10/155]: 2.6142\n",
      "\n",
      "Batch Loss [11/155]: 2.9882\n",
      "\n",
      "Batch Loss [12/155]: 2.7989\n",
      "\n",
      "Batch Loss [13/155]: 3.2420\n",
      "\n",
      "Batch Loss [14/155]: 3.1664\n",
      "\n",
      "Batch Loss [15/155]: 3.1133\n",
      "\n",
      "Batch Loss [16/155]: 2.8073\n",
      "\n",
      "Batch Loss [17/155]: 3.4149\n",
      "\n",
      "Batch Loss [18/155]: 2.9738\n",
      "\n",
      "Batch Loss [19/155]: 3.3024\n",
      "\n",
      "Batch Loss [20/155]: 3.4123\n",
      "\n",
      "Batch Loss [21/155]: 3.2222\n",
      "\n",
      "Batch Loss [22/155]: 3.0960\n",
      "\n",
      "Batch Loss [23/155]: 2.9716\n",
      "\n",
      "Batch Loss [24/155]: 3.0357\n",
      "\n",
      "Batch Loss [25/155]: 3.0447\n",
      "\n",
      "Batch Loss [26/155]: 3.0406\n",
      "\n",
      "Batch Loss [27/155]: 3.5186\n",
      "\n",
      "Batch Loss [28/155]: 3.2198\n",
      "\n",
      "Batch Loss [29/155]: 3.2987\n",
      "\n",
      "Batch Loss [30/155]: 3.0716\n",
      "\n",
      "Batch Loss [31/155]: 2.9497\n",
      "\n",
      "Batch Loss [32/155]: 2.7856\n",
      "\n",
      "Batch Loss [33/155]: 2.8548\n",
      "\n",
      "Batch Loss [34/155]: 3.2370\n",
      "\n",
      "Batch Loss [35/155]: 3.0016\n",
      "\n",
      "Batch Loss [36/155]: 2.9200\n",
      "\n",
      "Batch Loss [37/155]: 3.4413\n",
      "\n",
      "Batch Loss [38/155]: 3.0805\n",
      "\n",
      "Batch Loss [39/155]: 3.1638\n",
      "\n",
      "Batch Loss [40/155]: 3.1350\n",
      "\n",
      "Batch Loss [41/155]: 3.0174\n",
      "\n",
      "Batch Loss [42/155]: 3.0603\n",
      "\n",
      "Batch Loss [43/155]: 3.3145\n",
      "\n",
      "Batch Loss [44/155]: 3.5385\n",
      "\n",
      "Batch Loss [45/155]: 3.1140\n",
      "\n",
      "Batch Loss [46/155]: 2.9865\n",
      "\n",
      "Batch Loss [47/155]: 2.8986\n",
      "\n",
      "Batch Loss [48/155]: 2.8841\n",
      "\n",
      "Batch Loss [49/155]: 2.9100\n",
      "\n",
      "Batch Loss [50/155]: 2.7590\n",
      "\n",
      "Batch Loss [51/155]: 2.7962\n",
      "\n",
      "Batch Loss [52/155]: 2.8350\n",
      "\n",
      "Batch Loss [53/155]: 3.6023\n",
      "\n",
      "Batch Loss [54/155]: 2.6016\n",
      "\n",
      "Batch Loss [55/155]: 2.6124\n",
      "\n",
      "Batch Loss [56/155]: 2.7947\n",
      "\n",
      "Batch Loss [57/155]: 2.7113\n",
      "\n",
      "Batch Loss [58/155]: 3.0060\n",
      "\n",
      "Batch Loss [59/155]: 2.6722\n",
      "\n",
      "Batch Loss [60/155]: 2.7617\n",
      "\n",
      "Batch Loss [61/155]: 3.0752\n",
      "\n",
      "Batch Loss [62/155]: 3.3472\n",
      "\n",
      "Batch Loss [63/155]: 2.9780\n",
      "\n",
      "Batch Loss [64/155]: 3.5184\n",
      "\n",
      "Batch Loss [65/155]: 2.8785\n",
      "\n",
      "Batch Loss [66/155]: 2.8896\n",
      "\n",
      "Batch Loss [67/155]: 2.9619\n",
      "\n",
      "Batch Loss [68/155]: 2.9113\n",
      "\n",
      "Batch Loss [69/155]: 3.0947\n",
      "\n",
      "Batch Loss [70/155]: 3.0006\n",
      "\n",
      "Batch Loss [71/155]: 2.9219\n",
      "\n",
      "Batch Loss [72/155]: 2.7272\n",
      "\n",
      "Batch Loss [73/155]: 3.0085\n",
      "\n",
      "Batch Loss [74/155]: 2.9537\n",
      "\n",
      "Batch Loss [75/155]: 2.7356\n",
      "\n",
      "Batch Loss [76/155]: 3.4262\n",
      "\n",
      "Batch Loss [77/155]: 2.8769\n",
      "\n",
      "Batch Loss [78/155]: 3.1168\n",
      "\n",
      "Batch Loss [79/155]: 3.3565\n",
      "\n",
      "Batch Loss [80/155]: 2.7134\n",
      "\n",
      "Batch Loss [81/155]: 3.0948\n",
      "\n",
      "Batch Loss [82/155]: 3.2129\n",
      "\n",
      "Batch Loss [83/155]: 3.1300\n",
      "\n",
      "Batch Loss [84/155]: 2.9480\n",
      "\n",
      "Batch Loss [85/155]: 2.7002\n",
      "\n",
      "Batch Loss [86/155]: 3.0216\n",
      "\n",
      "Batch Loss [87/155]: 2.6972\n",
      "\n",
      "Batch Loss [88/155]: 2.7272\n",
      "\n",
      "Batch Loss [89/155]: 3.1783\n",
      "\n",
      "Batch Loss [90/155]: 3.2371\n",
      "\n",
      "Batch Loss [91/155]: 2.7298\n",
      "\n",
      "Batch Loss [92/155]: 3.0892\n",
      "\n",
      "Batch Loss [93/155]: 2.8986\n",
      "\n",
      "Batch Loss [94/155]: 2.9480\n",
      "\n",
      "Batch Loss [95/155]: 2.5205\n",
      "\n",
      "Batch Loss [96/155]: 3.3365\n",
      "\n",
      "Batch Loss [97/155]: 3.0936\n",
      "\n",
      "Batch Loss [98/155]: 3.3065\n",
      "\n",
      "Batch Loss [99/155]: 3.0999\n",
      "\n",
      "Batch Loss [100/155]: 2.8892\n",
      "\n",
      "Batch Loss [101/155]: 2.7140\n",
      "\n",
      "Batch Loss [102/155]: 2.9835\n",
      "\n",
      "Batch Loss [103/155]: 2.5984\n",
      "\n",
      "Batch Loss [104/155]: 3.0172\n",
      "\n",
      "Batch Loss [105/155]: 3.6111\n",
      "\n",
      "Batch Loss [106/155]: 2.9248\n",
      "\n",
      "Batch Loss [107/155]: 2.6792\n",
      "\n",
      "Batch Loss [108/155]: 2.7016\n",
      "\n",
      "Batch Loss [109/155]: 2.6962\n",
      "\n",
      "Batch Loss [110/155]: 2.7880\n",
      "\n",
      "Batch Loss [111/155]: 3.0256\n",
      "\n",
      "Batch Loss [112/155]: 2.8786\n",
      "\n",
      "Batch Loss [113/155]: 2.8839\n",
      "\n",
      "Batch Loss [114/155]: 2.4012\n",
      "\n",
      "Batch Loss [115/155]: 3.1343\n",
      "\n",
      "Batch Loss [116/155]: 2.4680\n",
      "\n",
      "Batch Loss [117/155]: 2.7450\n",
      "\n",
      "Batch Loss [118/155]: 3.0076\n",
      "\n",
      "Batch Loss [119/155]: 3.3614\n",
      "\n",
      "Batch Loss [120/155]: 2.4953\n",
      "\n",
      "Batch Loss [121/155]: 3.2846\n",
      "\n",
      "Batch Loss [122/155]: 2.8713\n",
      "\n",
      "Batch Loss [123/155]: 2.9788\n",
      "\n",
      "Batch Loss [124/155]: 3.2256\n",
      "\n",
      "Batch Loss [125/155]: 2.5486\n",
      "\n",
      "Batch Loss [126/155]: 2.8989\n",
      "\n",
      "Batch Loss [127/155]: 3.1261\n",
      "\n",
      "Batch Loss [128/155]: 3.5339\n",
      "\n",
      "Batch Loss [129/155]: 3.2353\n",
      "\n",
      "Batch Loss [130/155]: 3.0490\n",
      "\n",
      "Batch Loss [131/155]: 2.8239\n",
      "\n",
      "Batch Loss [132/155]: 3.0378\n",
      "\n",
      "Batch Loss [133/155]: 2.8143\n",
      "\n",
      "Batch Loss [134/155]: 2.9566\n",
      "\n",
      "Batch Loss [135/155]: 3.5463\n",
      "\n",
      "Batch Loss [136/155]: 2.8477\n",
      "\n",
      "Batch Loss [137/155]: 2.8530\n",
      "\n",
      "Batch Loss [138/155]: 3.2045\n",
      "\n",
      "Batch Loss [139/155]: 2.8721\n",
      "\n",
      "Batch Loss [140/155]: 2.9069\n",
      "\n",
      "Batch Loss [141/155]: 2.5985\n",
      "\n",
      "Batch Loss [142/155]: 2.7303\n",
      "\n",
      "Batch Loss [143/155]: 2.6796\n",
      "\n",
      "Batch Loss [144/155]: 3.3118\n",
      "\n",
      "Batch Loss [145/155]: 2.7683\n",
      "\n",
      "Batch Loss [146/155]: 2.8950\n",
      "\n",
      "Batch Loss [147/155]: 2.8745\n",
      "\n",
      "Batch Loss [148/155]: 2.7804\n",
      "\n",
      "Batch Loss [149/155]: 2.9901\n",
      "\n",
      "Batch Loss [150/155]: 3.0508\n",
      "\n",
      "Batch Loss [151/155]: 3.4168\n",
      "\n",
      "Batch Loss [152/155]: 2.7431\n",
      "\n",
      "Batch Loss [153/155]: 3.0677\n",
      "\n",
      "Batch Loss [154/155]: 2.7110\n",
      "\n",
      "Batch Loss [155/155]: 2.6251\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9a9426db44c4d12a7214ae6cf13651c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train:  2.9896\n",
      "Test:  Contrastive Loss: 2.9599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70c8c74d71f34a1ebd484b6b91ffebed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 2.8469\n",
      "\n",
      "Batch Loss [2/155]: 2.7223\n",
      "\n",
      "Batch Loss [3/155]: 2.8344\n",
      "\n",
      "Batch Loss [4/155]: 3.1824\n",
      "\n",
      "Batch Loss [5/155]: 3.3654\n",
      "\n",
      "Batch Loss [6/155]: 3.0972\n",
      "\n",
      "Batch Loss [7/155]: 2.9265\n",
      "\n",
      "Batch Loss [8/155]: 3.0113\n",
      "\n",
      "Batch Loss [9/155]: 3.0162\n",
      "\n",
      "Batch Loss [10/155]: 3.0195\n",
      "\n",
      "Batch Loss [11/155]: 2.9068\n",
      "\n",
      "Batch Loss [12/155]: 2.7497\n",
      "\n",
      "Batch Loss [13/155]: 3.1078\n",
      "\n",
      "Batch Loss [14/155]: 2.9592\n",
      "\n",
      "Batch Loss [15/155]: 3.0224\n",
      "\n",
      "Batch Loss [16/155]: 3.0663\n",
      "\n",
      "Batch Loss [17/155]: 2.7032\n",
      "\n",
      "Batch Loss [18/155]: 3.1835\n",
      "\n",
      "Batch Loss [19/155]: 2.6513\n",
      "\n",
      "Batch Loss [20/155]: 3.2090\n",
      "\n",
      "Batch Loss [21/155]: 2.6095\n",
      "\n",
      "Batch Loss [22/155]: 3.0229\n",
      "\n",
      "Batch Loss [23/155]: 3.2113\n",
      "\n",
      "Batch Loss [24/155]: 2.8797\n",
      "\n",
      "Batch Loss [25/155]: 2.9817\n",
      "\n",
      "Batch Loss [26/155]: 2.9433\n",
      "\n",
      "Batch Loss [27/155]: 3.3217\n",
      "\n",
      "Batch Loss [28/155]: 2.9197\n",
      "\n",
      "Batch Loss [29/155]: 2.9864\n",
      "\n",
      "Batch Loss [30/155]: 2.7503\n",
      "\n",
      "Batch Loss [31/155]: 2.6702\n",
      "\n",
      "Batch Loss [32/155]: 2.6200\n",
      "\n",
      "Batch Loss [33/155]: 2.7730\n",
      "\n",
      "Batch Loss [34/155]: 3.4008\n",
      "\n",
      "Batch Loss [35/155]: 2.7132\n",
      "\n",
      "Batch Loss [36/155]: 2.5746\n",
      "\n",
      "Batch Loss [37/155]: 2.4985\n",
      "\n",
      "Batch Loss [38/155]: 2.7937\n",
      "\n",
      "Batch Loss [39/155]: 3.0083\n",
      "\n",
      "Batch Loss [40/155]: 2.5419\n",
      "\n",
      "Batch Loss [41/155]: 2.9992\n",
      "\n",
      "Batch Loss [42/155]: 3.0100\n",
      "\n",
      "Batch Loss [43/155]: 3.3081\n",
      "\n",
      "Batch Loss [44/155]: 2.9658\n",
      "\n",
      "Batch Loss [45/155]: 3.0974\n",
      "\n",
      "Batch Loss [46/155]: 2.8850\n",
      "\n",
      "Batch Loss [47/155]: 2.6205\n",
      "\n",
      "Batch Loss [48/155]: 2.8558\n",
      "\n",
      "Batch Loss [49/155]: 2.9915\n",
      "\n",
      "Batch Loss [50/155]: 2.9435\n",
      "\n",
      "Batch Loss [51/155]: 2.5700\n",
      "\n",
      "Batch Loss [52/155]: 3.2210\n",
      "\n",
      "Batch Loss [53/155]: 2.6520\n",
      "\n",
      "Batch Loss [54/155]: 2.8096\n",
      "\n",
      "Batch Loss [55/155]: 2.7566\n",
      "\n",
      "Batch Loss [56/155]: 2.7643\n",
      "\n",
      "Batch Loss [57/155]: 3.2108\n",
      "\n",
      "Batch Loss [58/155]: 3.2120\n",
      "\n",
      "Batch Loss [59/155]: 2.6324\n",
      "\n",
      "Batch Loss [60/155]: 2.7029\n",
      "\n",
      "Batch Loss [61/155]: 3.3729\n",
      "\n",
      "Batch Loss [62/155]: 3.2536\n",
      "\n",
      "Batch Loss [63/155]: 2.8895\n",
      "\n",
      "Batch Loss [64/155]: 3.0201\n",
      "\n",
      "Batch Loss [65/155]: 3.0671\n",
      "\n",
      "Batch Loss [66/155]: 2.7035\n",
      "\n",
      "Batch Loss [67/155]: 2.8077\n",
      "\n",
      "Batch Loss [68/155]: 3.1022\n",
      "\n",
      "Batch Loss [69/155]: 2.6643\n",
      "\n",
      "Batch Loss [70/155]: 3.1090\n",
      "\n",
      "Batch Loss [71/155]: 2.6819\n",
      "\n",
      "Batch Loss [72/155]: 2.9383\n",
      "\n",
      "Batch Loss [73/155]: 2.6676\n",
      "\n",
      "Batch Loss [74/155]: 3.1954\n",
      "\n",
      "Batch Loss [75/155]: 3.5394\n",
      "\n",
      "Batch Loss [76/155]: 2.4865\n",
      "\n",
      "Batch Loss [77/155]: 2.6955\n",
      "\n",
      "Batch Loss [78/155]: 3.1549\n",
      "\n",
      "Batch Loss [79/155]: 3.0821\n",
      "\n",
      "Batch Loss [80/155]: 2.8065\n",
      "\n",
      "Batch Loss [81/155]: 2.6995\n",
      "\n",
      "Batch Loss [82/155]: 2.6863\n",
      "\n",
      "Batch Loss [83/155]: 2.9396\n",
      "\n",
      "Batch Loss [84/155]: 3.0876\n",
      "\n",
      "Batch Loss [85/155]: 3.6233\n",
      "\n",
      "Batch Loss [86/155]: 3.0495\n",
      "\n",
      "Batch Loss [87/155]: 3.1431\n",
      "\n",
      "Batch Loss [88/155]: 2.9814\n",
      "\n",
      "Batch Loss [89/155]: 3.0859\n",
      "\n",
      "Batch Loss [90/155]: 3.2702\n",
      "\n",
      "Batch Loss [91/155]: 2.9942\n",
      "\n",
      "Batch Loss [92/155]: 2.9515\n",
      "\n",
      "Batch Loss [93/155]: 2.5519\n",
      "\n",
      "Batch Loss [94/155]: 2.5820\n",
      "\n",
      "Batch Loss [95/155]: 2.8485\n",
      "\n",
      "Batch Loss [96/155]: 3.1312\n",
      "\n",
      "Batch Loss [97/155]: 2.8686\n",
      "\n",
      "Batch Loss [98/155]: 2.8015\n",
      "\n",
      "Batch Loss [99/155]: 2.7400\n",
      "\n",
      "Batch Loss [100/155]: 3.1335\n",
      "\n",
      "Batch Loss [101/155]: 2.9692\n",
      "\n",
      "Batch Loss [102/155]: 2.8094\n",
      "\n",
      "Batch Loss [103/155]: 2.7510\n",
      "\n",
      "Batch Loss [104/155]: 2.7796\n",
      "\n",
      "Batch Loss [105/155]: 2.9707\n",
      "\n",
      "Batch Loss [106/155]: 2.8467\n",
      "\n",
      "Batch Loss [107/155]: 3.6866\n",
      "\n",
      "Batch Loss [108/155]: 2.7386\n",
      "\n",
      "Batch Loss [109/155]: 2.9048\n",
      "\n",
      "Batch Loss [110/155]: 2.9582\n",
      "\n",
      "Batch Loss [111/155]: 3.1171\n",
      "\n",
      "Batch Loss [112/155]: 2.6696\n",
      "\n",
      "Batch Loss [113/155]: 2.9838\n",
      "\n",
      "Batch Loss [114/155]: 2.9327\n",
      "\n",
      "Batch Loss [115/155]: 2.8406\n",
      "\n",
      "Batch Loss [116/155]: 2.7209\n",
      "\n",
      "Batch Loss [117/155]: 2.9486\n",
      "\n",
      "Batch Loss [118/155]: 2.7487\n",
      "\n",
      "Batch Loss [119/155]: 3.0486\n",
      "\n",
      "Batch Loss [120/155]: 2.9375\n",
      "\n",
      "Batch Loss [121/155]: 2.7468\n",
      "\n",
      "Batch Loss [122/155]: 2.8888\n",
      "\n",
      "Batch Loss [123/155]: 2.7087\n",
      "\n",
      "Batch Loss [124/155]: 2.7843\n",
      "\n",
      "Batch Loss [125/155]: 2.8726\n",
      "\n",
      "Batch Loss [126/155]: 2.8126\n",
      "\n",
      "Batch Loss [127/155]: 2.9579\n",
      "\n",
      "Batch Loss [128/155]: 2.9831\n",
      "\n",
      "Batch Loss [129/155]: 3.1059\n",
      "\n",
      "Batch Loss [130/155]: 2.5911\n",
      "\n",
      "Batch Loss [131/155]: 3.0202\n",
      "\n",
      "Batch Loss [132/155]: 2.9382\n",
      "\n",
      "Batch Loss [133/155]: 2.9913\n",
      "\n",
      "Batch Loss [134/155]: 3.0164\n",
      "\n",
      "Batch Loss [135/155]: 2.6176\n",
      "\n",
      "Batch Loss [136/155]: 2.7424\n",
      "\n",
      "Batch Loss [137/155]: 2.8202\n",
      "\n",
      "Batch Loss [138/155]: 2.5709\n",
      "\n",
      "Batch Loss [139/155]: 3.3422\n",
      "\n",
      "Batch Loss [140/155]: 2.4687\n",
      "\n",
      "Batch Loss [141/155]: 3.0487\n",
      "\n",
      "Batch Loss [142/155]: 2.9572\n",
      "\n",
      "Batch Loss [143/155]: 2.8989\n",
      "\n",
      "Batch Loss [144/155]: 2.9293\n",
      "\n",
      "Batch Loss [145/155]: 2.7531\n",
      "\n",
      "Batch Loss [146/155]: 2.8716\n",
      "\n",
      "Batch Loss [147/155]: 2.6982\n",
      "\n",
      "Batch Loss [148/155]: 2.8683\n",
      "\n",
      "Batch Loss [149/155]: 2.7876\n",
      "\n",
      "Batch Loss [150/155]: 2.4056\n",
      "\n",
      "Batch Loss [151/155]: 2.4917\n",
      "\n",
      "Batch Loss [152/155]: 2.5668\n",
      "\n",
      "Batch Loss [153/155]: 2.8308\n",
      "\n",
      "Batch Loss [154/155]: 2.8820\n",
      "\n",
      "Batch Loss [155/155]: 2.8840\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "669da14889e64b368ac55145d7556798"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train:  2.9073\n",
      "Test:  Contrastive Loss: 2.8778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "875be6da1fcb4b31bcbc6a8ff4a989c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 3.3330\n",
      "\n",
      "Batch Loss [2/155]: 3.2400\n",
      "\n",
      "Batch Loss [3/155]: 2.8894\n",
      "\n",
      "Batch Loss [4/155]: 2.6360\n",
      "\n",
      "Batch Loss [5/155]: 2.8867\n",
      "\n",
      "Batch Loss [6/155]: 2.2880\n",
      "\n",
      "Batch Loss [7/155]: 2.8384\n",
      "\n",
      "Batch Loss [8/155]: 3.0213\n",
      "\n",
      "Batch Loss [9/155]: 2.6251\n",
      "\n",
      "Batch Loss [10/155]: 3.1536\n",
      "\n",
      "Batch Loss [11/155]: 2.8734\n",
      "\n",
      "Batch Loss [12/155]: 2.9391\n",
      "\n",
      "Batch Loss [13/155]: 3.1916\n",
      "\n",
      "Batch Loss [14/155]: 2.7642\n",
      "\n",
      "Batch Loss [15/155]: 2.6112\n",
      "\n",
      "Batch Loss [16/155]: 2.6110\n",
      "\n",
      "Batch Loss [17/155]: 2.9648\n",
      "\n",
      "Batch Loss [18/155]: 2.5096\n",
      "\n",
      "Batch Loss [19/155]: 2.9813\n",
      "\n",
      "Batch Loss [20/155]: 2.7457\n",
      "\n",
      "Batch Loss [21/155]: 2.8642\n",
      "\n",
      "Batch Loss [22/155]: 3.2810\n",
      "\n",
      "Batch Loss [23/155]: 2.6882\n",
      "\n",
      "Batch Loss [24/155]: 2.9640\n",
      "\n",
      "Batch Loss [25/155]: 2.4064\n",
      "\n",
      "Batch Loss [26/155]: 2.6226\n",
      "\n",
      "Batch Loss [27/155]: 3.0960\n",
      "\n",
      "Batch Loss [28/155]: 3.1989\n",
      "\n",
      "Batch Loss [29/155]: 2.8461\n",
      "\n",
      "Batch Loss [30/155]: 2.6999\n",
      "\n",
      "Batch Loss [31/155]: 2.9911\n",
      "\n",
      "Batch Loss [32/155]: 2.3826\n",
      "\n",
      "Batch Loss [33/155]: 2.9716\n",
      "\n",
      "Batch Loss [34/155]: 2.9422\n",
      "\n",
      "Batch Loss [35/155]: 2.7008\n",
      "\n",
      "Batch Loss [36/155]: 2.6218\n",
      "\n",
      "Batch Loss [37/155]: 3.1695\n",
      "\n",
      "Batch Loss [38/155]: 3.5421\n",
      "\n",
      "Batch Loss [39/155]: 2.5893\n",
      "\n",
      "Batch Loss [40/155]: 2.8575\n",
      "\n",
      "Batch Loss [41/155]: 2.7755\n",
      "\n",
      "Batch Loss [42/155]: 2.8935\n",
      "\n",
      "Batch Loss [43/155]: 2.9821\n",
      "\n",
      "Batch Loss [44/155]: 2.5895\n",
      "\n",
      "Batch Loss [45/155]: 2.6180\n",
      "\n",
      "Batch Loss [46/155]: 2.7341\n",
      "\n",
      "Batch Loss [47/155]: 3.0877\n",
      "\n",
      "Batch Loss [48/155]: 2.7430\n",
      "\n",
      "Batch Loss [49/155]: 3.2105\n",
      "\n",
      "Batch Loss [50/155]: 2.7230\n",
      "\n",
      "Batch Loss [51/155]: 2.6581\n",
      "\n",
      "Batch Loss [52/155]: 3.1472\n",
      "\n",
      "Batch Loss [53/155]: 2.7125\n",
      "\n",
      "Batch Loss [54/155]: 2.8256\n",
      "\n",
      "Batch Loss [55/155]: 2.4348\n",
      "\n",
      "Batch Loss [56/155]: 2.9806\n",
      "\n",
      "Batch Loss [57/155]: 2.9136\n",
      "\n",
      "Batch Loss [58/155]: 2.6904\n",
      "\n",
      "Batch Loss [59/155]: 2.4570\n",
      "\n",
      "Batch Loss [60/155]: 2.5091\n",
      "\n",
      "Batch Loss [61/155]: 2.7316\n",
      "\n",
      "Batch Loss [62/155]: 2.7127\n",
      "\n",
      "Batch Loss [63/155]: 2.6345\n",
      "\n",
      "Batch Loss [64/155]: 3.0272\n",
      "\n",
      "Batch Loss [65/155]: 2.9303\n",
      "\n",
      "Batch Loss [66/155]: 2.8968\n",
      "\n",
      "Batch Loss [67/155]: 2.8535\n",
      "\n",
      "Batch Loss [68/155]: 2.5487\n",
      "\n",
      "Batch Loss [69/155]: 2.7112\n",
      "\n",
      "Batch Loss [70/155]: 2.6736\n",
      "\n",
      "Batch Loss [71/155]: 2.6445\n",
      "\n",
      "Batch Loss [72/155]: 2.8541\n",
      "\n",
      "Batch Loss [73/155]: 2.7155\n",
      "\n",
      "Batch Loss [74/155]: 2.4526\n",
      "\n",
      "Batch Loss [75/155]: 2.5852\n",
      "\n",
      "Batch Loss [76/155]: 2.8512\n",
      "\n",
      "Batch Loss [77/155]: 2.8818\n",
      "\n",
      "Batch Loss [78/155]: 2.7836\n",
      "\n",
      "Batch Loss [79/155]: 2.3991\n",
      "\n",
      "Batch Loss [80/155]: 3.2319\n",
      "\n",
      "Batch Loss [81/155]: 2.8700\n",
      "\n",
      "Batch Loss [82/155]: 2.7808\n",
      "\n",
      "Batch Loss [83/155]: 2.7585\n",
      "\n",
      "Batch Loss [84/155]: 2.9761\n",
      "\n",
      "Batch Loss [85/155]: 2.9147\n",
      "\n",
      "Batch Loss [86/155]: 3.6029\n",
      "\n",
      "Batch Loss [87/155]: 2.9553\n",
      "\n",
      "Batch Loss [88/155]: 2.8960\n",
      "\n",
      "Batch Loss [89/155]: 3.0707\n",
      "\n",
      "Batch Loss [90/155]: 2.8112\n",
      "\n",
      "Batch Loss [91/155]: 3.0374\n",
      "\n",
      "Batch Loss [92/155]: 2.7891\n",
      "\n",
      "Batch Loss [93/155]: 3.0875\n",
      "\n",
      "Batch Loss [94/155]: 2.5971\n",
      "\n",
      "Batch Loss [95/155]: 3.0279\n",
      "\n",
      "Batch Loss [96/155]: 3.0359\n",
      "\n",
      "Batch Loss [97/155]: 3.1075\n",
      "\n",
      "Batch Loss [98/155]: 2.7842\n",
      "\n",
      "Batch Loss [99/155]: 2.7776\n",
      "\n",
      "Batch Loss [100/155]: 2.7907\n",
      "\n",
      "Batch Loss [101/155]: 2.8884\n",
      "\n",
      "Batch Loss [102/155]: 2.7857\n",
      "\n",
      "Batch Loss [103/155]: 2.7836\n",
      "\n",
      "Batch Loss [104/155]: 3.1221\n",
      "\n",
      "Batch Loss [105/155]: 3.1396\n",
      "\n",
      "Batch Loss [106/155]: 2.7722\n",
      "\n",
      "Batch Loss [107/155]: 3.1819\n",
      "\n",
      "Batch Loss [108/155]: 2.5063\n",
      "\n",
      "Batch Loss [109/155]: 2.5197\n",
      "\n",
      "Batch Loss [110/155]: 2.6293\n",
      "\n",
      "Batch Loss [111/155]: 2.9444\n",
      "\n",
      "Batch Loss [112/155]: 2.9367\n",
      "\n",
      "Batch Loss [113/155]: 3.0741\n",
      "\n",
      "Batch Loss [114/155]: 2.8260\n",
      "\n",
      "Batch Loss [115/155]: 2.7626\n",
      "\n",
      "Batch Loss [116/155]: 2.1762\n",
      "\n",
      "Batch Loss [117/155]: 3.4407\n",
      "\n",
      "Batch Loss [118/155]: 2.4977\n",
      "\n",
      "Batch Loss [119/155]: 2.4627\n",
      "\n",
      "Batch Loss [120/155]: 2.4344\n",
      "\n",
      "Batch Loss [121/155]: 2.4659\n",
      "\n",
      "Batch Loss [122/155]: 3.0532\n",
      "\n",
      "Batch Loss [123/155]: 2.7361\n",
      "\n",
      "Batch Loss [124/155]: 2.7167\n",
      "\n",
      "Batch Loss [125/155]: 3.0773\n",
      "\n",
      "Batch Loss [126/155]: 2.8459\n",
      "\n",
      "Batch Loss [127/155]: 2.8916\n",
      "\n",
      "Batch Loss [128/155]: 3.0672\n",
      "\n",
      "Batch Loss [129/155]: 2.6416\n",
      "\n",
      "Batch Loss [130/155]: 2.8210\n",
      "\n",
      "Batch Loss [131/155]: 2.7786\n",
      "\n",
      "Batch Loss [132/155]: 2.8085\n",
      "\n",
      "Batch Loss [133/155]: 2.7129\n",
      "\n",
      "Batch Loss [134/155]: 2.9631\n",
      "\n",
      "Batch Loss [135/155]: 3.0678\n",
      "\n",
      "Batch Loss [136/155]: 2.9266\n",
      "\n",
      "Batch Loss [137/155]: 3.0638\n",
      "\n",
      "Batch Loss [138/155]: 2.7440\n",
      "\n",
      "Batch Loss [139/155]: 2.6484\n",
      "\n",
      "Batch Loss [140/155]: 2.6933\n",
      "\n",
      "Batch Loss [141/155]: 2.9208\n",
      "\n",
      "Batch Loss [142/155]: 2.5612\n",
      "\n",
      "Batch Loss [143/155]: 2.5993\n",
      "\n",
      "Batch Loss [144/155]: 2.7631\n",
      "\n",
      "Batch Loss [145/155]: 2.6916\n",
      "\n",
      "Batch Loss [146/155]: 2.6775\n",
      "\n",
      "Batch Loss [147/155]: 2.8109\n",
      "\n",
      "Batch Loss [148/155]: 2.6590\n",
      "\n",
      "Batch Loss [149/155]: 2.7855\n",
      "\n",
      "Batch Loss [150/155]: 2.5672\n",
      "\n",
      "Batch Loss [151/155]: 2.8777\n",
      "\n",
      "Batch Loss [152/155]: 2.9124\n",
      "\n",
      "Batch Loss [153/155]: 2.5913\n",
      "\n",
      "Batch Loss [154/155]: 2.5473\n",
      "\n",
      "Batch Loss [155/155]: 2.6946\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ecb7d434ffb4210ad5ae7c34c81e0af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train:  2.8212\n",
      "Test:  Contrastive Loss: 2.7419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7283c2e8f35f49ac9ae1a27f7fd9c631"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 2.7353\n",
      "\n",
      "Batch Loss [2/155]: 2.7859\n",
      "\n",
      "Batch Loss [3/155]: 2.6750\n",
      "\n",
      "Batch Loss [4/155]: 3.0084\n",
      "\n",
      "Batch Loss [5/155]: 2.6063\n",
      "\n",
      "Batch Loss [6/155]: 2.7833\n",
      "\n",
      "Batch Loss [7/155]: 3.4698\n",
      "\n",
      "Batch Loss [8/155]: 2.7249\n",
      "\n",
      "Batch Loss [9/155]: 2.7267\n",
      "\n",
      "Batch Loss [10/155]: 2.7425\n",
      "\n",
      "Batch Loss [11/155]: 2.3420\n",
      "\n",
      "Batch Loss [12/155]: 2.9534\n",
      "\n",
      "Batch Loss [13/155]: 2.9140\n",
      "\n",
      "Batch Loss [14/155]: 2.6093\n",
      "\n",
      "Batch Loss [15/155]: 2.8584\n",
      "\n",
      "Batch Loss [16/155]: 2.8271\n",
      "\n",
      "Batch Loss [17/155]: 2.4673\n",
      "\n",
      "Batch Loss [18/155]: 2.3253\n",
      "\n",
      "Batch Loss [19/155]: 2.8019\n",
      "\n",
      "Batch Loss [20/155]: 3.1746\n",
      "\n",
      "Batch Loss [21/155]: 2.5976\n",
      "\n",
      "Batch Loss [22/155]: 2.7936\n",
      "\n",
      "Batch Loss [23/155]: 3.0719\n",
      "\n",
      "Batch Loss [24/155]: 2.6509\n",
      "\n",
      "Batch Loss [25/155]: 3.1183\n",
      "\n",
      "Batch Loss [26/155]: 2.6229\n",
      "\n",
      "Batch Loss [27/155]: 2.5749\n",
      "\n",
      "Batch Loss [28/155]: 3.0110\n",
      "\n",
      "Batch Loss [29/155]: 2.3895\n",
      "\n",
      "Batch Loss [30/155]: 2.7478\n",
      "\n",
      "Batch Loss [31/155]: 2.4448\n",
      "\n",
      "Batch Loss [32/155]: 2.7105\n",
      "\n",
      "Batch Loss [33/155]: 2.6872\n",
      "\n",
      "Batch Loss [34/155]: 2.5083\n",
      "\n",
      "Batch Loss [35/155]: 2.6381\n",
      "\n",
      "Batch Loss [36/155]: 2.4283\n",
      "\n",
      "Batch Loss [37/155]: 3.0519\n",
      "\n",
      "Batch Loss [38/155]: 3.5093\n",
      "\n",
      "Batch Loss [39/155]: 2.7042\n",
      "\n",
      "Batch Loss [40/155]: 2.9284\n",
      "\n",
      "Batch Loss [41/155]: 2.5440\n",
      "\n",
      "Batch Loss [42/155]: 2.8254\n",
      "\n",
      "Batch Loss [43/155]: 2.6883\n",
      "\n",
      "Batch Loss [44/155]: 2.7046\n",
      "\n",
      "Batch Loss [45/155]: 2.8481\n",
      "\n",
      "Batch Loss [46/155]: 2.9232\n",
      "\n",
      "Batch Loss [47/155]: 2.6395\n",
      "\n",
      "Batch Loss [48/155]: 2.8262\n",
      "\n",
      "Batch Loss [49/155]: 2.8857\n",
      "\n",
      "Batch Loss [50/155]: 2.7276\n",
      "\n",
      "Batch Loss [51/155]: 2.5386\n",
      "\n",
      "Batch Loss [52/155]: 2.4761\n",
      "\n",
      "Batch Loss [53/155]: 2.6039\n",
      "\n",
      "Batch Loss [54/155]: 2.7582\n",
      "\n",
      "Batch Loss [55/155]: 2.6121\n",
      "\n",
      "Batch Loss [56/155]: 2.5798\n",
      "\n",
      "Batch Loss [57/155]: 3.1233\n",
      "\n",
      "Batch Loss [58/155]: 2.5838\n",
      "\n",
      "Batch Loss [59/155]: 3.1429\n",
      "\n",
      "Batch Loss [60/155]: 2.2612\n",
      "\n",
      "Batch Loss [61/155]: 2.8056\n",
      "\n",
      "Batch Loss [62/155]: 2.4423\n",
      "\n",
      "Batch Loss [63/155]: 2.6095\n",
      "\n",
      "Batch Loss [64/155]: 2.8270\n",
      "\n",
      "Batch Loss [65/155]: 2.2422\n",
      "\n",
      "Batch Loss [66/155]: 2.8941\n",
      "\n",
      "Batch Loss [67/155]: 2.2864\n",
      "\n",
      "Batch Loss [68/155]: 3.0125\n",
      "\n",
      "Batch Loss [69/155]: 3.0325\n",
      "\n",
      "Batch Loss [70/155]: 2.7566\n",
      "\n",
      "Batch Loss [71/155]: 2.6526\n",
      "\n",
      "Batch Loss [72/155]: 2.5998\n",
      "\n",
      "Batch Loss [73/155]: 2.3815\n",
      "\n",
      "Batch Loss [74/155]: 2.4709\n",
      "\n",
      "Batch Loss [75/155]: 2.4564\n",
      "\n",
      "Batch Loss [76/155]: 2.6302\n",
      "\n",
      "Batch Loss [77/155]: 2.5899\n",
      "\n",
      "Batch Loss [78/155]: 2.8377\n",
      "\n",
      "Batch Loss [79/155]: 2.7560\n",
      "\n",
      "Batch Loss [80/155]: 2.7985\n",
      "\n",
      "Batch Loss [81/155]: 3.1048\n",
      "\n",
      "Batch Loss [82/155]: 2.8733\n",
      "\n",
      "Batch Loss [83/155]: 2.4817\n",
      "\n",
      "Batch Loss [84/155]: 2.7265\n",
      "\n",
      "Batch Loss [85/155]: 2.7345\n",
      "\n",
      "Batch Loss [86/155]: 2.8235\n",
      "\n",
      "Batch Loss [87/155]: 2.7290\n",
      "\n",
      "Batch Loss [88/155]: 2.8181\n",
      "\n",
      "Batch Loss [89/155]: 2.6229\n",
      "\n",
      "Batch Loss [90/155]: 2.6209\n",
      "\n",
      "Batch Loss [91/155]: 2.8446\n",
      "\n",
      "Batch Loss [92/155]: 3.1649\n",
      "\n",
      "Batch Loss [93/155]: 2.6667\n",
      "\n",
      "Batch Loss [94/155]: 2.3572\n",
      "\n",
      "Batch Loss [95/155]: 2.9635\n",
      "\n",
      "Batch Loss [96/155]: 2.5188\n",
      "\n",
      "Batch Loss [97/155]: 2.7967\n",
      "\n",
      "Batch Loss [98/155]: 2.5246\n",
      "\n",
      "Batch Loss [99/155]: 2.7008\n",
      "\n",
      "Batch Loss [100/155]: 3.0811\n",
      "\n",
      "Batch Loss [101/155]: 2.9671\n",
      "\n",
      "Batch Loss [102/155]: 2.7508\n",
      "\n",
      "Batch Loss [103/155]: 2.5457\n",
      "\n",
      "Batch Loss [104/155]: 2.1389\n",
      "\n",
      "Batch Loss [105/155]: 2.3880\n",
      "\n",
      "Batch Loss [106/155]: 3.2675\n",
      "\n",
      "Batch Loss [107/155]: 2.3924\n",
      "\n",
      "Batch Loss [108/155]: 2.6042\n",
      "\n",
      "Batch Loss [109/155]: 2.8896\n",
      "\n",
      "Batch Loss [110/155]: 2.9464\n",
      "\n",
      "Batch Loss [111/155]: 2.4111\n",
      "\n",
      "Batch Loss [112/155]: 2.6601\n",
      "\n",
      "Batch Loss [113/155]: 2.5967\n",
      "\n",
      "Batch Loss [114/155]: 3.0373\n",
      "\n",
      "Batch Loss [115/155]: 2.6545\n",
      "\n",
      "Batch Loss [116/155]: 2.6406\n",
      "\n",
      "Batch Loss [117/155]: 2.7580\n",
      "\n",
      "Batch Loss [118/155]: 2.3948\n",
      "\n",
      "Batch Loss [119/155]: 2.7162\n",
      "\n",
      "Batch Loss [120/155]: 2.6434\n",
      "\n",
      "Batch Loss [121/155]: 2.5422\n",
      "\n",
      "Batch Loss [122/155]: 3.0990\n",
      "\n",
      "Batch Loss [123/155]: 2.4673\n",
      "\n",
      "Batch Loss [124/155]: 2.2524\n",
      "\n",
      "Batch Loss [125/155]: 2.8920\n",
      "\n",
      "Batch Loss [126/155]: 2.7955\n",
      "\n",
      "Batch Loss [127/155]: 2.8406\n",
      "\n",
      "Batch Loss [128/155]: 2.6966\n",
      "\n",
      "Batch Loss [129/155]: 2.7518\n",
      "\n",
      "Batch Loss [130/155]: 2.8372\n",
      "\n",
      "Batch Loss [131/155]: 2.7356\n",
      "\n",
      "Batch Loss [132/155]: 3.0785\n",
      "\n",
      "Batch Loss [133/155]: 2.5452\n",
      "\n",
      "Batch Loss [134/155]: 2.8065\n",
      "\n",
      "Batch Loss [135/155]: 2.9498\n",
      "\n",
      "Batch Loss [136/155]: 3.1306\n",
      "\n",
      "Batch Loss [137/155]: 2.8603\n",
      "\n",
      "Batch Loss [138/155]: 2.9094\n",
      "\n",
      "Batch Loss [139/155]: 2.6149\n",
      "\n",
      "Batch Loss [140/155]: 2.3150\n",
      "\n",
      "Batch Loss [141/155]: 2.4938\n",
      "\n",
      "Batch Loss [142/155]: 2.7364\n",
      "\n",
      "Batch Loss [143/155]: 2.8043\n",
      "\n",
      "Batch Loss [144/155]: 2.7918\n",
      "\n",
      "Batch Loss [145/155]: 2.7418\n",
      "\n",
      "Batch Loss [146/155]: 2.8257\n",
      "\n",
      "Batch Loss [147/155]: 2.3900\n",
      "\n",
      "Batch Loss [148/155]: 2.7657\n",
      "\n",
      "Batch Loss [149/155]: 2.4405\n",
      "\n",
      "Batch Loss [150/155]: 2.5102\n",
      "\n",
      "Batch Loss [151/155]: 2.6100\n",
      "\n",
      "Batch Loss [152/155]: 3.0615\n",
      "\n",
      "Batch Loss [153/155]: 2.9383\n",
      "\n",
      "Batch Loss [154/155]: 3.0843\n",
      "\n",
      "Batch Loss [155/155]: 2.6346\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e06c65e93b44672bd1eac1fbd975c1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train:  2.7265\n",
      "Test:  Contrastive Loss: 2.7541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01573e9b85c143939000fbae7171643d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 2.1357\n",
      "\n",
      "Batch Loss [2/155]: 2.4402\n",
      "\n",
      "Batch Loss [3/155]: 2.7098\n",
      "\n",
      "Batch Loss [4/155]: 3.0688\n",
      "\n",
      "Batch Loss [5/155]: 2.6289\n",
      "\n",
      "Batch Loss [6/155]: 2.3479\n",
      "\n",
      "Batch Loss [7/155]: 2.5101\n",
      "\n",
      "Batch Loss [8/155]: 2.5024\n",
      "\n",
      "Batch Loss [9/155]: 2.3499\n",
      "\n",
      "Batch Loss [10/155]: 2.6873\n",
      "\n",
      "Batch Loss [11/155]: 2.7202\n",
      "\n",
      "Batch Loss [12/155]: 2.4679\n",
      "\n",
      "Batch Loss [13/155]: 2.7765\n",
      "\n",
      "Batch Loss [14/155]: 2.7184\n",
      "\n",
      "Batch Loss [15/155]: 2.5186\n",
      "\n",
      "Batch Loss [16/155]: 2.5707\n",
      "\n",
      "Batch Loss [17/155]: 2.7563\n",
      "\n",
      "Batch Loss [18/155]: 2.4547\n",
      "\n",
      "Batch Loss [19/155]: 2.5311\n",
      "\n",
      "Batch Loss [20/155]: 2.1665\n",
      "\n",
      "Batch Loss [21/155]: 2.4842\n",
      "\n",
      "Batch Loss [22/155]: 2.7562\n",
      "\n",
      "Batch Loss [23/155]: 3.2144\n",
      "\n",
      "Batch Loss [24/155]: 2.3407\n",
      "\n",
      "Batch Loss [25/155]: 2.8353\n",
      "\n",
      "Batch Loss [26/155]: 2.9414\n",
      "\n",
      "Batch Loss [27/155]: 2.8967\n",
      "\n",
      "Batch Loss [28/155]: 2.5629\n",
      "\n",
      "Batch Loss [29/155]: 2.6213\n",
      "\n",
      "Batch Loss [30/155]: 3.1570\n",
      "\n",
      "Batch Loss [31/155]: 3.1657\n",
      "\n",
      "Batch Loss [32/155]: 2.8460\n",
      "\n",
      "Batch Loss [33/155]: 2.7603\n",
      "\n",
      "Batch Loss [34/155]: 2.5515\n",
      "\n",
      "Batch Loss [35/155]: 2.2473\n",
      "\n",
      "Batch Loss [36/155]: 2.4548\n",
      "\n",
      "Batch Loss [37/155]: 2.9405\n",
      "\n",
      "Batch Loss [38/155]: 3.0382\n",
      "\n",
      "Batch Loss [39/155]: 3.1790\n",
      "\n",
      "Batch Loss [40/155]: 2.5355\n",
      "\n",
      "Batch Loss [41/155]: 2.7783\n",
      "\n",
      "Batch Loss [42/155]: 3.0755\n",
      "\n",
      "Batch Loss [43/155]: 2.9044\n",
      "\n",
      "Batch Loss [44/155]: 2.6267\n",
      "\n",
      "Batch Loss [45/155]: 2.3048\n",
      "\n",
      "Batch Loss [46/155]: 2.4464\n",
      "\n",
      "Batch Loss [47/155]: 2.4994\n",
      "\n",
      "Batch Loss [48/155]: 2.4303\n",
      "\n",
      "Batch Loss [49/155]: 2.6733\n",
      "\n",
      "Batch Loss [50/155]: 2.4284\n",
      "\n",
      "Batch Loss [51/155]: 2.9283\n",
      "\n",
      "Batch Loss [52/155]: 2.7457\n",
      "\n",
      "Batch Loss [53/155]: 3.0904\n",
      "\n",
      "Batch Loss [54/155]: 3.0193\n",
      "\n",
      "Batch Loss [55/155]: 2.8849\n",
      "\n",
      "Batch Loss [56/155]: 3.0368\n",
      "\n",
      "Batch Loss [57/155]: 2.5431\n",
      "\n",
      "Batch Loss [58/155]: 2.6261\n",
      "\n",
      "Batch Loss [59/155]: 2.9119\n",
      "\n",
      "Batch Loss [60/155]: 2.4589\n",
      "\n",
      "Batch Loss [61/155]: 2.7461\n",
      "\n",
      "Batch Loss [62/155]: 2.6232\n",
      "\n",
      "Batch Loss [63/155]: 2.7813\n",
      "\n",
      "Batch Loss [64/155]: 2.7359\n",
      "\n",
      "Batch Loss [65/155]: 2.7123\n",
      "\n",
      "Batch Loss [66/155]: 2.4805\n",
      "\n",
      "Batch Loss [67/155]: 2.9697\n",
      "\n",
      "Batch Loss [68/155]: 2.2610\n",
      "\n",
      "Batch Loss [69/155]: 3.0684\n",
      "\n",
      "Batch Loss [70/155]: 2.2858\n",
      "\n",
      "Batch Loss [71/155]: 2.7875\n",
      "\n",
      "Batch Loss [72/155]: 2.6030\n",
      "\n",
      "Batch Loss [73/155]: 2.5500\n",
      "\n",
      "Batch Loss [74/155]: 2.7619\n",
      "\n",
      "Batch Loss [75/155]: 2.4839\n",
      "\n",
      "Batch Loss [76/155]: 2.7953\n",
      "\n",
      "Batch Loss [77/155]: 2.4839\n",
      "\n",
      "Batch Loss [78/155]: 2.3349\n",
      "\n",
      "Batch Loss [79/155]: 2.9018\n",
      "\n",
      "Batch Loss [80/155]: 3.2548\n",
      "\n",
      "Batch Loss [81/155]: 2.5578\n",
      "\n",
      "Batch Loss [82/155]: 2.4827\n",
      "\n",
      "Batch Loss [83/155]: 2.7984\n",
      "\n",
      "Batch Loss [84/155]: 2.6724\n",
      "\n",
      "Batch Loss [85/155]: 2.8318\n",
      "\n",
      "Batch Loss [86/155]: 2.3049\n",
      "\n",
      "Batch Loss [87/155]: 2.5014\n",
      "\n",
      "Batch Loss [88/155]: 2.6564\n",
      "\n",
      "Batch Loss [89/155]: 2.5197\n",
      "\n",
      "Batch Loss [90/155]: 2.0793\n",
      "\n",
      "Batch Loss [91/155]: 2.2566\n",
      "\n",
      "Batch Loss [92/155]: 2.4875\n",
      "\n",
      "Batch Loss [93/155]: 2.8109\n",
      "\n",
      "Batch Loss [94/155]: 2.7241\n",
      "\n",
      "Batch Loss [95/155]: 2.9457\n",
      "\n",
      "Batch Loss [96/155]: 2.6393\n",
      "\n",
      "Batch Loss [97/155]: 2.6070\n",
      "\n",
      "Batch Loss [98/155]: 2.6601\n",
      "\n",
      "Batch Loss [99/155]: 2.2095\n",
      "\n",
      "Batch Loss [100/155]: 3.1698\n",
      "\n",
      "Batch Loss [101/155]: 2.9843\n",
      "\n",
      "Batch Loss [102/155]: 2.6469\n",
      "\n",
      "Batch Loss [103/155]: 2.5706\n",
      "\n",
      "Batch Loss [104/155]: 2.6509\n",
      "\n",
      "Batch Loss [105/155]: 2.6692\n",
      "\n",
      "Batch Loss [106/155]: 2.3599\n",
      "\n",
      "Batch Loss [107/155]: 2.7956\n",
      "\n",
      "Batch Loss [108/155]: 2.4059\n",
      "\n",
      "Batch Loss [109/155]: 2.4734\n",
      "\n",
      "Batch Loss [110/155]: 2.9100\n",
      "\n",
      "Batch Loss [111/155]: 2.9049\n",
      "\n",
      "Batch Loss [112/155]: 2.4148\n",
      "\n",
      "Batch Loss [113/155]: 2.6384\n",
      "\n",
      "Batch Loss [114/155]: 2.2841\n",
      "\n",
      "Batch Loss [115/155]: 2.5340\n",
      "\n",
      "Batch Loss [116/155]: 2.3307\n",
      "\n",
      "Batch Loss [117/155]: 2.6370\n",
      "\n",
      "Batch Loss [118/155]: 2.5565\n",
      "\n",
      "Batch Loss [119/155]: 2.5149\n",
      "\n",
      "Batch Loss [120/155]: 2.6876\n",
      "\n",
      "Batch Loss [121/155]: 2.9129\n",
      "\n",
      "Batch Loss [122/155]: 2.8542\n",
      "\n",
      "Batch Loss [123/155]: 2.5967\n",
      "\n",
      "Batch Loss [124/155]: 2.4067\n",
      "\n",
      "Batch Loss [125/155]: 2.7740\n",
      "\n",
      "Batch Loss [126/155]: 2.5557\n",
      "\n",
      "Batch Loss [127/155]: 2.2955\n",
      "\n",
      "Batch Loss [128/155]: 2.4826\n",
      "\n",
      "Batch Loss [129/155]: 2.7412\n",
      "\n",
      "Batch Loss [130/155]: 3.0815\n",
      "\n",
      "Batch Loss [131/155]: 2.4222\n",
      "\n",
      "Batch Loss [132/155]: 2.7974\n",
      "\n",
      "Batch Loss [133/155]: 2.6669\n",
      "\n",
      "Batch Loss [134/155]: 2.5830\n",
      "\n",
      "Batch Loss [135/155]: 2.5854\n",
      "\n",
      "Batch Loss [136/155]: 2.3610\n",
      "\n",
      "Batch Loss [137/155]: 2.1183\n",
      "\n",
      "Batch Loss [138/155]: 2.7638\n",
      "\n",
      "Batch Loss [139/155]: 3.0652\n",
      "\n",
      "Batch Loss [140/155]: 2.8710\n",
      "\n",
      "Batch Loss [141/155]: 2.9712\n",
      "\n",
      "Batch Loss [142/155]: 2.6672\n",
      "\n",
      "Batch Loss [143/155]: 2.7830\n",
      "\n",
      "Batch Loss [144/155]: 2.5987\n",
      "\n",
      "Batch Loss [145/155]: 3.2357\n",
      "\n",
      "Batch Loss [146/155]: 2.8015\n",
      "\n",
      "Batch Loss [147/155]: 3.1708\n",
      "\n",
      "Batch Loss [148/155]: 3.0697\n",
      "\n",
      "Batch Loss [149/155]: 2.7349\n",
      "\n",
      "Batch Loss [150/155]: 2.6917\n",
      "\n",
      "Batch Loss [151/155]: 2.5154\n",
      "\n",
      "Batch Loss [152/155]: 2.6131\n",
      "\n",
      "Batch Loss [153/155]: 2.3109\n",
      "\n",
      "Batch Loss [154/155]: 2.9206\n",
      "\n",
      "Batch Loss [155/155]: 2.4146\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94ffdaf456d744669ede978d8b0f5e74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train:  2.6642\n",
      "Test:  Contrastive Loss: 2.5575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/155 [00:03<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5305094a73e74776bbb2a1351c0a90ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss [1/155]: 2.3081\n",
      "\n",
      "Batch Loss [2/155]: 2.4887\n",
      "\n",
      "Batch Loss [3/155]: 2.4743\n",
      "\n",
      "Batch Loss [4/155]: 2.4905\n",
      "\n",
      "Batch Loss [5/155]: 2.8146\n",
      "\n",
      "Batch Loss [6/155]: 2.7982\n",
      "\n",
      "Batch Loss [7/155]: 2.9638\n",
      "\n",
      "Batch Loss [8/155]: 2.4079\n",
      "\n",
      "Batch Loss [9/155]: 2.5997\n",
      "\n",
      "Batch Loss [10/155]: 2.7343\n",
      "\n",
      "Batch Loss [11/155]: 2.5180\n",
      "\n",
      "Batch Loss [12/155]: 2.6350\n",
      "\n",
      "Batch Loss [13/155]: 2.9264\n",
      "\n",
      "Batch Loss [14/155]: 2.8807\n",
      "\n",
      "Batch Loss [15/155]: 2.4665\n",
      "\n",
      "Batch Loss [16/155]: 2.7810\n",
      "\n",
      "Batch Loss [17/155]: 2.1995\n",
      "\n",
      "Batch Loss [18/155]: 2.5400\n",
      "\n",
      "Batch Loss [19/155]: 2.9568\n",
      "\n",
      "Batch Loss [20/155]: 2.1293\n",
      "\n",
      "Batch Loss [21/155]: 2.4708\n",
      "\n",
      "Batch Loss [22/155]: 2.2857\n",
      "\n",
      "Batch Loss [23/155]: 2.6717\n",
      "\n",
      "Batch Loss [24/155]: 2.2578\n",
      "\n",
      "Batch Loss [25/155]: 2.4573\n",
      "\n",
      "Batch Loss [26/155]: 2.4425\n",
      "\n",
      "Batch Loss [27/155]: 2.5966\n",
      "\n",
      "Batch Loss [28/155]: 2.8571\n",
      "\n",
      "Batch Loss [29/155]: 2.5796\n",
      "\n",
      "Batch Loss [30/155]: 2.2298\n",
      "\n",
      "Batch Loss [31/155]: 2.4132\n",
      "\n",
      "Batch Loss [32/155]: 2.4424\n",
      "\n",
      "Batch Loss [33/155]: 2.6939\n",
      "\n",
      "Batch Loss [34/155]: 2.5359\n",
      "\n",
      "Batch Loss [35/155]: 3.1215\n",
      "\n",
      "Batch Loss [36/155]: 2.9160\n",
      "\n",
      "Batch Loss [37/155]: 2.2973\n",
      "\n",
      "Batch Loss [38/155]: 2.6017\n",
      "\n",
      "Batch Loss [39/155]: 3.2859\n",
      "\n",
      "Batch Loss [40/155]: 2.5384\n",
      "\n",
      "Batch Loss [41/155]: 2.7106\n",
      "\n",
      "Batch Loss [42/155]: 2.2124\n",
      "\n",
      "Batch Loss [43/155]: 2.4140\n",
      "\n",
      "Batch Loss [44/155]: 3.0998\n",
      "\n",
      "Batch Loss [45/155]: 2.5819\n",
      "\n",
      "Batch Loss [46/155]: 2.1610\n",
      "\n",
      "Batch Loss [47/155]: 2.4236\n",
      "\n",
      "Batch Loss [48/155]: 2.5778\n",
      "\n",
      "Batch Loss [49/155]: 2.6951\n",
      "\n",
      "Batch Loss [50/155]: 2.3755\n",
      "\n",
      "Batch Loss [51/155]: 2.5243\n",
      "\n",
      "Batch Loss [52/155]: 2.3811\n",
      "\n",
      "Batch Loss [53/155]: 2.4435\n",
      "\n",
      "Batch Loss [54/155]: 2.5956\n",
      "\n",
      "Batch Loss [55/155]: 2.0115\n",
      "\n",
      "Batch Loss [56/155]: 2.8089\n",
      "\n",
      "Batch Loss [57/155]: 2.1828\n",
      "\n",
      "Batch Loss [58/155]: 2.6706\n",
      "\n",
      "Batch Loss [59/155]: 2.6527\n",
      "\n",
      "Batch Loss [60/155]: 2.4397\n",
      "\n",
      "Batch Loss [61/155]: 2.4805\n",
      "\n",
      "Batch Loss [62/155]: 2.0698\n",
      "\n",
      "Batch Loss [63/155]: 2.9179\n",
      "\n",
      "Batch Loss [64/155]: 2.5497\n",
      "\n",
      "Batch Loss [65/155]: 2.4749\n",
      "\n",
      "Batch Loss [66/155]: 2.3162\n",
      "\n",
      "Batch Loss [67/155]: 2.7925\n",
      "\n",
      "Batch Loss [68/155]: 2.8421\n",
      "\n",
      "Batch Loss [69/155]: 2.5664\n",
      "\n",
      "Batch Loss [70/155]: 2.2634\n",
      "\n",
      "Batch Loss [71/155]: 3.0113\n",
      "\n",
      "Batch Loss [72/155]: 2.3375\n",
      "\n",
      "Batch Loss [73/155]: 2.7539\n",
      "\n",
      "Batch Loss [74/155]: 2.6734\n",
      "\n",
      "Batch Loss [75/155]: 2.7632\n",
      "\n",
      "Batch Loss [76/155]: 3.1652\n",
      "\n",
      "Batch Loss [77/155]: 2.6014\n",
      "\n",
      "Batch Loss [78/155]: 2.7875\n",
      "\n",
      "Batch Loss [79/155]: 2.6436\n",
      "\n",
      "Batch Loss [80/155]: 2.6202\n",
      "\n",
      "Batch Loss [81/155]: 2.7640\n",
      "\n",
      "Batch Loss [82/155]: 2.6554\n",
      "\n",
      "Batch Loss [83/155]: 2.6527\n",
      "\n",
      "Batch Loss [84/155]: 2.3327\n",
      "\n",
      "Batch Loss [85/155]: 2.6849\n",
      "\n",
      "Batch Loss [86/155]: 2.4267\n",
      "\n",
      "Batch Loss [87/155]: 2.5033\n",
      "\n",
      "Batch Loss [88/155]: 2.6128\n",
      "\n",
      "Batch Loss [89/155]: 2.5591\n",
      "\n",
      "Batch Loss [90/155]: 2.5361\n",
      "\n",
      "Batch Loss [91/155]: 2.8073\n",
      "\n",
      "Batch Loss [92/155]: 2.4328\n",
      "\n",
      "Batch Loss [93/155]: 2.5986\n",
      "\n",
      "Batch Loss [94/155]: 2.2965\n",
      "\n",
      "Batch Loss [95/155]: 2.4818\n",
      "\n",
      "Batch Loss [96/155]: 2.3047\n",
      "\n",
      "Batch Loss [97/155]: 2.3443\n",
      "\n",
      "Batch Loss [98/155]: 2.4091\n",
      "\n",
      "Batch Loss [99/155]: 2.6614\n",
      "\n",
      "Batch Loss [100/155]: 2.5961\n",
      "\n",
      "Batch Loss [101/155]: 2.1475\n",
      "\n",
      "Batch Loss [102/155]: 2.5929\n",
      "\n",
      "Batch Loss [103/155]: 2.7818\n",
      "\n",
      "Batch Loss [104/155]: 2.5259\n",
      "\n",
      "Batch Loss [105/155]: 2.2976\n",
      "\n",
      "Batch Loss [106/155]: 2.5479\n",
      "\n",
      "Batch Loss [107/155]: 2.5770\n",
      "\n",
      "Batch Loss [108/155]: 2.8771\n",
      "\n",
      "Batch Loss [109/155]: 2.7533\n",
      "\n",
      "Batch Loss [110/155]: 2.6196\n",
      "\n",
      "Batch Loss [111/155]: 2.5003\n",
      "\n",
      "Batch Loss [112/155]: 2.5479\n",
      "\n",
      "Batch Loss [113/155]: 2.9427\n",
      "\n",
      "Batch Loss [114/155]: 2.7676\n",
      "\n",
      "Batch Loss [115/155]: 2.8010\n",
      "\n",
      "Batch Loss [116/155]: 2.9585\n",
      "\n",
      "Batch Loss [117/155]: 2.6093\n",
      "\n",
      "Batch Loss [118/155]: 2.2235\n",
      "\n",
      "Batch Loss [119/155]: 2.4053\n",
      "\n",
      "Batch Loss [120/155]: 2.7193\n",
      "\n",
      "Batch Loss [121/155]: 2.7568\n",
      "\n",
      "Batch Loss [122/155]: 2.7466\n",
      "\n",
      "Batch Loss [123/155]: 2.8534\n",
      "\n",
      "Batch Loss [124/155]: 2.6348\n",
      "\n",
      "Batch Loss [125/155]: 2.6731\n",
      "\n",
      "Batch Loss [126/155]: 2.7025\n",
      "\n",
      "Batch Loss [127/155]: 2.5594\n",
      "\n",
      "Batch Loss [128/155]: 2.8659\n",
      "\n",
      "Batch Loss [129/155]: 2.8514\n",
      "\n",
      "Batch Loss [130/155]: 2.6415\n",
      "\n",
      "Batch Loss [131/155]: 2.2279\n",
      "\n",
      "Batch Loss [132/155]: 3.1070\n",
      "\n",
      "Batch Loss [133/155]: 2.8268\n",
      "\n",
      "Batch Loss [134/155]: 2.9859\n",
      "\n",
      "Batch Loss [135/155]: 2.6475\n",
      "\n",
      "Batch Loss [136/155]: 2.5949\n",
      "\n",
      "Batch Loss [137/155]: 2.5451\n",
      "\n",
      "Batch Loss [138/155]: 2.4594\n",
      "\n",
      "Batch Loss [139/155]: 2.7057\n",
      "\n",
      "Batch Loss [140/155]: 2.5885\n",
      "\n",
      "Batch Loss [141/155]: 2.2975\n",
      "\n",
      "Batch Loss [142/155]: 2.9541\n",
      "\n",
      "Batch Loss [143/155]: 2.2840\n",
      "\n",
      "Batch Loss [144/155]: 2.5498\n",
      "\n",
      "Batch Loss [145/155]: 2.5053\n",
      "\n",
      "Batch Loss [146/155]: 2.4393\n",
      "\n",
      "Batch Loss [147/155]: 2.3981\n",
      "\n",
      "Batch Loss [148/155]: 2.5554\n",
      "\n",
      "Batch Loss [149/155]: 2.6158\n",
      "\n",
      "Batch Loss [150/155]: 2.3341\n",
      "\n",
      "Batch Loss [151/155]: 2.6318\n",
      "\n",
      "Batch Loss [152/155]: 2.3053\n",
      "\n",
      "Batch Loss [153/155]: 2.7304\n",
      "\n",
      "Batch Loss [154/155]: 2.3497\n",
      "\n",
      "Batch Loss [155/155]: 2.2125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eebbba46010646e3b2b2445be7d1bb29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train:  2.5832\n",
      "Test:  Contrastive Loss: 2.5203\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T18:57:45.178303200Z",
     "start_time": "2025-09-13T07:48:31.535382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training.autoencoding_training import train_autoencode\n",
    "train_autoencode(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ],
   "id": "ab759fc97a03a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5b2812346964ee388eb2800b402b8e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid-Epoch Loss [1/98]: 1.855\n",
      "\n",
      "Mid-Epoch Loss [2/98]: 1.814\n",
      "\n",
      "Mid-Epoch Loss [3/98]: 1.78\n",
      "\n",
      "Mid-Epoch Loss [4/98]: 1.751\n",
      "\n",
      "Mid-Epoch Loss [5/98]: 1.729\n",
      "\n",
      "Mid-Epoch Loss [6/98]: 1.707\n",
      "\n",
      "Mid-Epoch Loss [7/98]: 1.689\n",
      "\n",
      "Mid-Epoch Loss [8/98]: 1.675\n",
      "\n",
      "Mid-Epoch Loss [9/98]: 1.66\n",
      "\n",
      "Mid-Epoch Loss [10/98]: 1.648\n",
      "\n",
      "Mid-Epoch Loss [11/98]: 1.635\n",
      "\n",
      "Mid-Epoch Loss [12/98]: 1.623\n",
      "\n",
      "Mid-Epoch Loss [13/98]: 1.611\n",
      "\n",
      "Mid-Epoch Loss [14/98]: 1.6\n",
      "\n",
      "Mid-Epoch Loss [15/98]: 1.589\n",
      "\n",
      "Mid-Epoch Loss [16/98]: 1.577\n",
      "\n",
      "Mid-Epoch Loss [17/98]: 1.566\n",
      "\n",
      "Mid-Epoch Loss [18/98]: 1.556\n",
      "\n",
      "Mid-Epoch Loss [19/98]: 1.546\n",
      "\n",
      "Mid-Epoch Loss [20/98]: 1.536\n",
      "\n",
      "Mid-Epoch Loss [21/98]: 1.527\n",
      "\n",
      "Mid-Epoch Loss [22/98]: 1.517\n",
      "\n",
      "Mid-Epoch Loss [23/98]: 1.507\n",
      "\n",
      "Mid-Epoch Loss [24/98]: 1.499\n",
      "\n",
      "Mid-Epoch Loss [25/98]: 1.49\n",
      "\n",
      "Mid-Epoch Loss [26/98]: 1.481\n",
      "\n",
      "Mid-Epoch Loss [27/98]: 1.473\n",
      "\n",
      "Mid-Epoch Loss [28/98]: 1.465\n",
      "\n",
      "Mid-Epoch Loss [29/98]: 1.457\n",
      "\n",
      "Mid-Epoch Loss [30/98]: 1.449\n",
      "\n",
      "Mid-Epoch Loss [31/98]: 1.442\n",
      "\n",
      "Mid-Epoch Loss [32/98]: 1.434\n",
      "\n",
      "Mid-Epoch Loss [33/98]: 1.427\n",
      "\n",
      "Mid-Epoch Loss [34/98]: 1.421\n",
      "\n",
      "Mid-Epoch Loss [35/98]: 1.414\n",
      "\n",
      "Mid-Epoch Loss [36/98]: 1.407\n",
      "\n",
      "Mid-Epoch Loss [37/98]: 1.401\n",
      "\n",
      "Mid-Epoch Loss [38/98]: 1.394\n",
      "\n",
      "Mid-Epoch Loss [39/98]: 1.388\n",
      "\n",
      "Mid-Epoch Loss [40/98]: 1.381\n",
      "\n",
      "Mid-Epoch Loss [41/98]: 1.375\n",
      "\n",
      "Mid-Epoch Loss [42/98]: 1.369\n",
      "\n",
      "Mid-Epoch Loss [43/98]: 1.363\n",
      "\n",
      "Mid-Epoch Loss [44/98]: 1.357\n",
      "\n",
      "Mid-Epoch Loss [45/98]: 1.351\n",
      "\n",
      "Mid-Epoch Loss [46/98]: 1.345\n",
      "\n",
      "Mid-Epoch Loss [47/98]: 1.339\n",
      "\n",
      "Mid-Epoch Loss [48/98]: 1.333\n",
      "\n",
      "Mid-Epoch Loss [49/98]: 1.327\n",
      "\n",
      "Mid-Epoch Loss [50/98]: 1.322\n",
      "\n",
      "Mid-Epoch Loss [51/98]: 1.316\n",
      "\n",
      "Mid-Epoch Loss [52/98]: 1.31\n",
      "\n",
      "Mid-Epoch Loss [53/98]: 1.305\n",
      "\n",
      "Mid-Epoch Loss [54/98]: 1.299\n",
      "\n",
      "Mid-Epoch Loss [55/98]: 1.293\n",
      "\n",
      "Mid-Epoch Loss [56/98]: 1.288\n",
      "\n",
      "Mid-Epoch Loss [57/98]: 1.282\n",
      "\n",
      "Mid-Epoch Loss [58/98]: 1.277\n",
      "\n",
      "Mid-Epoch Loss [59/98]: 1.272\n",
      "\n",
      "Mid-Epoch Loss [60/98]: 1.267\n",
      "\n",
      "Mid-Epoch Loss [61/98]: 1.262\n",
      "\n",
      "Mid-Epoch Loss [62/98]: 1.257\n",
      "\n",
      "Mid-Epoch Loss [63/98]: 1.251\n",
      "\n",
      "Mid-Epoch Loss [64/98]: 1.247\n",
      "\n",
      "Mid-Epoch Loss [65/98]: 1.242\n",
      "\n",
      "Mid-Epoch Loss [66/98]: 1.237\n",
      "\n",
      "Mid-Epoch Loss [67/98]: 1.232\n",
      "\n",
      "Mid-Epoch Loss [68/98]: 1.227\n",
      "\n",
      "Mid-Epoch Loss [69/98]: 1.223\n",
      "\n",
      "Mid-Epoch Loss [70/98]: 1.218\n",
      "\n",
      "Mid-Epoch Loss [71/98]: 1.213\n",
      "\n",
      "Mid-Epoch Loss [72/98]: 1.209\n",
      "\n",
      "Mid-Epoch Loss [73/98]: 1.204\n",
      "\n",
      "Mid-Epoch Loss [74/98]: 1.2\n",
      "\n",
      "Mid-Epoch Loss [75/98]: 1.196\n",
      "\n",
      "Mid-Epoch Loss [76/98]: 1.192\n",
      "\n",
      "Mid-Epoch Loss [77/98]: 1.187\n",
      "\n",
      "Mid-Epoch Loss [78/98]: 1.183\n",
      "\n",
      "Mid-Epoch Loss [79/98]: 1.179\n",
      "\n",
      "Mid-Epoch Loss [80/98]: 1.175\n",
      "\n",
      "Mid-Epoch Loss [81/98]: 1.171\n",
      "\n",
      "Mid-Epoch Loss [82/98]: 1.167\n",
      "\n",
      "Mid-Epoch Loss [83/98]: 1.163\n",
      "\n",
      "Mid-Epoch Loss [84/98]: 1.159\n",
      "\n",
      "Mid-Epoch Loss [85/98]: 1.155\n",
      "\n",
      "Mid-Epoch Loss [86/98]: 1.152\n",
      "\n",
      "Mid-Epoch Loss [87/98]: 1.148\n",
      "\n",
      "Mid-Epoch Loss [88/98]: 1.144\n",
      "\n",
      "Mid-Epoch Loss [89/98]: 1.14\n",
      "\n",
      "Mid-Epoch Loss [90/98]: 1.137\n",
      "\n",
      "Mid-Epoch Loss [91/98]: 1.133\n",
      "\n",
      "Mid-Epoch Loss [92/98]: 1.13\n",
      "\n",
      "Mid-Epoch Loss [93/98]: 1.126\n",
      "\n",
      "Mid-Epoch Loss [94/98]: 1.123\n",
      "\n",
      "Mid-Epoch Loss [95/98]: 1.119\n",
      "\n",
      "Mid-Epoch Loss [96/98]: 1.116\n",
      "\n",
      "Mid-Epoch Loss [97/98]: 1.113\n",
      "\n",
      "Mid-Epoch Loss [98/98]: 1.109\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a8551bcf77748859721392c4330e4ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train:  1.1093\n",
      "Test:  Cos: 0.4985, MSE: 0.7842\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T18:57:45.180303800Z",
     "start_time": "2025-09-13T02:25:21.349957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training.autoencoding_training import train_autoencode\n",
    "train_autoencode(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ],
   "id": "bb6af62a7e1524ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca785c632b3f4aed866489b6809a3195"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid-Epoch Loss [1/98]: 0.7627\n",
      "\n",
      "Mid-Epoch Loss [2/98]: 0.7602\n",
      "\n",
      "Mid-Epoch Loss [3/98]: 0.6974\n",
      "\n",
      "Mid-Epoch Loss [4/98]: 0.6474\n",
      "\n",
      "Mid-Epoch Loss [5/98]: 0.6145\n",
      "\n",
      "Mid-Epoch Loss [6/98]: 0.5882\n",
      "\n",
      "Mid-Epoch Loss [7/98]: 0.5669\n",
      "\n",
      "Mid-Epoch Loss [8/98]: 0.551\n",
      "\n",
      "Mid-Epoch Loss [9/98]: 0.535\n",
      "\n",
      "Mid-Epoch Loss [10/98]: 0.5224\n",
      "\n",
      "Mid-Epoch Loss [11/98]: 0.5126\n",
      "\n",
      "Mid-Epoch Loss [12/98]: 0.5031\n",
      "\n",
      "Mid-Epoch Loss [13/98]: 0.4956\n",
      "\n",
      "Mid-Epoch Loss [14/98]: 0.4897\n",
      "\n",
      "Mid-Epoch Loss [15/98]: 0.4836\n",
      "\n",
      "Mid-Epoch Loss [16/98]: 0.4773\n",
      "\n",
      "Mid-Epoch Loss [17/98]: 0.4733\n",
      "\n",
      "Mid-Epoch Loss [18/98]: 0.4682\n",
      "\n",
      "Mid-Epoch Loss [19/98]: 0.4636\n",
      "\n",
      "Mid-Epoch Loss [20/98]: 0.4598\n",
      "\n",
      "Mid-Epoch Loss [21/98]: 0.4568\n",
      "\n",
      "Mid-Epoch Loss [22/98]: 0.4535\n",
      "\n",
      "Mid-Epoch Loss [23/98]: 0.4503\n",
      "\n",
      "Mid-Epoch Loss [24/98]: 0.4477\n",
      "\n",
      "Mid-Epoch Loss [25/98]: 0.4455\n",
      "\n",
      "Mid-Epoch Loss [26/98]: 0.4429\n",
      "\n",
      "Mid-Epoch Loss [27/98]: 0.4401\n",
      "\n",
      "Mid-Epoch Loss [28/98]: 0.4383\n",
      "\n",
      "Mid-Epoch Loss [29/98]: 0.4359\n",
      "\n",
      "Mid-Epoch Loss [30/98]: 0.4338\n",
      "\n",
      "Mid-Epoch Loss [31/98]: 0.4319\n",
      "\n",
      "Mid-Epoch Loss [32/98]: 0.4299\n",
      "\n",
      "Mid-Epoch Loss [33/98]: 0.4281\n",
      "\n",
      "Mid-Epoch Loss [34/98]: 0.4257\n",
      "\n",
      "Mid-Epoch Loss [35/98]: 0.4243\n",
      "\n",
      "Mid-Epoch Loss [36/98]: 0.4226\n",
      "\n",
      "Mid-Epoch Loss [37/98]: 0.4213\n",
      "\n",
      "Mid-Epoch Loss [38/98]: 0.4198\n",
      "\n",
      "Mid-Epoch Loss [39/98]: 0.4183\n",
      "\n",
      "Mid-Epoch Loss [40/98]: 0.4172\n",
      "\n",
      "Mid-Epoch Loss [41/98]: 0.4159\n",
      "\n",
      "Mid-Epoch Loss [42/98]: 0.4151\n",
      "\n",
      "Mid-Epoch Loss [43/98]: 0.4137\n",
      "\n",
      "Mid-Epoch Loss [44/98]: 0.4125\n",
      "\n",
      "Mid-Epoch Loss [45/98]: 0.4115\n",
      "\n",
      "Mid-Epoch Loss [46/98]: 0.4103\n",
      "\n",
      "Mid-Epoch Loss [47/98]: 0.4094\n",
      "\n",
      "Mid-Epoch Loss [48/98]: 0.4082\n",
      "\n",
      "Mid-Epoch Loss [49/98]: 0.4069\n",
      "\n",
      "Mid-Epoch Loss [50/98]: 0.4059\n",
      "\n",
      "Mid-Epoch Loss [51/98]: 0.4051\n",
      "\n",
      "Mid-Epoch Loss [52/98]: 0.4042\n",
      "\n",
      "Mid-Epoch Loss [53/98]: 0.4035\n",
      "\n",
      "Mid-Epoch Loss [54/98]: 0.4026\n",
      "\n",
      "Mid-Epoch Loss [55/98]: 0.4019\n",
      "\n",
      "Mid-Epoch Loss [56/98]: 0.4008\n",
      "\n",
      "Mid-Epoch Loss [57/98]: 0.4007\n",
      "\n",
      "Mid-Epoch Loss [58/98]: 0.3997\n",
      "\n",
      "Mid-Epoch Loss [59/98]: 0.399\n",
      "\n",
      "Mid-Epoch Loss [60/98]: 0.3984\n",
      "\n",
      "Mid-Epoch Loss [61/98]: 0.3975\n",
      "\n",
      "Mid-Epoch Loss [62/98]: 0.3968\n",
      "\n",
      "Mid-Epoch Loss [63/98]: 0.3959\n",
      "\n",
      "Mid-Epoch Loss [64/98]: 0.3952\n",
      "\n",
      "Mid-Epoch Loss [65/98]: 0.3945\n",
      "\n",
      "Mid-Epoch Loss [66/98]: 0.3939\n",
      "\n",
      "Mid-Epoch Loss [67/98]: 0.3932\n",
      "\n",
      "Mid-Epoch Loss [68/98]: 0.3926\n",
      "\n",
      "Mid-Epoch Loss [69/98]: 0.3918\n",
      "\n",
      "Mid-Epoch Loss [70/98]: 0.3911\n",
      "\n",
      "Mid-Epoch Loss [71/98]: 0.3904\n",
      "\n",
      "Mid-Epoch Loss [72/98]: 0.3901\n",
      "\n",
      "Mid-Epoch Loss [73/98]: 0.3896\n",
      "\n",
      "Mid-Epoch Loss [74/98]: 0.3892\n",
      "\n",
      "Mid-Epoch Loss [75/98]: 0.3886\n",
      "\n",
      "Mid-Epoch Loss [76/98]: 0.3881\n",
      "\n",
      "Mid-Epoch Loss [77/98]: 0.3877\n",
      "\n",
      "Mid-Epoch Loss [78/98]: 0.387\n",
      "\n",
      "Mid-Epoch Loss [79/98]: 0.3866\n",
      "\n",
      "Mid-Epoch Loss [80/98]: 0.3863\n",
      "\n",
      "Mid-Epoch Loss [81/98]: 0.386\n",
      "\n",
      "Mid-Epoch Loss [82/98]: 0.3854\n",
      "\n",
      "Mid-Epoch Loss [83/98]: 0.385\n",
      "\n",
      "Mid-Epoch Loss [84/98]: 0.3845\n",
      "\n",
      "Mid-Epoch Loss [85/98]: 0.3841\n",
      "\n",
      "Mid-Epoch Loss [86/98]: 0.3837\n",
      "\n",
      "Mid-Epoch Loss [87/98]: 0.3831\n",
      "\n",
      "Mid-Epoch Loss [88/98]: 0.3828\n",
      "\n",
      "Mid-Epoch Loss [89/98]: 0.3824\n",
      "\n",
      "Mid-Epoch Loss [90/98]: 0.3821\n",
      "\n",
      "Mid-Epoch Loss [91/98]: 0.3817\n",
      "\n",
      "Mid-Epoch Loss [92/98]: 0.3815\n",
      "\n",
      "Mid-Epoch Loss [93/98]: 0.3813\n",
      "\n",
      "Mid-Epoch Loss [94/98]: 0.3811\n",
      "\n",
      "Mid-Epoch Loss [95/98]: 0.3809\n",
      "\n",
      "Mid-Epoch Loss [96/98]: 0.3806\n",
      "\n",
      "Mid-Epoch Loss [97/98]: 0.3801\n",
      "\n",
      "Mid-Epoch Loss [98/98]: 0.3797\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a065a50b271f4a78bbdc14f45e3542bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train:  0.3797\n",
      "Test:  Cos: 0.2006, MSE: 0.3512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdcfe378e3174e2a9256a71ef45cf705"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid-Epoch Loss [1/98]: 0.3538\n",
      "\n",
      "Mid-Epoch Loss [2/98]: 0.3486\n",
      "\n",
      "Mid-Epoch Loss [3/98]: 0.35\n",
      "\n",
      "Mid-Epoch Loss [4/98]: 0.351\n",
      "\n",
      "Mid-Epoch Loss [5/98]: 0.3511\n",
      "\n",
      "Mid-Epoch Loss [6/98]: 0.3497\n",
      "\n",
      "Mid-Epoch Loss [7/98]: 0.3504\n",
      "\n",
      "Mid-Epoch Loss [8/98]: 0.348\n",
      "\n",
      "Mid-Epoch Loss [9/98]: 0.3482\n",
      "\n",
      "Mid-Epoch Loss [10/98]: 0.3481\n",
      "\n",
      "Mid-Epoch Loss [11/98]: 0.3483\n",
      "\n",
      "Mid-Epoch Loss [12/98]: 0.3478\n",
      "\n",
      "Mid-Epoch Loss [13/98]: 0.3489\n",
      "\n",
      "Mid-Epoch Loss [14/98]: 0.3499\n",
      "\n",
      "Mid-Epoch Loss [15/98]: 0.3486\n",
      "\n",
      "Mid-Epoch Loss [16/98]: 0.3487\n",
      "\n",
      "Mid-Epoch Loss [17/98]: 0.3483\n",
      "\n",
      "Mid-Epoch Loss [18/98]: 0.3492\n",
      "\n",
      "Mid-Epoch Loss [19/98]: 0.3497\n",
      "\n",
      "Mid-Epoch Loss [20/98]: 0.3502\n",
      "\n",
      "Mid-Epoch Loss [21/98]: 0.3498\n",
      "\n",
      "Mid-Epoch Loss [22/98]: 0.3493\n",
      "\n",
      "Mid-Epoch Loss [23/98]: 0.3498\n",
      "\n",
      "Mid-Epoch Loss [24/98]: 0.3501\n",
      "\n",
      "Mid-Epoch Loss [25/98]: 0.3501\n",
      "\n",
      "Mid-Epoch Loss [26/98]: 0.3501\n",
      "\n",
      "Mid-Epoch Loss [27/98]: 0.3506\n",
      "\n",
      "Mid-Epoch Loss [28/98]: 0.3512\n",
      "\n",
      "Mid-Epoch Loss [29/98]: 0.3512\n",
      "\n",
      "Mid-Epoch Loss [30/98]: 0.3513\n",
      "\n",
      "Mid-Epoch Loss [31/98]: 0.3517\n",
      "\n",
      "Mid-Epoch Loss [32/98]: 0.3515\n",
      "\n",
      "Mid-Epoch Loss [33/98]: 0.3519\n",
      "\n",
      "Mid-Epoch Loss [34/98]: 0.3525\n",
      "\n",
      "Mid-Epoch Loss [35/98]: 0.3526\n",
      "\n",
      "Mid-Epoch Loss [36/98]: 0.3525\n",
      "\n",
      "Mid-Epoch Loss [37/98]: 0.3524\n",
      "\n",
      "Mid-Epoch Loss [38/98]: 0.353\n",
      "\n",
      "Mid-Epoch Loss [39/98]: 0.353\n",
      "\n",
      "Mid-Epoch Loss [40/98]: 0.3533\n",
      "\n",
      "Mid-Epoch Loss [41/98]: 0.3533\n",
      "\n",
      "Mid-Epoch Loss [42/98]: 0.3534\n",
      "\n",
      "Mid-Epoch Loss [43/98]: 0.3534\n",
      "\n",
      "Mid-Epoch Loss [44/98]: 0.3537\n",
      "\n",
      "Mid-Epoch Loss [45/98]: 0.3535\n",
      "\n",
      "Mid-Epoch Loss [46/98]: 0.3531\n",
      "\n",
      "Mid-Epoch Loss [47/98]: 0.3531\n",
      "\n",
      "Mid-Epoch Loss [48/98]: 0.3532\n",
      "\n",
      "Mid-Epoch Loss [49/98]: 0.3533\n",
      "\n",
      "Mid-Epoch Loss [50/98]: 0.3533\n",
      "\n",
      "Mid-Epoch Loss [51/98]: 0.3535\n",
      "\n",
      "Mid-Epoch Loss [52/98]: 0.3537\n",
      "\n",
      "Mid-Epoch Loss [53/98]: 0.3537\n",
      "\n",
      "Mid-Epoch Loss [54/98]: 0.354\n",
      "\n",
      "Mid-Epoch Loss [55/98]: 0.3538\n",
      "\n",
      "Mid-Epoch Loss [56/98]: 0.3538\n",
      "\n",
      "Mid-Epoch Loss [57/98]: 0.3533\n",
      "\n",
      "Mid-Epoch Loss [58/98]: 0.3531\n",
      "\n",
      "Mid-Epoch Loss [59/98]: 0.3531\n",
      "\n",
      "Mid-Epoch Loss [60/98]: 0.3531\n",
      "\n",
      "Mid-Epoch Loss [61/98]: 0.3531\n",
      "\n",
      "Mid-Epoch Loss [62/98]: 0.3527\n",
      "\n",
      "Mid-Epoch Loss [63/98]: 0.3525\n",
      "\n",
      "Mid-Epoch Loss [64/98]: 0.353\n",
      "\n",
      "Mid-Epoch Loss [65/98]: 0.3529\n",
      "\n",
      "Mid-Epoch Loss [66/98]: 0.3527\n",
      "\n",
      "Mid-Epoch Loss [67/98]: 0.3526\n",
      "\n",
      "Mid-Epoch Loss [68/98]: 0.3526\n",
      "\n",
      "Mid-Epoch Loss [69/98]: 0.3526\n",
      "\n",
      "Mid-Epoch Loss [70/98]: 0.3526\n",
      "\n",
      "Mid-Epoch Loss [71/98]: 0.3525\n",
      "\n",
      "Mid-Epoch Loss [72/98]: 0.3522\n",
      "\n",
      "Mid-Epoch Loss [73/98]: 0.3521\n",
      "\n",
      "Mid-Epoch Loss [74/98]: 0.3518\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtraining\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautoencoding_training\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_autoencode\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mtrain_autoencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_graph\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\autoencoding_training.py:61\u001B[39m, in \u001B[36mtrain_autoencode\u001B[39m\u001B[34m(model, test_dataloader, train_dataloader, config, show_graph)\u001B[39m\n\u001B[32m     58\u001B[39m loss = criterion(outputs, data_minibatch)\n\u001B[32m     60\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     62\u001B[39m optimizer.step()\n\u001B[32m     64\u001B[39m loss_per_batch += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    571\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    572\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    573\u001B[39m         Tensor.backward,\n\u001B[32m    574\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    579\u001B[39m         inputs=inputs,\n\u001B[32m    580\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m581\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    582\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    583\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    823\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    826\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    827\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    829\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import IPython\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from datasets import tqdm\n",
    "from training.inference import load_and_parse_audio\n",
    "\n",
    "def test(model):\n",
    "    path = \"E:\\\\SongsDataset\\\\songs\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for each_song in tqdm(all_folders[100:110]):\n",
    "            song_path = os.path.join(path, each_song)\n",
    "            chunks = load_and_parse_audio(song_path, convert=True, chunk_size=1024).to(\"cuda\")\n",
    "            permuted_chunks = torch.stack([c for c in chunks])\n",
    "\n",
    "            mean = permuted_chunks.mean(dim=[1, 2], keepdim=True)\n",
    "            std = permuted_chunks.std(dim=[1, 2], keepdim=True)\n",
    "\n",
    "            permuted_chunks = (permuted_chunks - mean) / (std + 1e-6)\n",
    "\n",
    "            reconstructed, latent = model(permuted_chunks)\n",
    "\n",
    "            input_tensor = np.concatenate(permuted_chunks.cpu().detach().numpy(), axis=1)\n",
    "            reconstructed = np.concatenate(reconstructed.cpu().detach().numpy(), axis=1)\n",
    "\n",
    "            input_tensor = input_tensor[:, :512]\n",
    "            reconstructed = reconstructed[:, :512]\n",
    "\n",
    "            graph(input_tensor, reconstructed)\n",
    "\n",
    "            S_recon = librosa.feature.inverse.mel_to_stft(reconstructed)\n",
    "            Y_recon = librosa.griffinlim(S_recon)\n",
    "\n",
    "            S_orig = librosa.feature.inverse.mel_to_stft(input_tensor)\n",
    "            Y_orig = librosa.griffinlim(S_orig)\n",
    "\n",
    "            IPython.display.display(IPython.display.Audio(Y_orig, rate=44100))\n",
    "            IPython.display.display(IPython.display.Audio(Y_recon, rate=44100))"
   ],
   "id": "ba82afdf885f8438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test(model)",
   "id": "5f270a44ee4bf4dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
