{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data.processing import ParseBalanced\n",
    "\n",
    "directory = \"melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "ParseBalanced(subset_file_name, f\"{data_directory}\", f\"D:/SongsDataset/{directory}\", convert=True, target_per_genre=1300, chunk_size=256, chunks_per_batch=1, write_individually=True)"
   ],
   "id": "60706242edea8ffb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-08T03:11:27.937052Z",
     "start_time": "2025-10-08T03:11:20.258171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchaudio.transforms import TimeMasking, FrequencyMasking\n",
    "from info_nce import InfoNCE\n",
    "from data.data_utils import *\n",
    "\n",
    "directory = \"melspec-dataset-top-50-LIBROSA-256-Triplet\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "\n",
    "# augmentations = Compose([\n",
    "#     AddGaussianNoise(std=0.25),\n",
    "# ])\n",
    "\n",
    "class Config:\n",
    "    # === General ===\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    model_name = \"ViT-Contrastive-Embeddings-Masking-0.9\"\n",
    "    save_path = f\"trained_models\\\\{model_name}\\\\\"\n",
    "    seed = 42\n",
    "\n",
    "    # === Training ===\n",
    "    num_classes = 50\n",
    "    num_epochs = 128\n",
    "    batch_size = 128\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    coef = 1\n",
    "    cycles = 42\n",
    "    warmup = 0\n",
    "    gamma = 2.0\n",
    "\n",
    "    # === Dataset ===\n",
    "    transforms = None\n",
    "    use_masks = True\n",
    "    num_workers = 2\n",
    "    prefetch_factor = 1\n",
    "    val_split = 0.1\n",
    "    #pos_weight = (torch.ones(num_classes) * 50).to(\"cuda\")\n",
    "    criterion = InfoNCE()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:11:30.057510Z",
     "start_time": "2025-10-08T03:11:27.944318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "large_directory = directory\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\data\", f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\genre_labels\", pair_album=True)\n",
    "test_dataset  = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\data\", f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\genre_labels\", pair_album=True)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")"
   ],
   "id": "25b92472f73db5a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 54380 tracks, 11107 albums, 3517 artists\n",
      "Reading: 54380 tracks, 11107 albums, 3517 artists\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.AudioPreCNNTransformer import AudioPreCNNTransformer\n",
    "from utils import misc\n",
    "\n",
    "model = AudioPreCNNTransformer(latent_space=512, input_dim=128, length=1024, num_heads=8, transformer_layers=8, d_model=256, dropout=0.1)\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "3cc0b5351b3b35fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.AudioTransformer import AudioTransformer\n",
    "from utils import misc\n",
    "\n",
    "model = AudioTransformer(latent_space=32, input_dim=128, d_model=256, dim_feedforward=512, length=256, num_heads=8, encoder_layers=16, decoder_layers=16, dropout=0.1, use_alibi=True, custom_slopes=True)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "b699c79369ea9dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.AudioTransformerWeaved import AudioTransformerWeaved\n",
    "from utils import misc\n",
    "\n",
    "model = AudioTransformerWeaved(latent_space=128, input_dim=128, d_model=256, dim_feedforward=512, length=1024, num_heads=8, encoder_layers=8, decoder_layers=8, dropout=0.1, use_alibi=True)\n",
    "print(f\"{misc.model_size(model)} Parameters\")"
   ],
   "id": "92dabd0320a8ecaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-08T03:12:16.213041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training.contrastive_training import train_contrastive\n",
    "from models.AudioViTEncoder import AudioViTEncoder\n",
    "from utils import misc\n",
    "\n",
    "Config.model_name = \"ViT-Contrastive-Embeddings-Masking-0.9-Variational-Full-Album\"\n",
    "Config.save_path = f\"trained_models\\\\{Config.model_name}\\\\\"\n",
    "\n",
    "model = torch.load(\"E:/Coding/SongAnalyzer/Analyzer/src/trained_models/ViT-Contrastive-Embeddings-Masking-0.9-Variational-Full/Classifier-Epoch-63.pt\", weights_only=False)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=True, train_masked=True, test_masked=False)"
   ],
   "id": "2e6f15ad34c069a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8123008 Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/79 [00:11<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "975c18cbe1d84cdeb81419c7985ef1a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from training.contrastive_training import train_contrastive\n",
    "from models.AudioViTEncoder import AudioViTEncoder\n",
    "from utils import misc\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "large_directory = directory\n",
    "\n",
    "train_dataset = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\data\", f\"D:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\genre_labels\", pair_album=True)\n",
    "test_dataset  = StreamViewDataset(f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\data\", f\"D:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\genre_labels\", pair_album=True)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")\n",
    "\n",
    "Config.model_name = \"ViT-Contrastive-Embeddings-Masking-0.9-Variational-Full-Album\"\n",
    "Config.save_path = f\"trained_models\\\\{Config.model_name}\\\\\"\n",
    "\n",
    "model = torch.load(\"E:/Coding/SongAnalyzer/Analyzer/src/trained_models/ViT-Contrastive-Embeddings-Masking-0.9-Variational/Classifier-Epoch-36.pt\", weights_only=False)\n",
    "\n",
    "print(f\"{misc.model_size(model)} Parameters\")\n",
    "\n",
    "train_contrastive(model, test_dataloader, train_dataloader, Config, variational=True, train_masked=True, test_masked=False)"
   ],
   "id": "38d4330af515b0a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from training.autoencoding_training import train_autoencode\n",
    "train_autoencode(model, test_dataloader, train_dataloader, Config, show_graph=False)"
   ],
   "id": "bb6af62a7e1524ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import IPython\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from datasets import tqdm\n",
    "from training.inference import load_and_parse_audio\n",
    "\n",
    "def test(model):\n",
    "    path = \"E:\\\\SongsDataset\\\\songs\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for each_song in tqdm(all_folders[100:110]):\n",
    "            song_path = os.path.join(path, each_song)\n",
    "            chunks = load_and_parse_audio(song_path, convert=True, chunk_size=1024).to(\"cuda\")\n",
    "            permuted_chunks = torch.stack([c for c in chunks])\n",
    "\n",
    "            mean = permuted_chunks.mean(dim=[1, 2], keepdim=True)\n",
    "            std = permuted_chunks.std(dim=[1, 2], keepdim=True)\n",
    "\n",
    "            permuted_chunks = (permuted_chunks - mean) / (std + 1e-6)\n",
    "\n",
    "            reconstructed, latent = model(permuted_chunks)\n",
    "\n",
    "            input_tensor = np.concatenate(permuted_chunks.cpu().detach().numpy(), axis=1)\n",
    "            reconstructed = np.concatenate(reconstructed.cpu().detach().numpy(), axis=1)\n",
    "\n",
    "            input_tensor = input_tensor[:, :512]\n",
    "            reconstructed = reconstructed[:, :512]\n",
    "\n",
    "            graph(input_tensor, reconstructed)\n",
    "\n",
    "            S_recon = librosa.feature.inverse.mel_to_stft(reconstructed)\n",
    "            Y_recon = librosa.griffinlim(S_recon)\n",
    "\n",
    "            S_orig = librosa.feature.inverse.mel_to_stft(input_tensor)\n",
    "            Y_orig = librosa.griffinlim(S_orig)\n",
    "\n",
    "            IPython.display.display(IPython.display.Audio(Y_orig, rate=44100))\n",
    "            IPython.display.display(IPython.display.Audio(Y_recon, rate=44100))"
   ],
   "id": "ba82afdf885f8438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test(model)",
   "id": "5f270a44ee4bf4dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
