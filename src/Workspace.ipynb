{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data & Training",
   "id": "ebf89474ab777d38"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-05-13T22:36:00.163351Z",
     "start_time": "2025-05-13T22:36:00.149352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_dataset(length=256):\n",
    "    directory = \"\"  #\"latents\\\\\" #\"reconstruction_test_latents\\\\\" #mtg-jamendo\n",
    "    num_per = 1000\n",
    "    count = 1\n",
    "\n",
    "    for start in tqdm(range(0, 16150, num_per)):\n",
    "        mtg_dataset, masks = retrieve_data(\"E:\\SongsDataset\\\\mtg-jamendo\\\\\", directory, start=start, count=num_per, sample_length=length)\n",
    "        torch.save(mtg_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}.pt\")\n",
    "        torch.save(masks, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}-masks.pt\")\n",
    "        count += 1\n",
    "\n",
    "    for start in tqdm(range(0, 3975, num_per)):\n",
    "        spotify_dataset, masks  = retrieve_data(\"E:\\\\SongsDataset\\\\latents\\\\\", directory, start=start, count=num_per, sample_length=length)\n",
    "        torch.save(spotify_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}.pt\")\n",
    "        torch.save(masks, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}-masks.pt\")\n",
    "        count += 1\n",
    "\n",
    "    full_dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset1.pt\")\n",
    "    full_masks = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset1-masks.pt\")\n",
    "\n",
    "    for start in tqdm(range(2, 21)):\n",
    "        new_data = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{start}.pt\")\n",
    "        new_masks = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{start}-masks.pt\")\n",
    "\n",
    "        full_dataset = torch.cat((full_dataset, new_data))\n",
    "        full_masks = torch.cat((full_masks, new_masks))\n",
    "\n",
    "    bool_masks = [[0 if (length - i) > x else 1 for i in range(length)] for x in full_masks]\n",
    "    bool_masks = torch.tensor(bool_masks).bool()\n",
    "\n",
    "    torch.save(full_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\full_dataset.pt\")\n",
    "    torch.save(bool_masks, f\"E:\\\\SongsDataset\\\\length_{length}\\\\full_masks.pt\")"
   ],
   "id": "b8e1130f3fe4d3cb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:56:52.968362Z",
     "start_time": "2025-05-13T22:36:00.174353Z"
    }
   },
   "cell_type": "code",
   "source": "parse_dataset(256)",
   "id": "5c59e7733cec7e6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]E:\\Coding\\SongAnalyzer\\Analyzer\\src\\Data.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  tens = torch.tensor(dataset).to(torch.float32)\n",
      "100%|██████████| 17/17 [11:41<00:00, 41.24s/it]\n",
      "100%|██████████| 4/4 [02:20<00:00, 35.15s/it]\n",
      "100%|██████████| 19/19 [04:51<00:00, 15.35s/it]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:50:31.413710Z",
     "start_time": "2025-05-30T04:50:12.695418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from Training import train, evaluate\n",
    "from Data import AudioDataset, retrieve_data"
   ],
   "id": "6afff8a0a232dd33",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:50:31.429710Z",
     "start_time": "2025-05-30T04:50:31.419711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==== Model & Optimizer ====\n",
    "num_heads = 16\n",
    "num_layers = 16\n",
    "d_model = 256\n",
    "latent_space = 512\n",
    "dim_feedforward = 1024\n",
    "sample_length = 256\n",
    "projection_dim = 128\n",
    "dropout = 0.1\n",
    "batch_size = 10\n",
    "\n",
    "device = \"cuda\""
   ],
   "id": "1112734e300ad7bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:52:06.011738Z",
     "start_time": "2025-05-30T04:50:32.308835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\full_dataset.pt\")\n",
    "#masks = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\full_masks.pt\")\n",
    "\n",
    "dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\dataset1.pt\")\n",
    "masks = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\dataset1-masks.pt\").bool()"
   ],
   "id": "932e5667035a22a1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:52:06.633351Z",
     "start_time": "2025-05-30T04:52:06.526222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "num_samples, seq_length, embed_dim = dataset.shape\n",
    "\n",
    "collective_dataset = AudioDataset(dataset, masks)\n",
    "\n",
    "train_len = int(len(collective_dataset) * 0.9)\n",
    "train_set, test_set = random_split(collective_dataset, [train_len, len(collective_dataset) - train_len])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ],
   "id": "d91d65a8dceff284",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:53:19.259831Z",
     "start_time": "2025-05-30T04:53:14.528078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Loss import combined_loss\n",
    "from AudioTransformerLatentAttention import AudioTransformerLatentAttention\n",
    "\n",
    "model = AudioTransformerLatentAttention(d_model=d_model, projection_dim=projection_dim, num_heads=num_heads, transformer_layers=num_layers, dim_feedforward=dim_feedforward, latent_space=latent_space, length=sample_length, dropout=dropout, name_extension=\"-fft-cos-256\", use_rope=True, use_alibi=True)\n",
    "model.set_checkpointing(False)"
   ],
   "id": "6fb9d73b1157f0f2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from Loss import combined_loss\n",
    "from AudioTransformer import AudioTransformer\n",
    "\n",
    "model = AudioTransformer(d_model=d_model, num_heads=num_heads, transformer_layers=num_layers, dim_feedforward=dim_feedforward, latent_space=latent_space, length=sample_length, dropout=dropout, name_extension=\"-fft-cos-bidirectional\", use_rope=True, use_alibi=True)\n",
    "model.set_checkpointing(False)"
   ],
   "id": "2185d723bef9ffee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:53:19.322830Z",
     "start_time": "2025-05-30T04:53:19.277829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ],
   "id": "3ad5e047c9361568",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94150464\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:53:23.810234Z",
     "start_time": "2025-05-30T04:53:19.391017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0)"
   ],
   "id": "fb94e024564875f7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T06:26:05.768252Z",
     "start_time": "2025-05-30T05:03:34.312980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, device=device, loss_func=combined_loss)\n",
    "evaluate(model, test_dataloader)"
   ],
   "id": "88f128483e2a08c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18530/18530 [1:02:53<00:00,  4.91it/s]\n",
      "100%|██████████| 2059/2059 [02:16<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 1.4206 \t Validation Loss: 1.3447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5099/18530 [17:18<45:34,  4.91it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombined_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m evaluate(model, test_dataloader)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\Training.py:89\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, test_dataloader, optimizer, num_epochs, device, loss_func, masks)\u001B[0m\n\u001B[0;32m     86\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m reconstructed \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Compute loss (only on masked positions)\u001B[39;00m\n\u001B[0;32m     92\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_func(reconstructed, batch)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\AudioTransformerLatentAttention.py:58\u001B[0m, in \u001B[0;36mAudioTransformerLatentAttention.forward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     57\u001B[0m     memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_latent(x, mask)\n\u001B[1;32m---> 58\u001B[0m     memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_latent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m memory\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\AudioTransformerLatentAttention.py:94\u001B[0m, in \u001B[0;36mAudioTransformerLatentAttention.from_latent\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     91\u001B[0m memory \u001B[38;5;241m=\u001B[39m memory\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), queries\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     93\u001B[0m \u001B[38;5;66;03m# Decoder Block\u001B[39;00m\n\u001B[1;32m---> 94\u001B[0m memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# Project to Output Dimension\u001B[39;00m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpointing:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\LatentAttentionComponents.py:283\u001B[0m, in \u001B[0;36mRoPEALiBiLatentTransformerDecoder.forward\u001B[1;34m(self, tgt, memory, mask)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tgt, memory, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;66;03m# if self.checkpointing:\u001B[39;00m\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;66;03m#     tgt = checkpoint(layer, memory, tgt, self.self_alibi_bias, self.self_pos_emb, self.cross_alibi_bias, self.cross_pos_emb, mask)\u001B[39;00m\n\u001B[0;32m    282\u001B[0m         \u001B[38;5;66;03m# else:\u001B[39;00m\n\u001B[1;32m--> 283\u001B[0m         tgt \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_alibi_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_pos_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_alibi_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_pos_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(tgt)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\LatentAttentionComponents.py:230\u001B[0m, in \u001B[0;36mRoPEALiBiLatentTransformerDecoderLayer.forward\u001B[1;34m(self, memory, tgt, self_alibi_bias, self_pos_emb, cross_alibi_bias, cross_pos_emb, mask)\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, memory, tgt, self_alibi_bias, self_pos_emb, cross_alibi_bias, cross_pos_emb, mask):\n\u001B[0;32m    228\u001B[0m \n\u001B[0;32m    229\u001B[0m     \u001B[38;5;66;03m# Self Attention Block\u001B[39;00m\n\u001B[1;32m--> 230\u001B[0m     tgt2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_alibi_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_pos_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     tgt2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(tgt2)\n\u001B[0;32m    232\u001B[0m     tgt \u001B[38;5;241m=\u001B[39m tgt \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(tgt2)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\LatentAttentionComponents.py:37\u001B[0m, in \u001B[0;36mRoPEALiBiLatentSelfAttention.forward\u001B[1;34m(self, src, alibi_bias, pos_emb, mask)\u001B[0m\n\u001B[0;32m     34\u001B[0m projection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent_proj(src)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Linear projections\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(B, L, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     38\u001B[0m k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_proj(projection)\u001B[38;5;241m.\u001B[39mview(B, L, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     39\u001B[0m v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv_proj(projection)\u001B[38;5;241m.\u001B[39mview(B, L, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T02:23:30.689171Z",
     "start_time": "2025-05-30T02:23:30.164138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, device=device, loss_func=combined_loss)\n",
    "evaluate(model, test_dataloader)"
   ],
   "id": "6fd21035a6b3257",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1009 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Shape: torch.Size([10, 256, 256])\n",
      "q Shape: torch.Size([10, 256, 256])\n",
      "q Shape: torch.Size([10, 16, 256, 16])\n",
      "k Shape: torch.Size([10, 256, 256])\n",
      "k Shape: torch.Size([10, 16, 256, 16])\n",
      "v Shape: torch.Size([10, 256, 256])\n",
      "v Shape: torch.Size([10, 16, 256, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1009 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "masked_fill only supports boolean masks, but got dtype Float",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombined_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m evaluate(model, test_dataloader)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\Training.py:89\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, test_dataloader, optimizer, num_epochs, device, loss_func, masks)\u001B[0m\n\u001B[0;32m     86\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m reconstructed \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Compute loss (only on masked positions)\u001B[39;00m\n\u001B[0;32m     92\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_func(reconstructed, batch)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\AudioTransformer.py:52\u001B[0m, in \u001B[0;36mAudioTransformer.forward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 52\u001B[0m     memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_latent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m     memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_latent(memory, mask)\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m memory\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\AudioTransformer.py:62\u001B[0m, in \u001B[0;36mAudioTransformer.to_latent\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     59\u001B[0m memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprojection_gelu(memory)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Encoder Block\u001B[39;00m\n\u001B[1;32m---> 62\u001B[0m memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Pass through Transformer encoder\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# Reshape for decoder input (Concatenation)\u001B[39;00m\n\u001B[0;32m     65\u001B[0m memory \u001B[38;5;241m=\u001B[39m memory\u001B[38;5;241m.\u001B[39mreshape(memory\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\ModelComponents.py:142\u001B[0m, in \u001B[0;36mRoPEALiBiTransformerEncoder.forward\u001B[1;34m(self, src, mask)\u001B[0m\n\u001B[0;32m    140\u001B[0m         src \u001B[38;5;241m=\u001B[39m checkpoint(layer, src, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malibi_bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_emb, mask)\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 142\u001B[0m         src \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(src)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\ModelComponents.py:97\u001B[0m, in \u001B[0;36mRoPEALiBiTransformerEncoderLayer.forward\u001B[1;34m(self, src, alibi_bias, pos_emb, mask)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, src, alibi_bias, pos_emb, mask):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;66;03m# src: [Batch, Time, Dim]\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \n\u001B[0;32m     96\u001B[0m     \u001B[38;5;66;03m# Self-attention Block\u001B[39;00m\n\u001B[1;32m---> 97\u001B[0m     src2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malibi_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     98\u001B[0m     src \u001B[38;5;241m=\u001B[39m src \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(src2)\n\u001B[0;32m     99\u001B[0m     src \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(src)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\ModelComponents.py:56\u001B[0m, in \u001B[0;36mRoPEALiBiMultiheadAttention.forward\u001B[1;34m(self, query, key, value, alibi_bias, pos_emb, mask)\u001B[0m\n\u001B[0;32m     54\u001B[0m     mask \u001B[38;5;241m=\u001B[39m mask\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# [batch_size, 1, 1, seq_len]\u001B[39;00m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;66;03m#mask = mask.expand(-1, attn_scores.size(1), attn_scores.size(2),-1)  # [batch_size, num_heads, seq_len, seq_len]\u001B[39;00m\n\u001B[1;32m---> 56\u001B[0m     \u001B[43mattn_scores\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmasked_fill_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m-inf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Add ALiBi bias\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m alibi_bias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: masked_fill only supports boolean masks, but got dtype Float"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T22:24:57.496690Z",
     "start_time": "2025-05-13T22:09:44.820347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, device=device, loss_func=combined_loss)\n",
    "evaluate(model, test_dataloader)"
   ],
   "id": "c484f8be6b9b10ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]E:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "100%|██████████| 40/40 [01:27<00:00,  2.20s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]E:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 1.7508 \t Validation Loss: 1.5583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:26<00:00,  2.17s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 1.5815 \t Validation Loss: 1.4251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:26<00:00,  2.17s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 1.4626 \t Validation Loss: 1.3847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:26<00:00,  2.16s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 1.3910 \t Validation Loss: 1.3523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:26<00:00,  2.17s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 1.3601 \t Validation Loss: 1.3346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 1.3409 \t Validation Loss: 1.3169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 1.3263 \t Validation Loss: 1.3089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 1.3171 \t Validation Loss: 1.3065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 1.3122 \t Validation Loss: 1.3039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 1.3108 \t Validation Loss: 1.3046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reconstructive Loss: 0.5965870023\n",
      "Avg Cross FFT Loss: 0.7100642323\n",
      "Avg Contrastive Loss: 1.3066511154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.6133, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T06:08:40.895318Z",
     "start_time": "2025-05-13T06:05:35.011125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, device=device, loss_func=combined_loss)\n",
    "evaluate(model, test_dataloader)"
   ],
   "id": "2344619c52ef2185",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]E:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "100%|██████████| 40/40 [01:22<00:00,  2.05s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]E:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 1.7499 \t Validation Loss: 1.5711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:18<00:00,  1.96s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 1.5855 \t Validation Loss: 1.4284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:16<01:16,  2.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombined_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m evaluate(model, test_dataloader)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\Training.py:96\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, test_dataloader, optimizer, num_epochs, device, loss_func)\u001B[0m\n\u001B[0;32m     93\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     94\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 96\u001B[0m     ls \u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m ls\n\u001B[0;32m     99\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Output Stuff",
   "id": "be18100accf986cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "reconstruction_examples = retrieve_data(\"E:\\SongsDataset\\\\\",  \"reconstruction_test_latents\\\\\", sample_length=256)"
   ],
   "id": "cf4253aa15817256",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "song_set, _ = random_split(reconstruction_examples, [len(reconstruction_examples), 0])\n",
    "song_dataset = AudioDataset(song_set)\n",
    "song_dataloader = DataLoader(song_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "fbc2621ca2ea37d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:50:20.154879Z",
     "start_time": "2025-05-22T19:50:18.735879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\n",
    "    f\"AudioTransformer-LatentSpace512-Heads16-TrasformerLayers16-DModel256-Dropout0.1-fft-cos-256\\-Epoch-10.pt\",\n",
    "        weights_only=False)\n",
    "evaluate(model, song_dataset)"
   ],
   "id": "7a8ae7c9d6b12217",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'song_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAudioTransformer-LatentSpace512-Heads16-TrasformerLayers16-DModel256-Dropout0.1-fft-cos-256\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m-Epoch-10.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      3\u001B[0m         weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 4\u001B[0m evaluate(model, \u001B[43msong_dataset\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'song_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "455f9eaa3c09be8b",
   "metadata": {},
   "source": [
    "from Loss import combined_loss\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "new_song = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(song_dataloader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed = model(batch)\n",
    "\n",
    "        new_song.extend(reconstructed.to(\"cpu\"))\n",
    "\n",
    "        loss = combined_loss(reconstructed, batch)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "l = np.array(np.stack(new_song)).reshape(64, -1)\n",
    "np.save(\"reconstructed_song-2D-256-Campfire.npy\", l)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "latent_space = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(song_dataloader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        latent = model.to_latent(batch)\n",
    "\n",
    "        latent_space.extend(latent.to(\"cpu\"))\n",
    "\n",
    "        num_batches += 1"
   ],
   "id": "500f58f1e962f7d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(new_song))\n",
    "print(len(new_song[0]))\n",
    "print(len(new_song[0][0]))"
   ],
   "id": "b136dab70b1d939c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99e97d55b0fe4e2b",
   "metadata": {},
   "source": [
    "import IPython\n",
    "from music2latent import EncoderDecoder\n",
    "import numpy as np\n",
    "\n",
    "encdec = EncoderDecoder()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "l = np.load(\"reconstructed_song-2D-256-Banana.npy\")",
   "id": "c551566086e9ad0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compressed_song = np.load(\"reconstructed_song-256-FFT.npy\")\n",
    "wv_rec = encdec.decode(l)"
   ],
   "id": "37adcbe9ec1ff8a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IPython.display.display(IPython.display.Audio(wv_rec, rate=44100))",
   "id": "3bf378f8f7bd1e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Output",
   "id": "fb377fc03ba185eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:51:34.122428Z",
     "start_time": "2025-05-22T19:51:31.536916Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from Data import AudioDataset, retrieve_data"
   ],
   "id": "ea5e81db5410c8c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:51:35.609938Z",
     "start_time": "2025-05-22T19:51:34.378428Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "347ec462d939c27a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioTransformer(\n",
       "  (projection): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (projection_gelu): GELU(approximate='none')\n",
       "  (encoder): RoPEALiBiTransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x RoPEALiBiTransformerEncoderLayer(\n",
       "        (self_attn): RoPEALiBiMultiheadAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (encode_to_latent): Linear(in_features=65536, out_features=512, bias=True)\n",
       "  (encode_to_latent_gelu): GELU(approximate='none')\n",
       "  (encode_from_latent): Linear(in_features=512, out_features=65536, bias=True)\n",
       "  (encode_from_latent_gelu): GELU(approximate='none')\n",
       "  (decoder): RoPEALiBiTransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x RoPEALiBiTransformerDecoderLayer(\n",
       "        (self_attn): RoPEALiBiMultiheadAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attn): RoPEALiBiMultiheadAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc_gelu): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = \"\"\n",
    "spotify_dataset = retrieve_data(\"E:\\\\SongsDataset\\\\latents\\\\\", directory, sample_length=256, keep_song_data_option=True)"
   ],
   "id": "d74b57bfb1d69555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T21:30:41.092129Z",
     "start_time": "2025-05-22T21:30:41.078133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compress_song_average(song, model, mask=None):\n",
    "    with torch.no_grad():\n",
    "        latent_space = model.to_latent(song, mask)\n",
    "        return torch.sum(latent_space.to('cpu'), dim=0) / len(song)"
   ],
   "id": "7b90b9c27234c4e8",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T21:30:41.265129Z",
     "start_time": "2025-05-22T21:30:41.251128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Data import chunk_song\n",
    "import os\n",
    "\n",
    "def compute(model, name, length=256):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    path = \"E:\\\\SongsDataset\\\\latents\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    file = open(f\"output_analysis\\\\output-{name}.csv\", \"w\", encoding='utf-8')\n",
    "\n",
    "    for each_song in tqdm(all_folders):\n",
    "        song_path = os.path.join(path, each_song)\n",
    "\n",
    "        padded_data, zeros  = chunk_song(song_path, length)\n",
    "        input_tensor = torch.Tensor(padded_data).reshape(-1, length, 64).to(\"cuda\")\n",
    "\n",
    "        mask = [0 for _ in range(padded_data.shape[0] // length - 1)]\n",
    "        mask.append(zeros)\n",
    "        bool_masks = [[0 if (length - i) > x else 1 for i in range(length)] for x in mask]\n",
    "        bool_masks = torch.tensor(bool_masks).bool()\n",
    "        bool_masks = bool_masks.to(\"cuda\")\n",
    "\n",
    "        latent = compress_song_average(input_tensor, model, mask=bool_masks)\n",
    "\n",
    "        output = \"\"\n",
    "        for value in latent:\n",
    "            output += f\"{str(value.item())} \"\n",
    "\n",
    "        file.write(output + f\"\\\"{each_song}\\\"\\n\")"
   ],
   "id": "11349c5b539020d6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = torch.load(\"final-models\\\\AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft\\\\-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"CNN-FINAL\")"
   ],
   "id": "730561865b7288fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models\\\\AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft\\\\-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"CNN-FINAL\")"
   ],
   "id": "ebf06fb8b58faa6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models\\\\AudioTransformerSingleLinearReconstruction-LatentSpace512-Heads16-TrasformerLayers16-DModel256-Dropout0.1-fft\\\\-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"Linear-FINAL\")"
   ],
   "id": "6acce80a99de4c67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T16:47:41.929511Z",
     "start_time": "2025-05-12T16:45:15.418534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\Coding\\SongAnalyzer\\Analyzer\\Webscraper\\AudioTransformer-LatentSpace512-Heads8-TrasformerLayers8-DModel1024-Dropout0.1-fft-cos-\\-Epoch-9.pt\", weights_only=False)\n",
    "model.set_checkpointing(False)\n",
    "compute(model, \"Linear-1024-TEST\")"
   ],
   "id": "709dec0c65f8c805",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [02:25<00:00, 27.29it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T21:33:44.405637Z",
     "start_time": "2025-05-22T21:30:43.134128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"AudioTransformer-LatentSpace512-Heads16-TrasformerLayers16-DModel256-Dropout0.1-fft-cos-256\\-Epoch-10.pt\", weights_only=False) # Set model to evaluation mode\n",
    "compute(model, \"Linear-256-Mask\", length=256)"
   ],
   "id": "f934635a2a28fa3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [03:00<00:00, 22.02it/s]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Analysis",
   "id": "b415f5779e1b4264"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
