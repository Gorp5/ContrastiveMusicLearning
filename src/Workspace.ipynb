{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data & Training",
   "id": "ebf89474ab777d38"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_dataset(length=256):\n",
    "    directory = \"\"  #\"latents\\\\\" #\"reconstruction_test_latents\\\\\" #mtg-jamendo\n",
    "    num_per = 1000\n",
    "    count = 1\n",
    "\n",
    "    for start in tqdm(range(0, 16150, num_per)):\n",
    "        mtg_dataset = retrieve_data(\"E:\\SongsDataset\\\\mtg-jamendo\\\\\", directory, start=start, count=num_per, sample_length=length)\n",
    "        torch.save(mtg_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}.pt\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    for start in tqdm(range(0, 3975, num_per)):\n",
    "        spotify_dataset = retrieve_data(\"E:\\\\SongsDataset\\\\latents\\\\\", directory, start=start, count=num_per, sample_length=length)\n",
    "        torch.save(spotify_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}.pt\")\n",
    "        count += 1\n",
    "\n",
    "    full_dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset1.pt\")\n",
    "    for start in tqdm(range(2, 21)):\n",
    "        new_data = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{start}.pt\")\n",
    "        full_dataset = torch.cat((full_dataset, new_data))\n",
    "\n",
    "    torch.save(full_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\full_dataset.pt\")"
   ],
   "id": "b8e1130f3fe4d3cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parse_dataset(256)",
   "id": "5c59e7733cec7e6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from Training import train, evaluate\n",
    "from Data import AudioDataset, retrieve_data"
   ],
   "id": "6afff8a0a232dd33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==== Model & Optimizer ====\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "d_model = 256\n",
    "latent_space = 512\n",
    "dim_feedforward = 1024\n",
    "sample_length = 1024\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "device = \"cuda\""
   ],
   "id": "1112734e300ad7bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#full_dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\full_dataset.pt\")\n",
    "full_dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\dataset1.pt\")"
   ],
   "id": "932e5667035a22a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "num_samples, seq_length, embed_dim = full_dataset.shape\n",
    "\n",
    "train_len = int(len(full_dataset) * 0.9)\n",
    "train_set, test_set = random_split(full_dataset, [train_len, len(full_dataset) - train_len])\n",
    "\n",
    "train_dataset = AudioDataset(train_set)\n",
    "test_dataset = AudioDataset(test_set)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "d91d65a8dceff284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from Loss import combined_loss\n",
    "from AudioTransformer import AudioTransformer\n",
    "\n",
    "model = AudioTransformer(d_model=d_model, num_heads=num_heads, transformer_layers=num_layers, dim_feedforward=dim_feedforward, latent_space=latent_space, length=sample_length, dropout=0.1, name_extension=\"-fft-cos-only_RoPE\", use_rope=True, use_alibi=False)"
   ],
   "id": "c2dbff9d4271f59c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ],
   "id": "b838be6a44280c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0)"
   ],
   "id": "fb94e024564875f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, device=device, loss_func=combined_loss)\n",
    "evaluate(model, test_dataloader)"
   ],
   "id": "2344619c52ef2185",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Output Stuff",
   "id": "be18100accf986cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "reconstruction_examples = retrieve_data(\"E:\\SongsDataset\\\\\",  \"reconstruction_test_latents\\\\\", sample_length=256)"
   ],
   "id": "cf4253aa15817256",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "song_set, _ = random_split(reconstruction_examples, [len(reconstruction_examples), 0])\n",
    "song_dataset = AudioDataset(song_set)\n",
    "song_dataloader = DataLoader(song_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "fbc2621ca2ea37d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\n",
    "    f\"AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft-cos\\\\-Epoch-14.pt\",\n",
    "        weights_only=False)\n",
    "evaluate(model, song_dataset)"
   ],
   "id": "7a8ae7c9d6b12217",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "455f9eaa3c09be8b",
   "metadata": {},
   "source": [
    "from Loss import combined_loss\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "new_song = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(song_dataloader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed = model(batch)\n",
    "\n",
    "        new_song.extend(reconstructed.to(\"cpu\"))\n",
    "\n",
    "        loss = combined_loss(reconstructed, batch)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "l = np.array(np.stack(new_song)).reshape(64, -1)\n",
    "np.save(\"reconstructed_song-2D-256-Campfire.npy\", l)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "latent_space = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(song_dataloader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        latent = model.to_latent(batch)\n",
    "\n",
    "        latent_space.extend(latent.to(\"cpu\"))\n",
    "\n",
    "        num_batches += 1"
   ],
   "id": "500f58f1e962f7d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(new_song))\n",
    "print(len(new_song[0]))\n",
    "print(len(new_song[0][0]))"
   ],
   "id": "b136dab70b1d939c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99e97d55b0fe4e2b",
   "metadata": {},
   "source": [
    "import IPython\n",
    "from music2latent import EncoderDecoder\n",
    "import numpy as np\n",
    "\n",
    "encdec = EncoderDecoder()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "l = np.load(\"reconstructed_song-2D-256-Banana.npy\")",
   "id": "c551566086e9ad0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compressed_song = np.load(\"reconstructed_song-256-FFT.npy\")\n",
    "wv_rec = encdec.decode(l)"
   ],
   "id": "37adcbe9ec1ff8a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IPython.display.display(IPython.display.Audio(wv_rec, rate=44100))",
   "id": "3bf378f8f7bd1e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IPython.display.display(IPython.display.Audio(wv_rec, rate=44100))",
   "id": "829e14203c6c68c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from Data import AudioDataset, retrieve_data"
   ],
   "id": "ea5e81db5410c8c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Output",
   "id": "fb377fc03ba185eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models\\\\AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft\\\\-Epoch-10.pt\", weights_only=False)\n",
    "model.to(\"cuda\")\n",
    "model.eval()  # Set model to evaluation mode"
   ],
   "id": "347ec462d939c27a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = \"\"\n",
    "spotify_dataset = retrieve_data(\"E:\\\\SongsDataset\\\\latents\\\\\", directory, sample_length=256, keep_song_data_option=True)"
   ],
   "id": "d74b57bfb1d69555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compress_song_average(song, model):\n",
    "    with torch.no_grad():\n",
    "        latent_space = model.to_latent(song)\n",
    "        return torch.sum(latent_space.to('cpu'), dim=0) / len(song)"
   ],
   "id": "7b90b9c27234c4e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from Analyzer.Webscraper.Data import chunk_song\n",
    "import os\n",
    "\n",
    "def compute(model, name):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    path = \"E:\\\\SongsDataset\\\\latents\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    file = open(f\"output_analysis\\\\output-{name}.csv\", \"w\", encoding='utf-8')\n",
    "\n",
    "    for each_song in tqdm(all_folders):\n",
    "        song_path = os.path.join(path, each_song)\n",
    "\n",
    "        padded_data = chunk_song(song_path, 256)\n",
    "        input_tensor = torch.Tensor(padded_data).reshape(-1, 256, 64).to(\"cuda\")\n",
    "\n",
    "        latent = compress_song_average(input_tensor, model)\n",
    "\n",
    "        output = \"\"\n",
    "        for value in latent:\n",
    "            output += f\"{str(value.item())} \"\n",
    "\n",
    "        file.write(output + f\"\\\"{each_song}\\\"\\n\")"
   ],
   "id": "11349c5b539020d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models\\\\AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft\\\\-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"CNN-FINAL\")"
   ],
   "id": "ebf06fb8b58faa6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models\\\\AudioTransformerSingleLinearReconstruction-LatentSpace512-Heads16-TrasformerLayers16-DModel256-Dropout0.1-fft\\\\-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"Linear-FINAL\")"
   ],
   "id": "6acce80a99de4c67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"AudioTransformerCNNReconstruction-LatentSpace64-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft-cos\\\\-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"CNN-FFT-COS\")"
   ],
   "id": "709dec0c65f8c805",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Analysis",
   "id": "b415f5779e1b4264"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
