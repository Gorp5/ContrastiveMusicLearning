{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize Model",
   "id": "614913695eca1a35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:39:39.780727Z",
     "start_time": "2025-08-11T19:39:37.669496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from loss.loss_utils import combined_loss\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from data.data import *\n",
    "from models.AudioResnet import AudioResnet\n",
    "from loss.FocalLoss import FocalLoss\n",
    "from models.AudioTransformer import AudioTransformer\n",
    "\n",
    "augmentations = Compose([\n",
    "    AddGaussianNoise(std=0.5),\n",
    "    TimeMasking(max_mask_pct=0.05),\n",
    "    FrequencyMasking(max_mask_pct=0.05),\n",
    "])\n",
    "\n",
    "\n",
    "class Config:\n",
    "    # === General ===\n",
    "\n",
    "    model_name = \"Audio-Transformer-\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    save_path = f\"trained_models\\\\{model_name}\\\\\"\n",
    "    seed = 42\n",
    "\n",
    "    # === Training ===\n",
    "    num_classes = 50\n",
    "    num_epochs = 100\n",
    "    batch_size = 1\n",
    "    max_batch_size = 64\n",
    "    learning_rate = 5e-5\n",
    "    min_learning_rate = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    warmup_threshold = 1.0 / 100.0\n",
    "    step_coefficient = 25.0 / 100.0\n",
    "\n",
    "    gamma = 2.0\n",
    "    save_checkpoints = True\n",
    "\n",
    "    # === Dataset ===\n",
    "    transforms = None\n",
    "    use_masks = True\n",
    "    num_workers = 1\n",
    "    prefetch_factor = 3\n",
    "    val_split = 0.1\n",
    "    shuffle = True\n",
    "    #pos_weight = (torch.ones(num_classes) * 10).to(\"cuda\")\n",
    "    criterion = combined_loss"
   ],
   "id": "9912cf8d6cf75a20",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mAudioResnet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AudioResnet\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mloss\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mFocalLoss\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FocalLoss\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mAudioTransformer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AudioTransformer\n\u001B[0;32m     11\u001B[0m augmentations \u001B[38;5;241m=\u001B[39m Compose([\n\u001B[0;32m     12\u001B[0m     AddGaussianNoise(std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m),\n\u001B[0;32m     13\u001B[0m     TimeMasking(max_mask_pct\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.15\u001B[39m),\n\u001B[0;32m     14\u001B[0m     FrequencyMasking(max_mask_pct\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.15\u001B[39m),\n\u001B[0;32m     15\u001B[0m ])\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mConfig\u001B[39;00m:\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m# === General ===\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\AudioTransformer.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RopeALiBiModelComponents\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mAudioTransformer\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, num_heads\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, encoder_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, decoder_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, d_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, dim_feedforward\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, checkpointing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[0;32m      8\u001B[0m                  latent_space\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, name_extension\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, use_alibi\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, use_rope\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, autoregressive\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, genre_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, mood_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\RopeALiBiModelComponents.py:7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#from torch.utils.checkpoint import checkpoint\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mflash_attn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m flash_attn_func\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mRoPEALiBiMultiheadAttention\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, d_model, num_heads, alibi_slopes, dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'flash_attn'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:38:17.585908Z",
     "start_time": "2025-08-11T19:38:17.557774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "large_directory = \"large-melspec-dataset-top-50-LIBROSA\"\n",
    "\n",
    "train_dataset = StreamingSongDataset(f\"E:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\data\",\n",
    "                                     f\"E:\\\\SongsDataset\\\\{large_directory}\\\\train_set\\\\genre_labels\",\n",
    "                                     transform=augmentations)\n",
    "test_dataset = StreamingSongDataset(f\"E:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\data\",\n",
    "                                    f\"E:\\\\SongsDataset\\\\{large_directory}\\\\test_set\\\\genre_labels\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    prefetch_factor=Config.prefetch_factor,\n",
    ")"
   ],
   "id": "e59a10abb7e49172",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StreamingSongDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m      3\u001B[0m large_directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlarge-melspec-dataset-top-50-LIBROSA\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 5\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mStreamingSongDataset\u001B[49m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongsDataset\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mlarge_directory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtrain_set\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      6\u001B[0m                                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongsDataset\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mlarge_directory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtrain_set\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mgenre_labels\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      7\u001B[0m                                      transform\u001B[38;5;241m=\u001B[39maugmentations)\n\u001B[0;32m      8\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m StreamingSongDataset(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongsDataset\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mlarge_directory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtest_set\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m                                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongsDataset\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mlarge_directory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtest_set\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mgenre_labels\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[0;32m     12\u001B[0m     train_dataset,\n\u001B[0;32m     13\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m     prefetch_factor\u001B[38;5;241m=\u001B[39mConfig\u001B[38;5;241m.\u001B[39mprefetch_factor,\n\u001B[0;32m     17\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'StreamingSongDataset' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from data.processing import ParseBalanced\n",
    "\n",
    "directory = \"large-melspec-dataset-top-50-LIBROSA\"\n",
    "data_directory = \"E:/mtg-jamendo/\"\n",
    "subset_file_name = \"autotagging_top50tags\"\n",
    "ParseBalanced(subset_file_name, f\"{data_directory}\", f\"E:/SongsDataset/{directory}\", convert=True,\n",
    "              target_per_genre=1300)"
   ],
   "id": "bf12fdb87ed9bf24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:14:42.225606Z",
     "start_time": "2025-10-17T13:14:42.219605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "def cluster_elki(name, num_clusters):\n",
    "    # Define parameters\n",
    "    elki_jar = \"elki-bundle-0.8.0.jar\"\n",
    "    data_file = f\"output_analysis/output-{name}.csv\"\n",
    "\n",
    "    # Construct the ELKI command\n",
    "    cmd = [\n",
    "        \"java\", \"-jar\", elki_jar,\n",
    "        \"KDDCLIApplication\",\n",
    "        \"-dbc.in\", data_file,\n",
    "        \"-algorithm\", \"clustering.hierarchical.extraction.CutDendrogramByNumberOfClusters\",\n",
    "        \"-algorithm\", \"Anderberg\",\n",
    "        \"-algorithm.distancefunction\", \"CosineDistance\",\n",
    "        \"-hierarchical.minclusters\", str(num_clusters),\n",
    "        \"-resulthandler\", \"ResultWriter\",\n",
    "        \"-out.gzip\", \"false\",\n",
    "        \"-out\", f\"output_analysis/elki-TEST-{name}-{num_clusters}\",\n",
    "    ]\n",
    "\n",
    "    # Execute the command\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred:\\n\", e.stderr)"
   ],
   "id": "6ab21de71dbc50c2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:20:30.300242Z",
     "start_time": "2025-09-30T21:20:22.394249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import tqdm\n",
    "from training.inference import load_and_parse_audio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def compute(model, name):\n",
    "    path = \"E:\\\\SongsDataset\\\\songs\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "    length = 256\n",
    "    with open(name, 'w', encoding=\"utf-8\") as f:\n",
    "        with torch.no_grad():\n",
    "            for each_song in tqdm(all_folders):\n",
    "                song_path = os.path.join(path, each_song)\n",
    "\n",
    "                # audio, sr = librosa.load(song_path, sr=44100, mono=True)\n",
    "                # data = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "                # data = librosa.amplitude_to_db(data, ref=np.max)\n",
    "\n",
    "                chunks = load_and_parse_audio(song_path, convert=True, chunk_size=length).to(\"cuda\")\n",
    "                permuted_chunks = torch.stack([c for c in chunks])\n",
    "\n",
    "                # mean = permuted_chunks.mean(dim=[1, 2], keepdim=True)\n",
    "                # std = permuted_chunks.std(dim=[1, 2], keepdim=True)\n",
    "                # permuted_chunks = (permuted_chunks - mean) / (std + 1e-6)\n",
    "\n",
    "                # num_chunks = int(permuted_chunks.shape[0] / 64) + 1\n",
    "                # data_minibatches = torch.chunk(permuted_chunks, num_chunks, dim=0)\n",
    "                #\n",
    "                # latents = []\n",
    "                # for i, data_minibatch in enumerate(data_minibatches):\n",
    "                #     latent = model(data_minibatch)\n",
    "                #     latents.append(latent)\n",
    "\n",
    "                B, T, F = permuted_chunks.shape\n",
    "\n",
    "                if T > length:\n",
    "                    continue\n",
    "\n",
    "                _, latents, _ = model(permuted_chunks, masked=False)\n",
    "\n",
    "                #averages = torch.cat(latents, dim=0).mean(dim=0).cpu().detach().numpy()\n",
    "                averages = latents.mean(dim=0).cpu().detach().numpy()\n",
    "\n",
    "                line = \" \".join([str(x) for x in averages]) + f\" {each_song}\\n\"\n",
    "                f.write(line)"
   ],
   "id": "10018c681e04bbc2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:14:48.343130Z",
     "start_time": "2025-10-17T13:14:43.260377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from training.inference import load_and_parse_audio\n",
    "\n",
    "def process_song(song_path, length=256):\n",
    "    \"\"\"Preprocess one song (CPU only).\"\"\"\n",
    "    chunks = load_and_parse_audio(song_path, convert=True, chunk_size=length)\n",
    "    return song_path, chunks\n",
    "\n",
    "def compute(model, name, batch_size=16, num_workers=4, length=256, variational=False):\n",
    "    path = \"E:\\\\SongsDataset\\\\songs\\\\\"\n",
    "    all_songs = os.listdir(path)\n",
    "\n",
    "    with open(name, 'w', encoding=\"utf-8\") as f, torch.no_grad(), ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # submit all preprocessing jobs\n",
    "        partitions = 8\n",
    "        num_songs = len(all_songs) // partitions\n",
    "        for index in range(1, partitions):\n",
    "            futures = [executor.submit(process_song, os.path.join(path, song), length) for song in all_songs[(index - 1) * num_songs:index * num_songs]]\n",
    "\n",
    "            # process results in batches of X\n",
    "            batch = []\n",
    "            for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "                song_path, chunks = future.result()\n",
    "                batch.append((song_path, chunks))\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    run_batch(model, batch, f, length, variational)\n",
    "                    batch = []\n",
    "\n",
    "            # handle last small batch\n",
    "            if batch:\n",
    "                run_batch(model, batch, f, length)\n",
    "\n",
    "def run_batch(model, batch, file_handle, length=256, variational=False):\n",
    "    \"\"\"Run model on a batch of preprocessed songs and write results.\"\"\"\n",
    "    for song_path, chunks in batch:\n",
    "        chunks = chunks.to(\"cuda\")\n",
    "        B, T, F = chunks.shape\n",
    "\n",
    "        if B > length or T > length or F > length:\n",
    "            continue\n",
    "\n",
    "        chunks = chunks.unsqueeze(1)\n",
    "        latents = model(chunks)\n",
    "\n",
    "        averages = latents.mean(dim=0).cpu().numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "        line = \" \".join([str(x) for x in averages]) + f\" \\\"{os.path.basename(song_path)}\\\"\\n\"\n",
    "        file_handle.write(line)"
   ],
   "id": "5171d48bce862e2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:37:24.784275Z",
     "start_time": "2025-10-17T13:14:48.349138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:/Coding/SongAnalyzer/Analyzer/src/trained_models/Myna-CLS-Album/Epoch-131.pt\", weights_only=False)\n",
    "\n",
    "model.mask_ratio= 0.0\n",
    "\n",
    "compute(model, \"E:/Coding/SongAnalyzer/Analyzer/src/output_analysis/output-Myna-CLS-Album.csv\", variational=False)"
   ],
   "id": "1396a8efcf2e13d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [03:20<00:00,  2.48it/s]\n",
      "100%|██████████| 496/496 [02:51<00:00,  2.88it/s]\n",
      "100%|██████████| 496/496 [03:02<00:00,  2.71it/s]\n",
      "100%|██████████| 496/496 [04:10<00:00,  1.98it/s]\n",
      "100%|██████████| 496/496 [03:12<00:00,  2.57it/s]\n",
      "100%|██████████| 496/496 [03:08<00:00,  2.64it/s]\n",
      "100%|██████████| 496/496 [02:45<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:38:28.218568Z",
     "start_time": "2025-10-17T13:38:21.973209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cluster_elki(\"Myna-CLS-Album\", 64)\n",
    "cluster_elki(\"Myna-CLS-Album\", 256)"
   ],
   "id": "7a9bdf5a660cdd19",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:11:31.499800Z",
     "start_time": "2025-10-04T23:49:25.653982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:/Coding/SongAnalyzer/Analyzer/src/trained_models/ViT-Contrastive-Embeddings-Masking-0.9-Variational/Classifier-Epoch-36.pt\", weights_only=False)\n",
    "compute(model, \"E:/Coding/SongAnalyzer/Analyzer/src/output_analysis/output-contrastive-masking-09-Variational.csv\", variational=True)"
   ],
   "id": "2a8e5e071e56351a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [03:57<00:00,  2.09it/s]\n",
      "100%|██████████| 496/496 [02:39<00:00,  3.12it/s]\n",
      "100%|██████████| 496/496 [02:31<00:00,  3.26it/s]\n",
      "100%|██████████| 496/496 [02:37<00:00,  3.16it/s]\n",
      "100%|██████████| 496/496 [02:55<00:00,  2.82it/s]\n",
      "100%|██████████| 496/496 [02:41<00:00,  3.07it/s]\n",
      "100%|██████████| 496/496 [02:30<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:31:57.706023Z",
     "start_time": "2025-10-05T00:11:31.622939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:/Coding/SongAnalyzer/Analyzer/src/trained_models/ViT-Contrastive-Embeddings-Masking-0.9-ALIBI/Classifier-Epoch-15.pt\", weights_only=False)\n",
    "compute(model, \"E:/Coding/SongAnalyzer/Analyzer/src/output_analysis/output-contrastive-masking-09-alibi.csv\", variational=False)"
   ],
   "id": "f17da8d524db5412",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [02:34<00:00,  3.22it/s]\n",
      "100%|██████████| 496/496 [02:36<00:00,  3.18it/s]\n",
      "100%|██████████| 496/496 [02:28<00:00,  3.34it/s]\n",
      "100%|██████████| 496/496 [02:35<00:00,  3.18it/s]\n",
      "100%|██████████| 496/496 [02:40<00:00,  3.08it/s]\n",
      "100%|██████████| 496/496 [02:43<00:00,  3.03it/s]\n",
      "100%|██████████| 496/496 [02:29<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T00:33:25.301385Z",
     "start_time": "2025-10-05T00:33:01.144925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cluster_elki(\"contrastive-masking-09\", 64)\n",
    "cluster_elki(\"contrastive-masking-09\", 256)\n",
    "cluster_elki(\"contrastive-masking-09-variational\", 64)\n",
    "cluster_elki(\"contrastive-masking-09-variational\", 256)\n",
    "cluster_elki(\"contrastive-masking-09-alibi\", 64)\n",
    "cluster_elki(\"contrastive-masking-09-alibi\", 256)"
   ],
   "id": "c762fc5ad385e2c7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:15:37.475194Z",
     "start_time": "2025-07-09T17:15:27.049487Z"
    }
   },
   "cell_type": "code",
   "source": "cluster_elki(\"Classification-With-Reconstruction\", 100)",
   "id": "df2cdc9b6640bcb3",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
