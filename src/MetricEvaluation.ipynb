{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T00:55:34.460821Z",
     "start_time": "2025-10-21T00:55:27.743365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from training.inference import get_latents\n",
    "from datasets import tqdm\n",
    "\n",
    "def generate_evaluation_dataset(model, dataloader, name, chunking=True, chunk_size=256):\n",
    "    model.mask_ratio = 0.0\n",
    "    model.to(\"cuda\")\n",
    "    latents, labels = get_latents(dataloader, model, chunking=chunking, chunk_size=chunk_size)\n",
    "    print(\"Saving...\")\n",
    "    for index, latent, label in tqdm(enumerate(zip(latents, labels))):\n",
    "        directory = f\"D:\\\\SongsDataset\\\\GTZAN\\\\latent_datasets\\\\{name}\\\\\"\n",
    "        label_name = directory + f\"{index}_label.pt\"\n",
    "        latent_name = directory + f\"{index}_data.pt\"\n",
    "        torch.save(latents, latent_name)\n",
    "        torch.save(label, label_name)"
   ],
   "id": "26862cea14b0f113",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T00:55:58.419369Z",
     "start_time": "2025-10-21T00:55:34.671388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.data_utils import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\GTZAN\\\\genres_original\\\\\"\n",
    "GTZAN_dataset = GTZAN(directory)\n",
    "GTZAN_dataloader = DataLoader(\n",
    "    GTZAN_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")"
   ],
   "id": "5fe6cc6865b7454c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d28e115ebde457ba71244fb4065ba5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m      4\u001B[0m directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongsDataset\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mGTZAN\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mgenres_original\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 5\u001B[0m GTZAN_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mGTZAN\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m GTZAN_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[0;32m      7\u001B[0m     GTZAN_dataset,\n\u001B[0;32m      8\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m     prefetch_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     12\u001B[0m )\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\data\\data_utils.py:73\u001B[0m, in \u001B[0;36mGTZAN.__init__\u001B[1;34m(self, data_directory)\u001B[0m\n\u001B[0;32m     70\u001B[0m songs\u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(os\u001B[38;5;241m.\u001B[39mlistdir(folder_path))\n\u001B[0;32m     71\u001B[0m song_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(folder_path, songs[index_in_genre])\n\u001B[1;32m---> 73\u001B[0m mel_spec \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[43mget_melspec_from_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43msong_path\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspectrograms\u001B[38;5;241m.\u001B[39mappend(mel_spec)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\data\\data_utils.py:22\u001B[0m, in \u001B[0;36mget_melspec_from_file\u001B[1;34m(full_path)\u001B[0m\n\u001B[0;32m     19\u001B[0m hop_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mround\u001B[39m(\u001B[38;5;241m0.010\u001B[39m \u001B[38;5;241m*\u001B[39m sr))  \u001B[38;5;66;03m# 441 samples\u001B[39;00m\n\u001B[0;32m     20\u001B[0m n_fft \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2048\u001B[39m\n\u001B[1;32m---> 22\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mlibrosa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmelspectrogram\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_fft\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_fft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwin_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwin_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_mels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfmax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpower\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2.0\u001B[39;49m\n\u001B[0;32m     25\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m librosa\u001B[38;5;241m.\u001B[39mamplitude_to_db(data, ref\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mmax)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\librosa\\feature\\spectral.py:2150\u001B[0m, in \u001B[0;36mmelspectrogram\u001B[1;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001B[0m\n\u001B[0;32m   2147\u001B[0m \u001B[38;5;66;03m# Build a Mel filter\u001B[39;00m\n\u001B[0;32m   2148\u001B[0m mel_basis \u001B[38;5;241m=\u001B[39m filters\u001B[38;5;241m.\u001B[39mmel(sr\u001B[38;5;241m=\u001B[39msr, n_fft\u001B[38;5;241m=\u001B[39mn_fft, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 2150\u001B[0m melspec: np\u001B[38;5;241m.\u001B[39mndarray \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m...ft,mf->...mt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmel_basis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2151\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m melspec\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\numpy\\_core\\einsumfunc.py:1471\u001B[0m, in \u001B[0;36meinsum\u001B[1;34m(out, optimize, *operands, **kwargs)\u001B[0m\n\u001B[0;32m   1468\u001B[0m     right_pos\u001B[38;5;241m.\u001B[39mappend(input_right\u001B[38;5;241m.\u001B[39mfind(s))\n\u001B[0;32m   1470\u001B[0m \u001B[38;5;66;03m# Contract!\u001B[39;00m\n\u001B[1;32m-> 1471\u001B[0m new_view \u001B[38;5;241m=\u001B[39m \u001B[43mtensordot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1472\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtmp_operands\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mleft_pos\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mright_pos\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1473\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1475\u001B[0m \u001B[38;5;66;03m# Build a new view if needed\u001B[39;00m\n\u001B[0;32m   1476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (tensor_result \u001B[38;5;241m!=\u001B[39m results_index) \u001B[38;5;129;01mor\u001B[39;00m handle_out:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\numpy\\_core\\numeric.py:1177\u001B[0m, in \u001B[0;36mtensordot\u001B[1;34m(a, b, axes)\u001B[0m\n\u001B[0;32m   1175\u001B[0m at \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mtranspose(newaxes_a)\u001B[38;5;241m.\u001B[39mreshape(newshape_a)\n\u001B[0;32m   1176\u001B[0m bt \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mtranspose(newaxes_b)\u001B[38;5;241m.\u001B[39mreshape(newshape_b)\n\u001B[1;32m-> 1177\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\u001B[38;5;241m.\u001B[39mreshape(olda \u001B[38;5;241m+\u001B[39m oldb)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T00:55:58.421367500Z",
     "start_time": "2025-10-21T00:52:26.876022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-Sinusoidal\\\\Epoch-79.pt\", weights_only=False)\n",
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"Sinusoidal-Chunking-256\", chunking=True, chunk_size=256)"
   ],
   "id": "78096b22d19bab28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [128, 3001] at entry 0 and [128, 3002] at entry 2\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mCoding\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongAnalyzer\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mAnalyzer\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtrained_models\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mMyna-CLS-Sinusoidal\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mEpoch-79.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m, weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mgenerate_evaluation_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGTZAN_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSinusoidal-Chunking-256\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 9\u001B[0m, in \u001B[0;36mgenerate_evaluation_dataset\u001B[1;34m(model, dataloader, name, chunking, chunk_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mmask_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m latents, labels \u001B[38;5;241m=\u001B[39m \u001B[43mget_latents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, latent, label \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(latents, labels))):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:25\u001B[0m, in \u001B[0;36mget_latents\u001B[1;34m(dataloader, model, chunking, chunk_size)\u001B[0m\n\u001B[0;32m     22\u001B[0m model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 25\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchunked_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_chunks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mchunk_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   1182\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[0;32m   1183\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[0;32m   1184\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1463\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1464\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1489\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1490\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1491\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1492\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\_utils.py:715\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    711\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    712\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    713\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    714\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 715\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [128, 3001] at entry 0 and [128, 3002] at entry 2\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-ALIBI\\\\Epoch-103.pt\", weights_only=False)\n",
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-256\", chunking=True, chunk_size=256)"
   ],
   "id": "b03abf156baf1432"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-512\", chunking=True, chunk_size=512)",
   "id": "2d7c45006464f818"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-1024\", chunking=True, chunk_size=1024)",
   "id": "a532740122b261b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-2048\", chunking=True, chunk_size=2048)\n",
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-No-Chunking\", chunking=False, chunk_size=131072)"
   ],
   "id": "f74f278dbbc18557"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "def linear_probing(dataloader):\n",
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
