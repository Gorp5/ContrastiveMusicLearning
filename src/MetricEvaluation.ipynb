{
 "cells": [
  {
   "cell_type": "code",
   "id": "26862cea14b0f113",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:04:45.063147Z",
     "start_time": "2025-11-19T18:04:20.696351Z"
    }
   },
   "source": [
    "import torch.nn\n",
    "\n",
    "def generate_evaluation_dataset(model, dataset_name, dataloader, name, chunking=True, averaging=False, chunk_size=256):\n",
    "    model.mask_ratio = 0.0\n",
    "    get_and_save_latents(dataloader, model, dataset_name, name, chunking=chunking, averaging=averaging, chunk_size=chunk_size)\n",
    "    print(\"Saving...\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "42142ca3815ad3d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-11-19T18:05:11.080031Z",
     "start_time": "2025-11-19T18:04:46.030457Z"
    }
   },
   "source": [
    "from utils.visualization import visualize_ROC_PR_AUC\n",
    "import torch.nn\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input=128, output=10, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512, output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input=128, output=10):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(input, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# ----- Training Loop -----\n",
    "def train_model(model, train_dataloader, test_dataloader, config, device=\"cuda\", use_tqdm=False, criterion=nn.CrossEntropyLoss(), early_stopping=True):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    patience = 0\n",
    "\n",
    "    itr = range(config.num_epochs)\n",
    "    if use_tqdm:\n",
    "        itr = tqdm(itr)\n",
    "\n",
    "    for epoch in itr:\n",
    "        total_loss = 0\n",
    "        for labels, data in train_dataloader:\n",
    "            #print(labels.max().item())\n",
    "\n",
    "            # if dataset returns one-hot, convert back to integer for CrossEntropy\n",
    "            if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "                labels = torch.stack(labels).permute(1, 0, 2).squeeze(2).float()\n",
    "            elif labels.ndim > 1:\n",
    "                labels = labels.squeeze(1)\n",
    "\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            scores, accuracy, _ = evaluate_model(model, test_dataloader, criterion=criterion, use_tqdm=use_tqdm)\n",
    "            model.train()\n",
    "\n",
    "        if early_stopping:\n",
    "            if accuracy < best_accuracy:\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "                best_accuracy = accuracy\n",
    "\n",
    "            if patience >= 16:\n",
    "                return model\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, dataloader, device=\"cuda\", use_tqdm=False, criterion=nn.CrossEntropyLoss(), roc_pr_auc=False):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        itr = dataloader\n",
    "        if use_tqdm:\n",
    "            itr = tqdm(dataloader)\n",
    "\n",
    "        for labels, data in dataloader:\n",
    "            # if dataset returns one-hot, convert back to integer for CrossEntropy\n",
    "            if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "                labels = torch.stack(labels).permute(1, 0, 2).squeeze(2).float()\n",
    "            elif labels.ndim > 1:\n",
    "                labels = labels.squeeze(1)\n",
    "\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                predicted = outputs.argmax(dim=1).long()\n",
    "            elif isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "                probability = torch.sigmoid(outputs)\n",
    "                predicted = probability > 0.5\n",
    "            else:\n",
    "                predicted = outputs\n",
    "\n",
    "            B = labels.shape[0]\n",
    "            if B == 1:\n",
    "                all_predictions.append(predicted.clone().cpu())\n",
    "                all_labels.append(labels.clone().cpu())\n",
    "            else:\n",
    "                all_predictions.extend(predicted.clone().cpu())\n",
    "                all_labels.extend(labels.clone().cpu())\n",
    "\n",
    "    if isinstance(criterion, nn.MSELoss):\n",
    "        predictions = torch.stack(all_predictions)\n",
    "        labels = torch.stack(all_labels)\n",
    "\n",
    "        if labels.shape[1] == 1:\n",
    "            labels = labels.squeeze(1)\n",
    "\n",
    "        first = r2_score([y[0] for y in labels], [y[0] for y in predictions])\n",
    "        second = r2_score([y[1] for y in labels], [y[1] for y in predictions])\n",
    "\n",
    "        return first, second\n",
    "\n",
    "    if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        aucs, aps = [], []\n",
    "\n",
    "        predictions = torch.stack(all_predictions)\n",
    "        labels = torch.stack(all_labels)\n",
    "\n",
    "        for i in range(labels.shape[1]):\n",
    "            # only compute if the class has at least one positive and one negative\n",
    "            if len(np.unique(labels[:, i])) == 2:\n",
    "                aucs.append(roc_auc_score(labels[:, i], predictions[:, i]))\n",
    "                aps.append(average_precision_score(labels[:, i], predictions[:, i]))\n",
    "            else:\n",
    "                # skip or set to NaN\n",
    "                aucs.append(np.nan)\n",
    "                aps.append(np.nan)\n",
    "        return np.nanmean(aucs), np.nanmean(aps)\n",
    "\n",
    "    f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    return f1, accuracy, (all_predictions, all_labels)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b473860dd9f34292",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-11-19T18:05:30.756898Z",
     "start_time": "2025-11-19T18:05:11.093111Z"
    }
   },
   "source": [
    "import tqdm\n",
    "import torch.nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from training.evaluation import local_coherence\n",
    "from utils.Config import Config\n",
    "from data.data_utils import *\n",
    "from training.inference import get_and_save_latents\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def local_tag_coherence(latent_dataset, max_k=100, granularity=1):\n",
    "    latent_paths = latent_dataset.latents\n",
    "    labels = latent_dataset.labels\n",
    "\n",
    "    latents = []\n",
    "    for path in tqdm(latent_paths):\n",
    "        l = torch.load(path, weights_only=False)\n",
    "        latents.append(l)\n",
    "\n",
    "    l = torch.tensor(latents)\n",
    "\n",
    "    if l.shape[1] == 1:\n",
    "        l = l.squeeze(1)\n",
    "\n",
    "    latents = np.array(l)\n",
    "\n",
    "    tag_coherence = []\n",
    "    k_values = []\n",
    "    ks = [x for x in range(1, max_k, granularity)]\n",
    "    for k in tqdm(ks):\n",
    "        lgc = local_coherence(np.array(latents), np.array(labels), k=k)\n",
    "        k_values.append(k)\n",
    "        tag_coherence.append(lgc)\n",
    "\n",
    "    auc_coh = auc(k_values, tag_coherence)\n",
    "\n",
    "    print('computed AUC using sklearn.metrics.auc: {}'.format(auc_coh / k))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(k_values, tag_coherence, label='Coherence')\n",
    "    plt.xlabel(\"K-Neighbors\")\n",
    "    plt.ylabel(\"Coherence\")\n",
    "    plt.title(\"Coherence v.s. Neighboorhood Size\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def grid_search(train_latent_dataset, valid_latent_dataset, num_classes=10, dataset=\"\", criterion=nn.CrossEntropyLoss()):\n",
    "    best_params = []\n",
    "    best_accuracy = 0\n",
    "    best_F1 = 0\n",
    "    pb = tqdm(total=18)\n",
    "\n",
    "    # if latent_dataset_test is not None:\n",
    "    #     n_fold_length = len(latent_dataset)\n",
    "    #     n_factor = int(n_fold_length / 10)\n",
    "    #     random_indicies = np.random.permutation(n_fold_length)\n",
    "    #\n",
    "    #     n_fold = random_indicies[:n_factor]\n",
    "    #     other_folds = random_indicies[n_factor:]\n",
    "    #\n",
    "    #     train_set = torch.utils.data.Subset(latent_dataset, other_folds)\n",
    "    #     test_set = torch.utils.data.Subset(latent_dataset, n_fold)\n",
    "    # else:\n",
    "    #     train_set = latent_dataset\n",
    "    #     test_set = latent_dataset_test\n",
    "\n",
    "    print(\"Grid Search...\")\n",
    "    for model_type in [\"MLP\"]:\n",
    "        for weight_decay in [1e-4, 1e-3]:\n",
    "            for learning_rate in [1e-5, 1e-4, 1e-3]:\n",
    "                for batch in [64]:\n",
    "                    for dropout in [0.25, 0.5] if model_type == \"MLP\" else [0]:\n",
    "\n",
    "                        model_name = f\"LinearClassifier-{dataset}\"\n",
    "                        config = Config(\n",
    "                                save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "                                num_epochs=1024,\n",
    "                                learning_rate=learning_rate,\n",
    "                                weight_decay=weight_decay,\n",
    "                                num_workers=2,\n",
    "                                batch_size= batch,\n",
    "                                eval_batch_size=batch,\n",
    "                                dtype=torch.float32\n",
    "                            )\n",
    "\n",
    "                        train_latent_dataloader = DataLoader(\n",
    "                            train_latent_dataset,\n",
    "                            batch_size=batch,\n",
    "                            shuffle=True,\n",
    "                        )\n",
    "\n",
    "                        test_latent_dataloader = DataLoader(\n",
    "                            valid_latent_dataset,\n",
    "                            batch_size=batch,\n",
    "                            shuffle=True,\n",
    "                        )\n",
    "\n",
    "                        device = \"cuda\"\n",
    "\n",
    "                        if model_type == \"MLP\":\n",
    "                            model = MLP(128, num_classes, dropout=dropout).to(device)\n",
    "                        else:\n",
    "                            model = LinearProbe(128, num_classes).to(device)\n",
    "\n",
    "                        if best_params is None:\n",
    "                            best_params = [model_type, learning_rate, weight_decay, batch, dropout]\n",
    "\n",
    "                        train_model(model, train_latent_dataloader, test_latent_dataloader, config, device=device, use_tqdm=False, criterion=criterion)\n",
    "                        f1, accuracy = evaluate_model(model, test_latent_dataloader, device, use_tqdm=False, criterion=criterion)\n",
    "\n",
    "                        if accuracy > best_accuracy:\n",
    "                            best_params = [model_type, learning_rate, weight_decay, batch, dropout]\n",
    "                            best_accuracy = accuracy\n",
    "                            best_F1 = f1\n",
    "\n",
    "                        pb.update(1)\n",
    "\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n",
    "    print(f\"Best F1: {best_F1}\")\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def train_valid(train_latent_dataset, test_latent_dataset, params, num_classes=10, dataset=\"\", criterion=nn.CrossEntropyLoss()):\n",
    "    model_type, learning_rate, weight_decay, batch, dropout = params\n",
    "    model_name = f\"LinearClassifier-{dataset}\"\n",
    "    print(\"Evaluating Dataset\")\n",
    "\n",
    "    config = Config(\n",
    "            save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "            num_epochs=1024,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            num_workers=2,\n",
    "            batch_size= batch,\n",
    "            eval_batch_size=batch,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    train_latent_dataloader = DataLoader(\n",
    "        train_latent_dataset,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    test_latent_dataloader = DataLoader(\n",
    "        test_latent_dataset,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    if model_type == \"MLP\":\n",
    "        model = MLP(128, num_classes, dropout=dropout).to(device)\n",
    "    else:\n",
    "        model = LinearProbe(128, num_classes).to(device)\n",
    "\n",
    "    train_model(model, train_latent_dataloader, None, config, device=device, use_tqdm=True, criterion=criterion, early_stopping=False)\n",
    "    torch.save(model, \"model.pt\")\n",
    "    scores = evaluate_model(model, test_latent_dataloader, device, use_tqdm=True, criterion=criterion, roc_pr_auc=True)\n",
    "    return scores"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b401d27ca8f0c86b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-11-19T18:05:31.810546Z",
     "start_time": "2025-11-19T18:05:30.763905Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tsne(dataset, perplexity=30, n_iter=1000, random_state=42, figsize=(8, 6), save_path=None):\n",
    "    latent_paths = dataset.latents\n",
    "    labels = dataset.labels\n",
    "\n",
    "    latents = []\n",
    "    for path in tqdm(latent_paths):\n",
    "        l = torch.load(path, weights_only=False)\n",
    "        latents.append(l)\n",
    "\n",
    "    l = torch.tensor(latents)\n",
    "\n",
    "    if l.shape[1] == 1:\n",
    "        l = l.squeeze(1)\n",
    "\n",
    "    X = np.array(l)\n",
    "\n",
    "    # Convert latents to a numpy array\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"Expected latents of shape [N, dim], got {X.shape}\")\n",
    "\n",
    "    # Compute t-SNE\n",
    "    tsne_model = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n",
    "    X_tsne = tsne_model.fit_transform(X)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    if labels is not None:\n",
    "        labels = np.array(labels)\n",
    "        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', alpha=0.8)\n",
    "        plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "    else:\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.8)\n",
    "\n",
    "    plt.title(\"t-SNE of Latent Embeddings\")\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3e205acb4ce2d4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from data.data_utils import GS\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\GS\\\\\"\n",
    "GS_dataset = GS(directory)\n",
    "GS_dataloader = DataLoader(\n",
    "    GS_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\GS-MTG\\\\\"\n",
    "GS_MTG_train_dataset = GS(directory, split=\"train\")\n",
    "GS_MTG_train_dataloader = DataLoader(\n",
    "    GS_MTG_train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\GS-MTG\\\\\"\n",
    "GS_MTG_valid_dataset = GS(directory, split=\"valid\")\n",
    "GS_MTG_valid_dataloader = DataLoader(\n",
    "    GS_MTG_valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "def gs_eval(model, name, chunking=True, chunk_size=1024, averaging=True, k=100, already_generated=False,\n",
    "              granularity=5):\n",
    "\n",
    "    if not already_generated:\n",
    "        generate_evaluation_dataset(model, \"GS\", GS_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                    averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"GS-MTG-Valid\", GS_MTG_valid_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                    averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"GS-MTG-Train\", GS_MTG_train_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                    averaging=averaging)\n",
    "\n",
    "    num_classes = 36\n",
    "\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\GS-MTG-Train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\GS-MTG-Valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\GS\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "\n",
    "    train_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\GS-MTG-Train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    valid_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\GS-MTG-Valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    test_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\GS\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    combined_dataset = ConcatDataset([train_latent_dataset, valid_latent_dataset])\n",
    "\n",
    "    #tsne(combined_dataset)\n",
    "    #local_tag_coherence(train_latent_dataset, max_k=k, granularity=granularity)\n",
    "\n",
    "    params = grid_search(train_latent_dataset, valid_latent_dataset, num_classes, criterion=nn.CrossEntropyLoss())\n",
    "\n",
    "    for param in params:\n",
    "        print(param)\n",
    "\n",
    "    results = train_valid(combined_dataset, test_latent_dataset, params, criterion=nn.CrossEntropyLoss(), num_classes=num_classes)\n",
    "\n",
    "    accuracy_score, f1_score = results\n",
    "\n",
    "    print(f\"F1: {f1_score}\\t variance: {accuracy_score}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e922aef7-68f3-4c7f-b87a-de0135cb9a52",
   "metadata": {},
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from data.data_utils import MTAT\n",
    "from data.data_utils import EmoMusic\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\EmoMusic\\\\\"\n",
    "Emo_train_dataset = EmoMusic(directory, split=\"train\")\n",
    "Emo_train_dataloader = DataLoader(\n",
    "    Emo_train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "Emo_valid_dataset = EmoMusic(directory, split=\"valid\")\n",
    "Emo_valid_dataloader = DataLoader(\n",
    "    Emo_valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "Emo_test_dataset = EmoMusic(directory, split=\"test\")\n",
    "Emo_test_dataloader = DataLoader(\n",
    "    Emo_test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "def emo_eval(model, name, chunking=True, chunk_size=1024, averaging=True, already_generated=False):\n",
    "    if not already_generated:\n",
    "        generate_evaluation_dataset(model, \"EmoMusic-Train\", Emo_train_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                    averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"EmoMusic-Valid\", Emo_valid_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                        averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"EmoMusic-Test\", Emo_test_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                        averaging=averaging)\n",
    "\n",
    "    num_classes = 2\n",
    "\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\EmoMusic-Train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\EmoMusic-Valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\EmoMusic-Test\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "\n",
    "    train_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\EmoMusic-Train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    valid_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\EmoMusic-Valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    test_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\EmoMusic-Test\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    combined_dataset = ConcatDataset([train_latent_dataset, valid_latent_dataset])\n",
    "\n",
    "    #tsne(latent_train_dataset)\n",
    "    #local_tag_coherence(latent_train_dataset, max_k=k, granularity=granularity)\n",
    "\n",
    "    #tsne(latent_dataset)\n",
    "    #local_tag_coherence(combined_dataset, max_k=k, granularity=granularity)\n",
    "\n",
    "    params = grid_search(train_latent_dataset, valid_latent_dataset, num_classes=num_classes, criterion=nn.MSELoss())\n",
    "    for param in params:\n",
    "        print(param)\n",
    "\n",
    "    results = train_valid(combined_dataset, test_latent_dataset, params, criterion=nn.MSELoss(), num_classes=num_classes)\n",
    "\n",
    "    accuracy_score, f1_score = results\n",
    "\n",
    "    print(f\"F1: {f1_score}\\t variance: {accuracy_score}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fe6cc6865b7454c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import torch.nn\n",
    "\n",
    "from data.data_utils import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\GTZAN\\\\\"\n",
    "GTZAN_train_dataset = GTZAN(directory, split=\"train\")\n",
    "GTZAN_train_dataloader = DataLoader(\n",
    "    GTZAN_train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "GTZAN_valid_dataset = GTZAN(directory, split=\"valid\")\n",
    "GTZAN_valid_dataloader = DataLoader(\n",
    "    GTZAN_train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "GTZAN_test_dataset = GTZAN(directory, split=\"test\")\n",
    "GTZAN_test_dataloader = DataLoader(\n",
    "    GTZAN_train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "def gtzan_eval(model, name, chunking=True, chunk_size=256, averaging=True, k=100, already_generated=False, granularity=5):\n",
    "    if not already_generated:\n",
    "        generate_evaluation_dataset(model, \"GTZAN_train\", GTZAN_train_dataset, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                    averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"GTZAN_valid\", GTZAN_valid_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                    averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"GTZAN_test\", GTZAN_test_dataloader, name, chunking=chunking, chunk_size=chunk_size,\n",
    "                                    averaging=averaging)\n",
    "\n",
    "    num_classes = 10\n",
    "\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\GTZAN_train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\GTZAN_valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\GTZAN_test\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "\n",
    "    train_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\GTZAN_train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    valid_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\GTZAN_valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    test_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\GTZAN_test\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    combined_dataset = ConcatDataset([train_latent_dataset, valid_latent_dataset])\n",
    "\n",
    "    #tsne(combined_dataset)\n",
    "    #local_tag_coherence(combined_dataset, max_k=k, granularity=granularity)\n",
    "\n",
    "    params = grid_search(train_latent_dataset, valid_latent_dataset, num_classes, criterion=nn.CrossEntropyLoss())\n",
    "    for param in params:\n",
    "        print(param)\n",
    "\n",
    "    results = train_valid(combined_dataset, test_latent_dataset, params, criterion=nn.CrossEntropyLoss(), num_classes=num_classes)\n",
    "\n",
    "    accuracy_score, f1_score = results\n",
    "\n",
    "    print(f\"F1: {f1_score}\\t variance: {accuracy_score}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8bb97dbd299a397d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-11-19T18:05:37.620567Z",
     "start_time": "2025-11-19T18:05:31.825100Z"
    }
   },
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from data.data_utils import MTAT\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\MTAT\\\\\"\n",
    "\n",
    "MTAT_train_dataset = MTAT(directory, split=\"train\")\n",
    "MTAT_train_dataloader = DataLoader(\n",
    "    MTAT_train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "MTAT_valid_dataset = MTAT(directory, split=\"valid\")\n",
    "MTAT_valid_dataloader = DataLoader(\n",
    "    MTAT_valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "MTAT_test_dataset = MTAT(directory, split=\"test\")\n",
    "MTAT_test_dataloader = DataLoader(\n",
    "    MTAT_test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")\n",
    "\n",
    "def mtat_eval(model, name, chunking=True, chunk_size=1024, averaging=True, k=100, already_generated=False, granularity=5):\n",
    "    if not already_generated:\n",
    "        generate_evaluation_dataset(model, \"MTAT-Train\", MTAT_train_dataloader, name, chunking=chunking, chunk_size=chunk_size, averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"MTAT-Valid\", MTAT_valid_dataloader, name, chunking=chunking, chunk_size=chunk_size, averaging=averaging)\n",
    "        generate_evaluation_dataset(model, \"MTAT-Test\", MTAT_test_dataloader, name, chunking=chunking, chunk_size=chunk_size, averaging=averaging)\n",
    "\n",
    "    num_classes = 188\n",
    "\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\MTAT-Train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\MTAT-Valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "    os.makedirs(f\"D:\\\\SongsDataset\\\\MTAT-Test\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", exist_ok=True)\n",
    "\n",
    "    train_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\MTAT-Train\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    valid_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\MTAT-Valid\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "    test_latent_dataset = LatentDataset(f\"D:\\\\SongsDataset\\\\MTAT-Test\\\\latent_datasets\\\\{name}\\\\\" + \"full-set\\\\\", num_classes=num_classes)\n",
    "\n",
    "    combined_dataset = ConcatDataset([train_latent_dataset, valid_latent_dataset])\n",
    "\n",
    "    #tsne(combined_dataset)\n",
    "    #local_tag_coherence(combined_dataset, max_k=k, granularity=granularity)\n",
    "\n",
    "    #params = grid_search(train_latent_dataset, valid_latent_dataset, num_classes, criterion=nn.BCEWithLogitsLoss())\n",
    "    params = (\"MLP\", 0.0001, 0.0001, 64, 0.25)\n",
    "\n",
    "    for param in params:\n",
    "        print(param)\n",
    "\n",
    "    results = train_valid(combined_dataset, test_latent_dataset, params, num_classes=num_classes, criterion=nn.BCEWithLogitsLoss())\n",
    "\n",
    "    f1_score, accuracy,  = results\n",
    "\n",
    "    visualize_ROC_PR_AUC(all_predictions, all_labels)\n",
    "\n",
    "    print(f\"F1: {f1_score}\\t variance: {accuracy_score}\")\n",
    "\n",
    "    return (all_predictions, all_labels)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/25863 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68cfde6699004adbac5a65ce326e9279"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Coding\\SongAnalyzer\\Analyzer\\src\\data\\data_utils.py:18: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(full_path, sr=44100, mono=True)\n",
      "E:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/25863 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54995ecefc8444ba8b6d245ac6cbef42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/25863 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f12742d31e04ce09cef5ea75fded32a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a9bdab1ddf13a9a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-11-19T18:05:37.628210Z",
     "start_time": "2025-11-19T18:05:37.624574Z"
    }
   },
   "source": [
    "def add_fields(model, use_sinusoidal=False, use_y_emb=False,\n",
    "               use_rope_x=False, use_rope_y=False, rope_base=-1,\n",
    "               use_alibi_x=False, use_alibi_y=False):\n",
    "\n",
    "    model.use_cls = True\n",
    "    model.predict_tempo = False\n",
    "    model.use_sinusoidal = use_sinusoidal\n",
    "    model.use_y_emb = use_y_emb\n",
    "    model.use_rope_x = use_rope_x\n",
    "    model.use_rope_y = use_rope_y\n",
    "    model.rope_base = rope_base\n",
    "    model.use_alibi_x = use_alibi_x\n",
    "    model.use_alibi_y = use_alibi_y\n",
    "    model.needs_coordinates = use_rope_x or use_rope_y or use_alibi_x or use_alibi_y\n",
    "\n",
    "    if not (use_alibi_x or use_alibi_y):\n",
    "        model.transformer.alibi_2d = None\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-sinusoidal-ALIBI-256L-0.9M\\\\sinusoidal_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_sinusoidal=True)\n",
    "name = \"Sinusoidal-Chunking-256\"\n",
    "(all_predictions, all_labels) = mtat_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "50cfe09abfa79622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualize_ROC_PR_AUC(all_predictions, all_labels)",
   "id": "b92aeebcee09d321"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-1D-ALIBI-256L-0.9M\\\\1d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_x=True, use_y_emb=True)\n",
    "name = \"1D-ALIBI-No-Chunking\"\n",
    "emo_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=False)"
   ],
   "id": "3708ed48ac2b74c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "name = \"1D-ALIBI-No-Chunking\"\n",
    "emo_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=False)"
   ],
   "id": "2137e81dde89be66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-2D-ALIBI-256L-0.9M\\\\2d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_y=True, use_alibi_x=True)\n",
    "name = \"2D-ALIBI-Chunking-256\"\n",
    "#emo_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "d0d8ba1c4bfe1b5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "name = \"2D-ALIBI-No-Chunking\"\n",
    "emo_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=False)"
   ],
   "id": "4ec50e0c4af20fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# ---------------------------------------------------",
   "id": "aa15c67635dac474",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-sinusoidal-ALIBI-256L-0.9M\\\\sinusoidal_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_sinusoidal=True)\n",
    "name = \"Sinusoidal-Chunking-256\"\n",
    "gs_eval(model, name, chunking=True, chunk_size=256, averaging=True, k=200, already_generated=False, granularity=10)"
   ],
   "id": "29595bd1e2b79971",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-1D-ALIBI-256L-0.9M\\\\1d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_x=True, use_y_emb=True)\n",
    "name = \"1D-ALIBI-No-Chunking\"\n",
    "gs_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=False)"
   ],
   "id": "82efcfb9562e0412",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gs_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=False)",
   "id": "a2c436a6e97a1bfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-2D-ALIBI-256L-0.9M\\\\2d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_y=True, use_alibi_x=True)\n",
    "name = \"2D-ALIBI-No-Chunking\"\n",
    "gs_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "3f098b5533c2cbd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gs_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=False)",
   "id": "3744f1e25c58ce21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# ---------------------------------------------------",
   "id": "4f19e0fa29bb6834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-sinusoidal-ALIBI-256L-0.9M\\\\sinusoidal_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_sinusoidal=True)\n",
    "name = \"Sinusoidal-Chunking-256\"\n",
    "gtzan_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=False, k=200, granularity=10)"
   ],
   "id": "d1fc3f98dd11a610",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-1D-ALIBI-256L-0.9M\\\\1d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_x=True, use_y_emb=True)\n",
    "name = \"1D-ALIBI-Chunking-256\"\n",
    "#gtzan_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=True, k=200, granularity=10)"
   ],
   "id": "c705d9ff95f136f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "name = \"1D-ALIBI-No-Chunking\"\n",
    "gtzan_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=True, k=200, granularity=10)"
   ],
   "id": "210e8a71534769e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a2117ec76054623",
   "metadata": {},
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-2D-ALIBI-256L-0.9M\\\\2d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_y=True, use_alibi_x=True)\n",
    "name = \"2D-ALIBI-Chunking-256\"\n",
    "#gtzan_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=True, k=200, granularity=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "name = \"2D-ALIBI-No-Chunking\"\n",
    "gtzan_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=False, k=200, granularity=10)"
   ],
   "id": "33d43a455415ec35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# ---------------------------------------------------",
   "id": "214dcec88c22544e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-sinusoidal-ALIBI-256L-0.9M\\\\sinusoidal_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_sinusoidal=True)\n",
    "name = \"Sinusoidal-Chunking-256\"\n",
    "mtat_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "344333499fee534",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-1D-ALIBI-256L-0.9M\\\\1d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_x=True, use_y_emb=True)\n",
    "name = \"1D-ALIBI-Chunking-256\"\n",
    "#mtat_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "9315c1f77c749b66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "name = \"1D-ALIBI-No-Chunking\"\n",
    "mtat_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "b9906af0dcffcfbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\final-models\\\\Myna-2D-ALIBI-256L-0.9M\\\\2d_alibi_Epoch-511.pt\", weights_only=False)\n",
    "model.mask_ratio = 0\n",
    "model = add_fields(model, use_alibi_y=True, use_alibi_x=True)\n",
    "name = \"2D-ALIBI-Chunking-256\"\n",
    "#mtat_eval(model, name, chunking=True, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "954b9b0f9e6faad4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "name = \"2D-ALIBI-No-Chunking\"\n",
    "mtat_eval(model, name, chunking=False, chunk_size=256, averaging=True, already_generated=True)"
   ],
   "id": "5157770efc838b0a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
