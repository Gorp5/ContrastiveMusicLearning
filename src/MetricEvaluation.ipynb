{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:43:35.694594Z",
     "start_time": "2025-10-21T04:43:27.566900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from training.inference import get_latents\n",
    "from datasets import tqdm\n",
    "\n",
    "def generate_evaluation_dataset(model, dataloader, name, chunking=True, averaging=False, chunk_size=256):\n",
    "    model.mask_ratio = 0.0\n",
    "    latents, labels = get_latents(dataloader, model, chunking=chunking, averaging=averaging, chunk_size=chunk_size)\n",
    "    print(\"Saving...\")\n",
    "\n",
    "    index = 0\n",
    "    for latent, label in zip(latents, labels):\n",
    "        directory = f\"D:\\\\SongsDataset\\\\GTZAN\\\\latent_datasets\\\\{name}\\\\\"\n",
    "        os.makedirs(os.path.dirname(directory), exist_ok=True)\n",
    "\n",
    "        if not averaging and chunking:\n",
    "            for x in range(label.shape[0]):\n",
    "                label_name = directory + f\"{index}_label\"\n",
    "                latent_name = directory + f\"{index}_data\"\n",
    "                torch.save(latent, latent_name + \".pt\")\n",
    "                torch.save(label, label_name + \".pt\")\n",
    "                index += 1\n",
    "        else:\n",
    "            label_name = directory + f\"{index}_label\"\n",
    "            latent_name = directory + f\"{index}_data\"\n",
    "            torch.save(latent, latent_name + \".pt\")\n",
    "            torch.save(label, label_name + \".pt\")\n",
    "            index += 1\n"
   ],
   "id": "26862cea14b0f113",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:44:59.407191Z",
     "start_time": "2025-10-21T04:43:35.704596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.data_utils import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\GTZAN\\\\genres_original\\\\\"\n",
    "GTZAN_dataset = GTZAN(directory)\n",
    "GTZAN_dataloader = DataLoader(\n",
    "    GTZAN_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")"
   ],
   "id": "5fe6cc6865b7454c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fecdf952fd5b490dbaa277a32c9bb205"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:45:38.296553Z",
     "start_time": "2025-10-21T04:44:59.779977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-Sinusoidal\\\\Epoch-79.pt\", weights_only=False)\n",
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"Sinusoidal-Chunking-256\", chunking=True, chunk_size=256, averaging=True)"
   ],
   "id": "78096b22d19bab28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:30<00:00, 33.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:46:10.272219Z",
     "start_time": "2025-10-21T04:45:38.301553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-ALIBI\\\\Epoch-103.pt\", weights_only=False)\n",
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-256\", chunking=True, chunk_size=256, averaging=True)"
   ],
   "id": "b03abf156baf1432",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:29<00:00, 34.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:01:20.686306Z",
     "start_time": "2025-10-21T05:00:43.974118Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-512\", chunking=True, chunk_size=512, averaging=True)",
   "id": "2d7c45006464f818",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:35<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:01:50.899411Z",
     "start_time": "2025-10-21T05:01:20.692305Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-1024\", chunking=True, chunk_size=1024, averaging=True)",
   "id": "a532740122b261b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:29<00:00, 34.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:02:30.196075Z",
     "start_time": "2025-10-21T05:01:50.910074Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-2048\", chunking=True, chunk_size=2048, averaging=True)",
   "id": "f74f278dbbc18557",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:38<00:00, 26.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T03:52:49.646516Z",
     "start_time": "2025-10-21T03:52:34.162876Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-4096\", chunking=True, chunk_size=4096)",
   "id": "abf4a9871a802333",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgenerate_evaluation_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGTZAN_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mALIBI-Chunking-4096\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4096\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 9\u001B[0m, in \u001B[0;36mgenerate_evaluation_dataset\u001B[1;34m(model, dataloader, name, chunking, chunk_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_evaluation_dataset\u001B[39m(model, dataloader, name, chunking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m):\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m.\u001B[39mmask_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m----> 9\u001B[0m     latents, labels \u001B[38;5;241m=\u001B[39m \u001B[43mget_latents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m index, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(latents, labels)):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:33\u001B[0m, in \u001B[0;36mget_latents\u001B[1;34m(dataloader, model, chunking, chunk_size)\u001B[0m\n\u001B[0;32m     30\u001B[0m     data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(data)\n\u001B[0;32m     31\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m latent \u001B[38;5;241m=\u001B[39m \u001B[43mrun_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maveraging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m all_latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     35\u001B[0m all_labels\u001B[38;5;241m.\u001B[39mappend(label)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:51\u001B[0m, in \u001B[0;36mrun_batch\u001B[1;34m(model, batch, averaging)\u001B[0m\n\u001B[0;32m     49\u001B[0m latents \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[1;32m---> 51\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m     latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     54\u001B[0m latents \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(latents, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:163\u001B[0m, in \u001B[0;36mMyna.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    160\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((cls_tokens, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malibi:\n\u001B[1;32m--> 163\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoordinates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:86\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, coords, cls)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, coords\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m coords \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\PositionalEmbeddings.py:51\u001B[0m, in \u001B[0;36mAlibi2DBias.forward\u001B[1;34m(self, coords)\u001B[0m\n\u001B[0;32m     48\u001B[0m dist \u001B[38;5;241m=\u001B[39m dx \u001B[38;5;241m+\u001B[39m dy\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# raster order\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m flat \u001B[38;5;241m=\u001B[39m y \u001B[38;5;241m*\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m     52\u001B[0m le_mask \u001B[38;5;241m=\u001B[39m (flat[:, \u001B[38;5;28;01mNone\u001B[39;00m, :] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m flat[:, :, \u001B[38;5;28;01mNone\u001B[39;00m])\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     54\u001B[0m slopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslopes\u001B[38;5;241m.\u001B[39mto(coords\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T03:52:07.147307Z",
     "start_time": "2025-10-21T03:51:50.599583Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-No-Chunking\", chunking=True, chunk_size=16384)",
   "id": "3b4516ba90a5ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgenerate_evaluation_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGTZAN_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mALIBI-Chunking-No-Chunking\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16384\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 9\u001B[0m, in \u001B[0;36mgenerate_evaluation_dataset\u001B[1;34m(model, dataloader, name, chunking, chunk_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_evaluation_dataset\u001B[39m(model, dataloader, name, chunking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m):\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m.\u001B[39mmask_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m----> 9\u001B[0m     latents, labels \u001B[38;5;241m=\u001B[39m \u001B[43mget_latents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m index, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(latents, labels)):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:33\u001B[0m, in \u001B[0;36mget_latents\u001B[1;34m(dataloader, model, chunking, chunk_size)\u001B[0m\n\u001B[0;32m     30\u001B[0m     data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(data)\n\u001B[0;32m     31\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m latent \u001B[38;5;241m=\u001B[39m \u001B[43mrun_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maveraging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m all_latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     35\u001B[0m all_labels\u001B[38;5;241m.\u001B[39mappend(label)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:51\u001B[0m, in \u001B[0;36mrun_batch\u001B[1;34m(model, batch, averaging)\u001B[0m\n\u001B[0;32m     49\u001B[0m latents \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[1;32m---> 51\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m     latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     54\u001B[0m latents \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(latents, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:163\u001B[0m, in \u001B[0;36mMyna.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    160\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((cls_tokens, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malibi:\n\u001B[1;32m--> 163\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoordinates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:86\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, coords, cls)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, coords\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m coords \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\PositionalEmbeddings.py:51\u001B[0m, in \u001B[0;36mAlibi2DBias.forward\u001B[1;34m(self, coords)\u001B[0m\n\u001B[0;32m     48\u001B[0m dist \u001B[38;5;241m=\u001B[39m dx \u001B[38;5;241m+\u001B[39m dy\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# raster order\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m flat \u001B[38;5;241m=\u001B[39m y \u001B[38;5;241m*\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m     52\u001B[0m le_mask \u001B[38;5;241m=\u001B[39m (flat[:, \u001B[38;5;28;01mNone\u001B[39;00m, :] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m flat[:, :, \u001B[38;5;28;01mNone\u001B[39;00m])\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     54\u001B[0m slopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslopes\u001B[38;5;241m.\u001B[39mto(coords\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-21T04:58:49.241250Z",
     "start_time": "2025-10-21T04:58:49.228250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.data_utils import LatentDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import tqdm\n",
    "\n",
    "# ----- Training Loop -----\n",
    "def train_model(model, dataloader, config, device=\"cuda\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(config.num_epochs)):\n",
    "        total_loss = 0\n",
    "        for labels, data in dataloader:\n",
    "            # if dataset returns one-hot, convert back to integer for CrossEntropy\n",
    "            if labels.ndim > 1:\n",
    "                labels = labels.argmax(dim=1)\n",
    "\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        if epoch % 4 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config.num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T04:55:58.261604Z",
     "start_time": "2025-10-21T04:55:58.258077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "\n",
    "# ----- Model -----\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ],
   "id": "96cc0615fb6248ef",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:06:14.293154Z",
     "start_time": "2025-10-21T05:06:14.285984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.Config import Config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def test_latent_dataaset(dataset):\n",
    "    directory = f\"D:\\\\SongsDataset\\\\GTZAN\\\\latent_datasets\\\\{dataset}\\\\\"\n",
    "\n",
    "    num_classes = 10\n",
    "    latent_dataset = LatentDataset(directory, num_classes=num_classes)\n",
    "    latent_dataloader = DataLoader(\n",
    "        latent_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    model_name = f\"LinearClassifier-{dataset}\"\n",
    "    config = Config(\n",
    "            save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "            num_epochs=1024,\n",
    "            learning_rate=1e-3,\n",
    "            weight_decay=1e-3,\n",
    "            num_workers=2,\n",
    "            batch_size= 64,\n",
    "            eval_batch_size=64,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    device = \"cuda\"\n",
    "    model = LinearClassifier(128, num_classes).to(device)\n",
    "    train_model(model, latent_dataloader, config, device=device)"
   ],
   "id": "e0918c9fc1788c45",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:07:36.780744Z",
     "start_time": "2025-10-21T05:06:14.473420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"Sinusoidal-Chunking-256\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "de1b0ebaf9e0603b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4208ba8ac07b4ec0b1f88d249dc2fae5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024, Loss: 2.1795\n",
      "Epoch 5/1024, Loss: 1.2623\n",
      "Epoch 9/1024, Loss: 1.0303\n",
      "Epoch 13/1024, Loss: 0.9230\n",
      "Epoch 17/1024, Loss: 0.8548\n",
      "Epoch 21/1024, Loss: 0.8119\n",
      "Epoch 25/1024, Loss: 0.7707\n",
      "Epoch 29/1024, Loss: 0.7443\n",
      "Epoch 33/1024, Loss: 0.7243\n",
      "Epoch 37/1024, Loss: 0.7032\n",
      "Epoch 41/1024, Loss: 0.6867\n",
      "Epoch 45/1024, Loss: 0.6779\n",
      "Epoch 49/1024, Loss: 0.6594\n",
      "Epoch 53/1024, Loss: 0.6557\n",
      "Epoch 57/1024, Loss: 0.6393\n",
      "Epoch 61/1024, Loss: 0.6281\n",
      "Epoch 65/1024, Loss: 0.6190\n",
      "Epoch 69/1024, Loss: 0.6167\n",
      "Epoch 73/1024, Loss: 0.6026\n",
      "Epoch 77/1024, Loss: 0.5960\n",
      "Epoch 81/1024, Loss: 0.5854\n",
      "Epoch 85/1024, Loss: 0.5906\n",
      "Epoch 89/1024, Loss: 0.5787\n",
      "Epoch 93/1024, Loss: 0.5769\n",
      "Epoch 97/1024, Loss: 0.5724\n",
      "Epoch 101/1024, Loss: 0.5676\n",
      "Epoch 105/1024, Loss: 0.5646\n",
      "Epoch 109/1024, Loss: 0.5569\n",
      "Epoch 113/1024, Loss: 0.5494\n",
      "Epoch 117/1024, Loss: 0.5506\n",
      "Epoch 121/1024, Loss: 0.5449\n",
      "Epoch 125/1024, Loss: 0.5418\n",
      "Epoch 129/1024, Loss: 0.5424\n",
      "Epoch 133/1024, Loss: 0.5325\n",
      "Epoch 137/1024, Loss: 0.5346\n",
      "Epoch 141/1024, Loss: 0.5223\n",
      "Epoch 145/1024, Loss: 0.5203\n",
      "Epoch 149/1024, Loss: 0.5279\n",
      "Epoch 153/1024, Loss: 0.5215\n",
      "Epoch 157/1024, Loss: 0.5165\n",
      "Epoch 161/1024, Loss: 0.5121\n",
      "Epoch 165/1024, Loss: 0.5118\n",
      "Epoch 169/1024, Loss: 0.5107\n",
      "Epoch 173/1024, Loss: 0.5179\n",
      "Epoch 177/1024, Loss: 0.5016\n",
      "Epoch 181/1024, Loss: 0.4985\n",
      "Epoch 185/1024, Loss: 0.4987\n",
      "Epoch 189/1024, Loss: 0.4963\n",
      "Epoch 193/1024, Loss: 0.4913\n",
      "Epoch 197/1024, Loss: 0.4898\n",
      "Epoch 201/1024, Loss: 0.4904\n",
      "Epoch 205/1024, Loss: 0.4915\n",
      "Epoch 209/1024, Loss: 0.4828\n",
      "Epoch 213/1024, Loss: 0.4873\n",
      "Epoch 217/1024, Loss: 0.4816\n",
      "Epoch 221/1024, Loss: 0.4786\n",
      "Epoch 225/1024, Loss: 0.4771\n",
      "Epoch 229/1024, Loss: 0.4700\n",
      "Epoch 233/1024, Loss: 0.4703\n",
      "Epoch 237/1024, Loss: 0.4678\n",
      "Epoch 241/1024, Loss: 0.4701\n",
      "Epoch 245/1024, Loss: 0.4672\n",
      "Epoch 249/1024, Loss: 0.4645\n",
      "Epoch 253/1024, Loss: 0.4617\n",
      "Epoch 257/1024, Loss: 0.4624\n",
      "Epoch 261/1024, Loss: 0.4669\n",
      "Epoch 265/1024, Loss: 0.4596\n",
      "Epoch 269/1024, Loss: 0.4560\n",
      "Epoch 273/1024, Loss: 0.4576\n",
      "Epoch 277/1024, Loss: 0.4528\n",
      "Epoch 281/1024, Loss: 0.4568\n",
      "Epoch 285/1024, Loss: 0.4526\n",
      "Epoch 289/1024, Loss: 0.4506\n",
      "Epoch 293/1024, Loss: 0.4626\n",
      "Epoch 297/1024, Loss: 0.4503\n",
      "Epoch 301/1024, Loss: 0.4525\n",
      "Epoch 305/1024, Loss: 0.4409\n",
      "Epoch 309/1024, Loss: 0.4430\n",
      "Epoch 313/1024, Loss: 0.4417\n",
      "Epoch 317/1024, Loss: 0.4468\n",
      "Epoch 321/1024, Loss: 0.4445\n",
      "Epoch 325/1024, Loss: 0.4398\n",
      "Epoch 329/1024, Loss: 0.4441\n",
      "Epoch 333/1024, Loss: 0.4334\n",
      "Epoch 337/1024, Loss: 0.4342\n",
      "Epoch 341/1024, Loss: 0.4362\n",
      "Epoch 345/1024, Loss: 0.4390\n",
      "Epoch 349/1024, Loss: 0.4310\n",
      "Epoch 353/1024, Loss: 0.4256\n",
      "Epoch 357/1024, Loss: 0.4303\n",
      "Epoch 361/1024, Loss: 0.4274\n",
      "Epoch 365/1024, Loss: 0.4257\n",
      "Epoch 369/1024, Loss: 0.4306\n",
      "Epoch 373/1024, Loss: 0.4213\n",
      "Epoch 377/1024, Loss: 0.4193\n",
      "Epoch 381/1024, Loss: 0.4199\n",
      "Epoch 385/1024, Loss: 0.4254\n",
      "Epoch 389/1024, Loss: 0.4205\n",
      "Epoch 393/1024, Loss: 0.4189\n",
      "Epoch 397/1024, Loss: 0.4252\n",
      "Epoch 401/1024, Loss: 0.4179\n",
      "Epoch 405/1024, Loss: 0.4158\n",
      "Epoch 409/1024, Loss: 0.4136\n",
      "Epoch 413/1024, Loss: 0.4140\n",
      "Epoch 417/1024, Loss: 0.4145\n",
      "Epoch 421/1024, Loss: 0.4136\n",
      "Epoch 425/1024, Loss: 0.4125\n",
      "Epoch 429/1024, Loss: 0.4155\n",
      "Epoch 433/1024, Loss: 0.4113\n",
      "Epoch 437/1024, Loss: 0.4096\n",
      "Epoch 441/1024, Loss: 0.4085\n",
      "Epoch 445/1024, Loss: 0.4127\n",
      "Epoch 449/1024, Loss: 0.4088\n",
      "Epoch 453/1024, Loss: 0.4129\n",
      "Epoch 457/1024, Loss: 0.4022\n",
      "Epoch 461/1024, Loss: 0.4061\n",
      "Epoch 465/1024, Loss: 0.4070\n",
      "Epoch 469/1024, Loss: 0.4030\n",
      "Epoch 473/1024, Loss: 0.4039\n",
      "Epoch 477/1024, Loss: 0.4012\n",
      "Epoch 481/1024, Loss: 0.4044\n",
      "Epoch 485/1024, Loss: 0.4024\n",
      "Epoch 489/1024, Loss: 0.3972\n",
      "Epoch 493/1024, Loss: 0.4047\n",
      "Epoch 497/1024, Loss: 0.3961\n",
      "Epoch 501/1024, Loss: 0.3954\n",
      "Epoch 505/1024, Loss: 0.3985\n",
      "Epoch 509/1024, Loss: 0.3926\n",
      "Epoch 513/1024, Loss: 0.3933\n",
      "Epoch 517/1024, Loss: 0.4081\n",
      "Epoch 521/1024, Loss: 0.3942\n",
      "Epoch 525/1024, Loss: 0.3959\n",
      "Epoch 529/1024, Loss: 0.3927\n",
      "Epoch 533/1024, Loss: 0.3913\n",
      "Epoch 537/1024, Loss: 0.3902\n",
      "Epoch 541/1024, Loss: 0.3883\n",
      "Epoch 545/1024, Loss: 0.3965\n",
      "Epoch 549/1024, Loss: 0.3893\n",
      "Epoch 553/1024, Loss: 0.3880\n",
      "Epoch 557/1024, Loss: 0.3910\n",
      "Epoch 561/1024, Loss: 0.3857\n",
      "Epoch 565/1024, Loss: 0.3890\n",
      "Epoch 569/1024, Loss: 0.3855\n",
      "Epoch 573/1024, Loss: 0.3837\n",
      "Epoch 577/1024, Loss: 0.3850\n",
      "Epoch 581/1024, Loss: 0.3822\n",
      "Epoch 585/1024, Loss: 0.3836\n",
      "Epoch 589/1024, Loss: 0.3781\n",
      "Epoch 593/1024, Loss: 0.3863\n",
      "Epoch 597/1024, Loss: 0.3829\n",
      "Epoch 601/1024, Loss: 0.3799\n",
      "Epoch 605/1024, Loss: 0.3776\n",
      "Epoch 609/1024, Loss: 0.3785\n",
      "Epoch 613/1024, Loss: 0.3814\n",
      "Epoch 617/1024, Loss: 0.3775\n",
      "Epoch 621/1024, Loss: 0.3749\n",
      "Epoch 625/1024, Loss: 0.3740\n",
      "Epoch 629/1024, Loss: 0.3777\n",
      "Epoch 633/1024, Loss: 0.3755\n",
      "Epoch 637/1024, Loss: 0.3767\n",
      "Epoch 641/1024, Loss: 0.3804\n",
      "Epoch 645/1024, Loss: 0.3738\n",
      "Epoch 649/1024, Loss: 0.3706\n",
      "Epoch 653/1024, Loss: 0.3708\n",
      "Epoch 657/1024, Loss: 0.3739\n",
      "Epoch 661/1024, Loss: 0.3693\n",
      "Epoch 665/1024, Loss: 0.3741\n",
      "Epoch 669/1024, Loss: 0.3730\n",
      "Epoch 673/1024, Loss: 0.3817\n",
      "Epoch 677/1024, Loss: 0.3709\n",
      "Epoch 681/1024, Loss: 0.3683\n",
      "Epoch 685/1024, Loss: 0.3686\n",
      "Epoch 689/1024, Loss: 0.3710\n",
      "Epoch 693/1024, Loss: 0.3651\n",
      "Epoch 697/1024, Loss: 0.3624\n",
      "Epoch 701/1024, Loss: 0.3672\n",
      "Epoch 705/1024, Loss: 0.3625\n",
      "Epoch 709/1024, Loss: 0.3665\n",
      "Epoch 713/1024, Loss: 0.3653\n",
      "Epoch 717/1024, Loss: 0.3628\n",
      "Epoch 721/1024, Loss: 0.3666\n",
      "Epoch 725/1024, Loss: 0.3643\n",
      "Epoch 729/1024, Loss: 0.3635\n",
      "Epoch 733/1024, Loss: 0.3658\n",
      "Epoch 737/1024, Loss: 0.3620\n",
      "Epoch 741/1024, Loss: 0.3651\n",
      "Epoch 745/1024, Loss: 0.3665\n",
      "Epoch 749/1024, Loss: 0.3615\n",
      "Epoch 753/1024, Loss: 0.3583\n",
      "Epoch 757/1024, Loss: 0.3591\n",
      "Epoch 761/1024, Loss: 0.3574\n",
      "Epoch 765/1024, Loss: 0.3660\n",
      "Epoch 769/1024, Loss: 0.3568\n",
      "Epoch 773/1024, Loss: 0.3599\n",
      "Epoch 777/1024, Loss: 0.3557\n",
      "Epoch 781/1024, Loss: 0.3540\n",
      "Epoch 785/1024, Loss: 0.3532\n",
      "Epoch 789/1024, Loss: 0.3558\n",
      "Epoch 793/1024, Loss: 0.3537\n",
      "Epoch 797/1024, Loss: 0.3534\n",
      "Epoch 801/1024, Loss: 0.3527\n",
      "Epoch 805/1024, Loss: 0.3551\n",
      "Epoch 809/1024, Loss: 0.3509\n",
      "Epoch 813/1024, Loss: 0.3530\n",
      "Epoch 817/1024, Loss: 0.3537\n",
      "Epoch 821/1024, Loss: 0.3569\n",
      "Epoch 825/1024, Loss: 0.3578\n",
      "Epoch 829/1024, Loss: 0.3510\n",
      "Epoch 833/1024, Loss: 0.3510\n",
      "Epoch 837/1024, Loss: 0.3522\n",
      "Epoch 841/1024, Loss: 0.3475\n",
      "Epoch 845/1024, Loss: 0.3490\n",
      "Epoch 849/1024, Loss: 0.3497\n",
      "Epoch 853/1024, Loss: 0.3512\n",
      "Epoch 857/1024, Loss: 0.3463\n",
      "Epoch 861/1024, Loss: 0.3469\n",
      "Epoch 865/1024, Loss: 0.3503\n",
      "Epoch 869/1024, Loss: 0.3454\n",
      "Epoch 873/1024, Loss: 0.3465\n",
      "Epoch 877/1024, Loss: 0.3443\n",
      "Epoch 881/1024, Loss: 0.3464\n",
      "Epoch 885/1024, Loss: 0.3486\n",
      "Epoch 889/1024, Loss: 0.3433\n",
      "Epoch 893/1024, Loss: 0.3457\n",
      "Epoch 897/1024, Loss: 0.3454\n",
      "Epoch 901/1024, Loss: 0.3435\n",
      "Epoch 905/1024, Loss: 0.3459\n",
      "Epoch 909/1024, Loss: 0.3463\n",
      "Epoch 913/1024, Loss: 0.3427\n",
      "Epoch 917/1024, Loss: 0.3391\n",
      "Epoch 921/1024, Loss: 0.3454\n",
      "Epoch 925/1024, Loss: 0.3444\n",
      "Epoch 929/1024, Loss: 0.3417\n",
      "Epoch 933/1024, Loss: 0.3426\n",
      "Epoch 937/1024, Loss: 0.3371\n",
      "Epoch 941/1024, Loss: 0.3384\n",
      "Epoch 945/1024, Loss: 0.3405\n",
      "Epoch 949/1024, Loss: 0.3374\n",
      "Epoch 953/1024, Loss: 0.3392\n",
      "Epoch 957/1024, Loss: 0.3403\n",
      "Epoch 961/1024, Loss: 0.3364\n",
      "Epoch 965/1024, Loss: 0.3412\n",
      "Epoch 969/1024, Loss: 0.3378\n",
      "Epoch 973/1024, Loss: 0.3389\n",
      "Epoch 977/1024, Loss: 0.3349\n",
      "Epoch 981/1024, Loss: 0.3366\n",
      "Epoch 985/1024, Loss: 0.3356\n",
      "Epoch 989/1024, Loss: 0.3344\n",
      "Epoch 993/1024, Loss: 0.3364\n",
      "Epoch 997/1024, Loss: 0.3356\n",
      "Epoch 1001/1024, Loss: 0.3425\n",
      "Epoch 1005/1024, Loss: 0.3323\n",
      "Epoch 1009/1024, Loss: 0.3408\n",
      "Epoch 1013/1024, Loss: 0.3324\n",
      "Epoch 1017/1024, Loss: 0.3343\n",
      "Epoch 1021/1024, Loss: 0.3331\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:08:55.084297Z",
     "start_time": "2025-10-21T05:07:36.877122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-256\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "e87d4a5466e02ab6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74703c8577d04bfbaac49c00b28f6d6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024, Loss: 2.2860\n",
      "Epoch 5/1024, Loss: 1.2829\n",
      "Epoch 9/1024, Loss: 1.0390\n",
      "Epoch 13/1024, Loss: 0.9261\n",
      "Epoch 17/1024, Loss: 0.8586\n",
      "Epoch 21/1024, Loss: 0.8149\n",
      "Epoch 25/1024, Loss: 0.7776\n",
      "Epoch 29/1024, Loss: 0.7532\n",
      "Epoch 33/1024, Loss: 0.7280\n",
      "Epoch 37/1024, Loss: 0.7100\n",
      "Epoch 41/1024, Loss: 0.6954\n",
      "Epoch 45/1024, Loss: 0.6760\n",
      "Epoch 49/1024, Loss: 0.6613\n",
      "Epoch 53/1024, Loss: 0.6526\n",
      "Epoch 57/1024, Loss: 0.6410\n",
      "Epoch 61/1024, Loss: 0.6360\n",
      "Epoch 65/1024, Loss: 0.6271\n",
      "Epoch 69/1024, Loss: 0.6215\n",
      "Epoch 73/1024, Loss: 0.6145\n",
      "Epoch 77/1024, Loss: 0.6046\n",
      "Epoch 81/1024, Loss: 0.5948\n",
      "Epoch 85/1024, Loss: 0.5984\n",
      "Epoch 89/1024, Loss: 0.5848\n",
      "Epoch 93/1024, Loss: 0.5782\n",
      "Epoch 97/1024, Loss: 0.5722\n",
      "Epoch 101/1024, Loss: 0.5665\n",
      "Epoch 105/1024, Loss: 0.5638\n",
      "Epoch 109/1024, Loss: 0.5672\n",
      "Epoch 113/1024, Loss: 0.5578\n",
      "Epoch 117/1024, Loss: 0.5498\n",
      "Epoch 121/1024, Loss: 0.5486\n",
      "Epoch 125/1024, Loss: 0.5394\n",
      "Epoch 129/1024, Loss: 0.5358\n",
      "Epoch 133/1024, Loss: 0.5382\n",
      "Epoch 137/1024, Loss: 0.5355\n",
      "Epoch 141/1024, Loss: 0.5279\n",
      "Epoch 145/1024, Loss: 0.5244\n",
      "Epoch 149/1024, Loss: 0.5208\n",
      "Epoch 153/1024, Loss: 0.5221\n",
      "Epoch 157/1024, Loss: 0.5124\n",
      "Epoch 161/1024, Loss: 0.5115\n",
      "Epoch 165/1024, Loss: 0.5139\n",
      "Epoch 169/1024, Loss: 0.5081\n",
      "Epoch 173/1024, Loss: 0.5086\n",
      "Epoch 177/1024, Loss: 0.5032\n",
      "Epoch 181/1024, Loss: 0.5013\n",
      "Epoch 185/1024, Loss: 0.4959\n",
      "Epoch 189/1024, Loss: 0.4944\n",
      "Epoch 193/1024, Loss: 0.4962\n",
      "Epoch 197/1024, Loss: 0.4906\n",
      "Epoch 201/1024, Loss: 0.4966\n",
      "Epoch 205/1024, Loss: 0.4911\n",
      "Epoch 209/1024, Loss: 0.4872\n",
      "Epoch 213/1024, Loss: 0.4865\n",
      "Epoch 217/1024, Loss: 0.4830\n",
      "Epoch 221/1024, Loss: 0.4804\n",
      "Epoch 225/1024, Loss: 0.4791\n",
      "Epoch 229/1024, Loss: 0.4764\n",
      "Epoch 233/1024, Loss: 0.4744\n",
      "Epoch 237/1024, Loss: 0.4769\n",
      "Epoch 241/1024, Loss: 0.4817\n",
      "Epoch 245/1024, Loss: 0.4701\n",
      "Epoch 249/1024, Loss: 0.4648\n",
      "Epoch 253/1024, Loss: 0.4611\n",
      "Epoch 257/1024, Loss: 0.4632\n",
      "Epoch 261/1024, Loss: 0.4636\n",
      "Epoch 265/1024, Loss: 0.4627\n",
      "Epoch 269/1024, Loss: 0.4623\n",
      "Epoch 273/1024, Loss: 0.4598\n",
      "Epoch 277/1024, Loss: 0.4574\n",
      "Epoch 281/1024, Loss: 0.4604\n",
      "Epoch 285/1024, Loss: 0.4530\n",
      "Epoch 289/1024, Loss: 0.4525\n",
      "Epoch 293/1024, Loss: 0.4528\n",
      "Epoch 297/1024, Loss: 0.4434\n",
      "Epoch 301/1024, Loss: 0.4573\n",
      "Epoch 305/1024, Loss: 0.4508\n",
      "Epoch 309/1024, Loss: 0.4482\n",
      "Epoch 313/1024, Loss: 0.4455\n",
      "Epoch 317/1024, Loss: 0.4405\n",
      "Epoch 321/1024, Loss: 0.4391\n",
      "Epoch 325/1024, Loss: 0.4449\n",
      "Epoch 329/1024, Loss: 0.4427\n",
      "Epoch 333/1024, Loss: 0.4333\n",
      "Epoch 337/1024, Loss: 0.4355\n",
      "Epoch 341/1024, Loss: 0.4359\n",
      "Epoch 345/1024, Loss: 0.4423\n",
      "Epoch 349/1024, Loss: 0.4293\n",
      "Epoch 353/1024, Loss: 0.4337\n",
      "Epoch 357/1024, Loss: 0.4327\n",
      "Epoch 361/1024, Loss: 0.4324\n",
      "Epoch 365/1024, Loss: 0.4282\n",
      "Epoch 369/1024, Loss: 0.4322\n",
      "Epoch 373/1024, Loss: 0.4305\n",
      "Epoch 377/1024, Loss: 0.4266\n",
      "Epoch 381/1024, Loss: 0.4242\n",
      "Epoch 385/1024, Loss: 0.4244\n",
      "Epoch 389/1024, Loss: 0.4258\n",
      "Epoch 393/1024, Loss: 0.4190\n",
      "Epoch 397/1024, Loss: 0.4207\n",
      "Epoch 401/1024, Loss: 0.4192\n",
      "Epoch 405/1024, Loss: 0.4224\n",
      "Epoch 409/1024, Loss: 0.4215\n",
      "Epoch 413/1024, Loss: 0.4244\n",
      "Epoch 417/1024, Loss: 0.4191\n",
      "Epoch 421/1024, Loss: 0.4158\n",
      "Epoch 425/1024, Loss: 0.4161\n",
      "Epoch 429/1024, Loss: 0.4138\n",
      "Epoch 433/1024, Loss: 0.4109\n",
      "Epoch 437/1024, Loss: 0.4169\n",
      "Epoch 441/1024, Loss: 0.4120\n",
      "Epoch 445/1024, Loss: 0.4115\n",
      "Epoch 449/1024, Loss: 0.4150\n",
      "Epoch 453/1024, Loss: 0.4058\n",
      "Epoch 457/1024, Loss: 0.4149\n",
      "Epoch 461/1024, Loss: 0.4094\n",
      "Epoch 465/1024, Loss: 0.4071\n",
      "Epoch 469/1024, Loss: 0.4105\n",
      "Epoch 473/1024, Loss: 0.4042\n",
      "Epoch 477/1024, Loss: 0.4058\n",
      "Epoch 481/1024, Loss: 0.3989\n",
      "Epoch 485/1024, Loss: 0.4057\n",
      "Epoch 489/1024, Loss: 0.4046\n",
      "Epoch 493/1024, Loss: 0.4016\n",
      "Epoch 497/1024, Loss: 0.3987\n",
      "Epoch 501/1024, Loss: 0.3980\n",
      "Epoch 505/1024, Loss: 0.4000\n",
      "Epoch 509/1024, Loss: 0.3971\n",
      "Epoch 513/1024, Loss: 0.3969\n",
      "Epoch 517/1024, Loss: 0.3929\n",
      "Epoch 521/1024, Loss: 0.3978\n",
      "Epoch 525/1024, Loss: 0.3992\n",
      "Epoch 529/1024, Loss: 0.3988\n",
      "Epoch 533/1024, Loss: 0.3911\n",
      "Epoch 537/1024, Loss: 0.3922\n",
      "Epoch 541/1024, Loss: 0.3934\n",
      "Epoch 545/1024, Loss: 0.3961\n",
      "Epoch 549/1024, Loss: 0.3941\n",
      "Epoch 553/1024, Loss: 0.3941\n",
      "Epoch 557/1024, Loss: 0.3926\n",
      "Epoch 561/1024, Loss: 0.3957\n",
      "Epoch 565/1024, Loss: 0.3901\n",
      "Epoch 569/1024, Loss: 0.3975\n",
      "Epoch 573/1024, Loss: 0.3918\n",
      "Epoch 577/1024, Loss: 0.3828\n",
      "Epoch 581/1024, Loss: 0.3869\n",
      "Epoch 585/1024, Loss: 0.3875\n",
      "Epoch 589/1024, Loss: 0.3922\n",
      "Epoch 593/1024, Loss: 0.3882\n",
      "Epoch 597/1024, Loss: 0.3854\n",
      "Epoch 601/1024, Loss: 0.3829\n",
      "Epoch 605/1024, Loss: 0.3881\n",
      "Epoch 609/1024, Loss: 0.3865\n",
      "Epoch 613/1024, Loss: 0.3792\n",
      "Epoch 617/1024, Loss: 0.3781\n",
      "Epoch 621/1024, Loss: 0.3851\n",
      "Epoch 625/1024, Loss: 0.3790\n",
      "Epoch 629/1024, Loss: 0.3837\n",
      "Epoch 633/1024, Loss: 0.3844\n",
      "Epoch 637/1024, Loss: 0.3785\n",
      "Epoch 641/1024, Loss: 0.3776\n",
      "Epoch 645/1024, Loss: 0.3808\n",
      "Epoch 649/1024, Loss: 0.3829\n",
      "Epoch 653/1024, Loss: 0.3730\n",
      "Epoch 657/1024, Loss: 0.3706\n",
      "Epoch 661/1024, Loss: 0.3759\n",
      "Epoch 665/1024, Loss: 0.3749\n",
      "Epoch 669/1024, Loss: 0.3729\n",
      "Epoch 673/1024, Loss: 0.3748\n",
      "Epoch 677/1024, Loss: 0.3692\n",
      "Epoch 681/1024, Loss: 0.3728\n",
      "Epoch 685/1024, Loss: 0.3705\n",
      "Epoch 689/1024, Loss: 0.3675\n",
      "Epoch 693/1024, Loss: 0.3688\n",
      "Epoch 697/1024, Loss: 0.3675\n",
      "Epoch 701/1024, Loss: 0.3752\n",
      "Epoch 705/1024, Loss: 0.3668\n",
      "Epoch 709/1024, Loss: 0.3733\n",
      "Epoch 713/1024, Loss: 0.3635\n",
      "Epoch 717/1024, Loss: 0.3714\n",
      "Epoch 721/1024, Loss: 0.3663\n",
      "Epoch 725/1024, Loss: 0.3692\n",
      "Epoch 729/1024, Loss: 0.3651\n",
      "Epoch 733/1024, Loss: 0.3688\n",
      "Epoch 737/1024, Loss: 0.3651\n",
      "Epoch 741/1024, Loss: 0.3622\n",
      "Epoch 745/1024, Loss: 0.3657\n",
      "Epoch 749/1024, Loss: 0.3647\n",
      "Epoch 753/1024, Loss: 0.3639\n",
      "Epoch 757/1024, Loss: 0.3684\n",
      "Epoch 761/1024, Loss: 0.3626\n",
      "Epoch 765/1024, Loss: 0.3634\n",
      "Epoch 769/1024, Loss: 0.3624\n",
      "Epoch 773/1024, Loss: 0.3563\n",
      "Epoch 777/1024, Loss: 0.3589\n",
      "Epoch 781/1024, Loss: 0.3582\n",
      "Epoch 785/1024, Loss: 0.3617\n",
      "Epoch 789/1024, Loss: 0.3590\n",
      "Epoch 793/1024, Loss: 0.3653\n",
      "Epoch 797/1024, Loss: 0.3613\n",
      "Epoch 801/1024, Loss: 0.3612\n",
      "Epoch 805/1024, Loss: 0.3572\n",
      "Epoch 809/1024, Loss: 0.3571\n",
      "Epoch 813/1024, Loss: 0.3575\n",
      "Epoch 817/1024, Loss: 0.3544\n",
      "Epoch 821/1024, Loss: 0.3624\n",
      "Epoch 825/1024, Loss: 0.3566\n",
      "Epoch 829/1024, Loss: 0.3563\n",
      "Epoch 833/1024, Loss: 0.3545\n",
      "Epoch 837/1024, Loss: 0.3533\n",
      "Epoch 841/1024, Loss: 0.3550\n",
      "Epoch 845/1024, Loss: 0.3608\n",
      "Epoch 849/1024, Loss: 0.3520\n",
      "Epoch 853/1024, Loss: 0.3588\n",
      "Epoch 857/1024, Loss: 0.3503\n",
      "Epoch 861/1024, Loss: 0.3497\n",
      "Epoch 865/1024, Loss: 0.3513\n",
      "Epoch 869/1024, Loss: 0.3495\n",
      "Epoch 873/1024, Loss: 0.3544\n",
      "Epoch 877/1024, Loss: 0.3487\n",
      "Epoch 881/1024, Loss: 0.3475\n",
      "Epoch 885/1024, Loss: 0.3521\n",
      "Epoch 889/1024, Loss: 0.3518\n",
      "Epoch 893/1024, Loss: 0.3497\n",
      "Epoch 897/1024, Loss: 0.3493\n",
      "Epoch 901/1024, Loss: 0.3441\n",
      "Epoch 905/1024, Loss: 0.3476\n",
      "Epoch 909/1024, Loss: 0.3554\n",
      "Epoch 913/1024, Loss: 0.3442\n",
      "Epoch 917/1024, Loss: 0.3432\n",
      "Epoch 921/1024, Loss: 0.3477\n",
      "Epoch 925/1024, Loss: 0.3411\n",
      "Epoch 929/1024, Loss: 0.3475\n",
      "Epoch 933/1024, Loss: 0.3410\n",
      "Epoch 937/1024, Loss: 0.3449\n",
      "Epoch 941/1024, Loss: 0.3426\n",
      "Epoch 945/1024, Loss: 0.3399\n",
      "Epoch 949/1024, Loss: 0.3461\n",
      "Epoch 953/1024, Loss: 0.3449\n",
      "Epoch 957/1024, Loss: 0.3496\n",
      "Epoch 961/1024, Loss: 0.3384\n",
      "Epoch 965/1024, Loss: 0.3445\n",
      "Epoch 969/1024, Loss: 0.3448\n",
      "Epoch 973/1024, Loss: 0.3438\n",
      "Epoch 977/1024, Loss: 0.3451\n",
      "Epoch 981/1024, Loss: 0.3393\n",
      "Epoch 985/1024, Loss: 0.3385\n",
      "Epoch 989/1024, Loss: 0.3357\n",
      "Epoch 993/1024, Loss: 0.3406\n",
      "Epoch 997/1024, Loss: 0.3389\n",
      "Epoch 1001/1024, Loss: 0.3376\n",
      "Epoch 1005/1024, Loss: 0.3363\n",
      "Epoch 1009/1024, Loss: 0.3349\n",
      "Epoch 1013/1024, Loss: 0.3389\n",
      "Epoch 1017/1024, Loss: 0.3345\n",
      "Epoch 1021/1024, Loss: 0.3410\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:10:11.139237Z",
     "start_time": "2025-10-21T05:08:55.104569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-512\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "c7ae6d0abaaf8504",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3da972688a364981b9fa159ef70750ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024, Loss: 2.2402\n",
      "Epoch 5/1024, Loss: 1.2749\n",
      "Epoch 9/1024, Loss: 1.0442\n",
      "Epoch 13/1024, Loss: 0.9363\n",
      "Epoch 17/1024, Loss: 0.8684\n",
      "Epoch 21/1024, Loss: 0.8219\n",
      "Epoch 25/1024, Loss: 0.7851\n",
      "Epoch 29/1024, Loss: 0.7574\n",
      "Epoch 33/1024, Loss: 0.7348\n",
      "Epoch 37/1024, Loss: 0.7146\n",
      "Epoch 41/1024, Loss: 0.6967\n",
      "Epoch 45/1024, Loss: 0.6835\n",
      "Epoch 49/1024, Loss: 0.6730\n",
      "Epoch 53/1024, Loss: 0.6658\n",
      "Epoch 57/1024, Loss: 0.6509\n",
      "Epoch 61/1024, Loss: 0.6430\n",
      "Epoch 65/1024, Loss: 0.6310\n",
      "Epoch 69/1024, Loss: 0.6327\n",
      "Epoch 73/1024, Loss: 0.6142\n",
      "Epoch 77/1024, Loss: 0.6048\n",
      "Epoch 81/1024, Loss: 0.6017\n",
      "Epoch 85/1024, Loss: 0.5937\n",
      "Epoch 89/1024, Loss: 0.5899\n",
      "Epoch 93/1024, Loss: 0.5919\n",
      "Epoch 97/1024, Loss: 0.5795\n",
      "Epoch 101/1024, Loss: 0.5794\n",
      "Epoch 105/1024, Loss: 0.5681\n",
      "Epoch 109/1024, Loss: 0.5697\n",
      "Epoch 113/1024, Loss: 0.5621\n",
      "Epoch 117/1024, Loss: 0.5623\n",
      "Epoch 121/1024, Loss: 0.5538\n",
      "Epoch 125/1024, Loss: 0.5537\n",
      "Epoch 129/1024, Loss: 0.5423\n",
      "Epoch 133/1024, Loss: 0.5407\n",
      "Epoch 137/1024, Loss: 0.5401\n",
      "Epoch 141/1024, Loss: 0.5392\n",
      "Epoch 145/1024, Loss: 0.5385\n",
      "Epoch 149/1024, Loss: 0.5362\n",
      "Epoch 153/1024, Loss: 0.5271\n",
      "Epoch 157/1024, Loss: 0.5280\n",
      "Epoch 161/1024, Loss: 0.5234\n",
      "Epoch 165/1024, Loss: 0.5228\n",
      "Epoch 169/1024, Loss: 0.5209\n",
      "Epoch 173/1024, Loss: 0.5182\n",
      "Epoch 177/1024, Loss: 0.5105\n",
      "Epoch 181/1024, Loss: 0.5080\n",
      "Epoch 185/1024, Loss: 0.5105\n",
      "Epoch 189/1024, Loss: 0.5018\n",
      "Epoch 193/1024, Loss: 0.4987\n",
      "Epoch 197/1024, Loss: 0.4993\n",
      "Epoch 201/1024, Loss: 0.4974\n",
      "Epoch 205/1024, Loss: 0.4967\n",
      "Epoch 209/1024, Loss: 0.4948\n",
      "Epoch 213/1024, Loss: 0.5021\n",
      "Epoch 217/1024, Loss: 0.4938\n",
      "Epoch 221/1024, Loss: 0.4903\n",
      "Epoch 225/1024, Loss: 0.4865\n",
      "Epoch 229/1024, Loss: 0.4875\n",
      "Epoch 233/1024, Loss: 0.4864\n",
      "Epoch 237/1024, Loss: 0.4821\n",
      "Epoch 241/1024, Loss: 0.4786\n",
      "Epoch 245/1024, Loss: 0.4778\n",
      "Epoch 249/1024, Loss: 0.4797\n",
      "Epoch 253/1024, Loss: 0.4774\n",
      "Epoch 257/1024, Loss: 0.4725\n",
      "Epoch 261/1024, Loss: 0.4731\n",
      "Epoch 265/1024, Loss: 0.4694\n",
      "Epoch 269/1024, Loss: 0.4709\n",
      "Epoch 273/1024, Loss: 0.4684\n",
      "Epoch 277/1024, Loss: 0.4645\n",
      "Epoch 281/1024, Loss: 0.4587\n",
      "Epoch 285/1024, Loss: 0.4628\n",
      "Epoch 289/1024, Loss: 0.4588\n",
      "Epoch 293/1024, Loss: 0.4617\n",
      "Epoch 297/1024, Loss: 0.4608\n",
      "Epoch 301/1024, Loss: 0.4563\n",
      "Epoch 305/1024, Loss: 0.4539\n",
      "Epoch 309/1024, Loss: 0.4563\n",
      "Epoch 313/1024, Loss: 0.4569\n",
      "Epoch 317/1024, Loss: 0.4476\n",
      "Epoch 321/1024, Loss: 0.4518\n",
      "Epoch 325/1024, Loss: 0.4473\n",
      "Epoch 329/1024, Loss: 0.4482\n",
      "Epoch 333/1024, Loss: 0.4491\n",
      "Epoch 337/1024, Loss: 0.4461\n",
      "Epoch 341/1024, Loss: 0.4472\n",
      "Epoch 345/1024, Loss: 0.4412\n",
      "Epoch 349/1024, Loss: 0.4446\n",
      "Epoch 353/1024, Loss: 0.4419\n",
      "Epoch 357/1024, Loss: 0.4383\n",
      "Epoch 361/1024, Loss: 0.4393\n",
      "Epoch 365/1024, Loss: 0.4367\n",
      "Epoch 369/1024, Loss: 0.4375\n",
      "Epoch 373/1024, Loss: 0.4395\n",
      "Epoch 377/1024, Loss: 0.4414\n",
      "Epoch 381/1024, Loss: 0.4326\n",
      "Epoch 385/1024, Loss: 0.4329\n",
      "Epoch 389/1024, Loss: 0.4319\n",
      "Epoch 393/1024, Loss: 0.4296\n",
      "Epoch 397/1024, Loss: 0.4300\n",
      "Epoch 401/1024, Loss: 0.4286\n",
      "Epoch 405/1024, Loss: 0.4302\n",
      "Epoch 409/1024, Loss: 0.4267\n",
      "Epoch 413/1024, Loss: 0.4277\n",
      "Epoch 417/1024, Loss: 0.4271\n",
      "Epoch 421/1024, Loss: 0.4284\n",
      "Epoch 425/1024, Loss: 0.4254\n",
      "Epoch 429/1024, Loss: 0.4227\n",
      "Epoch 433/1024, Loss: 0.4252\n",
      "Epoch 437/1024, Loss: 0.4187\n",
      "Epoch 441/1024, Loss: 0.4236\n",
      "Epoch 445/1024, Loss: 0.4186\n",
      "Epoch 449/1024, Loss: 0.4206\n",
      "Epoch 453/1024, Loss: 0.4111\n",
      "Epoch 457/1024, Loss: 0.4156\n",
      "Epoch 461/1024, Loss: 0.4126\n",
      "Epoch 465/1024, Loss: 0.4185\n",
      "Epoch 469/1024, Loss: 0.4160\n",
      "Epoch 473/1024, Loss: 0.4128\n",
      "Epoch 477/1024, Loss: 0.4131\n",
      "Epoch 481/1024, Loss: 0.4134\n",
      "Epoch 485/1024, Loss: 0.4142\n",
      "Epoch 489/1024, Loss: 0.4126\n",
      "Epoch 493/1024, Loss: 0.4126\n",
      "Epoch 497/1024, Loss: 0.4109\n",
      "Epoch 501/1024, Loss: 0.4102\n",
      "Epoch 505/1024, Loss: 0.4084\n",
      "Epoch 509/1024, Loss: 0.4095\n",
      "Epoch 513/1024, Loss: 0.4058\n",
      "Epoch 517/1024, Loss: 0.3996\n",
      "Epoch 521/1024, Loss: 0.4094\n",
      "Epoch 525/1024, Loss: 0.4045\n",
      "Epoch 529/1024, Loss: 0.4048\n",
      "Epoch 533/1024, Loss: 0.4014\n",
      "Epoch 537/1024, Loss: 0.4022\n",
      "Epoch 541/1024, Loss: 0.4041\n",
      "Epoch 545/1024, Loss: 0.4029\n",
      "Epoch 549/1024, Loss: 0.3936\n",
      "Epoch 553/1024, Loss: 0.4004\n",
      "Epoch 557/1024, Loss: 0.3965\n",
      "Epoch 561/1024, Loss: 0.3992\n",
      "Epoch 565/1024, Loss: 0.3956\n",
      "Epoch 569/1024, Loss: 0.3983\n",
      "Epoch 573/1024, Loss: 0.3942\n",
      "Epoch 577/1024, Loss: 0.3921\n",
      "Epoch 581/1024, Loss: 0.3916\n",
      "Epoch 585/1024, Loss: 0.3902\n",
      "Epoch 589/1024, Loss: 0.3912\n",
      "Epoch 593/1024, Loss: 0.3898\n",
      "Epoch 597/1024, Loss: 0.3894\n",
      "Epoch 601/1024, Loss: 0.3948\n",
      "Epoch 605/1024, Loss: 0.3958\n",
      "Epoch 609/1024, Loss: 0.3934\n",
      "Epoch 613/1024, Loss: 0.3888\n",
      "Epoch 617/1024, Loss: 0.3860\n",
      "Epoch 621/1024, Loss: 0.3863\n",
      "Epoch 625/1024, Loss: 0.3919\n",
      "Epoch 629/1024, Loss: 0.3844\n",
      "Epoch 633/1024, Loss: 0.3870\n",
      "Epoch 637/1024, Loss: 0.3880\n",
      "Epoch 641/1024, Loss: 0.3857\n",
      "Epoch 645/1024, Loss: 0.3815\n",
      "Epoch 649/1024, Loss: 0.3908\n",
      "Epoch 653/1024, Loss: 0.3828\n",
      "Epoch 657/1024, Loss: 0.3837\n",
      "Epoch 661/1024, Loss: 0.3864\n",
      "Epoch 665/1024, Loss: 0.3820\n",
      "Epoch 669/1024, Loss: 0.3799\n",
      "Epoch 673/1024, Loss: 0.3856\n",
      "Epoch 677/1024, Loss: 0.3853\n",
      "Epoch 681/1024, Loss: 0.3822\n",
      "Epoch 685/1024, Loss: 0.3797\n",
      "Epoch 689/1024, Loss: 0.3765\n",
      "Epoch 693/1024, Loss: 0.3755\n",
      "Epoch 697/1024, Loss: 0.3754\n",
      "Epoch 701/1024, Loss: 0.3739\n",
      "Epoch 705/1024, Loss: 0.3740\n",
      "Epoch 709/1024, Loss: 0.3807\n",
      "Epoch 713/1024, Loss: 0.3801\n",
      "Epoch 717/1024, Loss: 0.3719\n",
      "Epoch 721/1024, Loss: 0.3745\n",
      "Epoch 725/1024, Loss: 0.3733\n",
      "Epoch 729/1024, Loss: 0.3728\n",
      "Epoch 733/1024, Loss: 0.3706\n",
      "Epoch 737/1024, Loss: 0.3687\n",
      "Epoch 741/1024, Loss: 0.3712\n",
      "Epoch 745/1024, Loss: 0.3708\n",
      "Epoch 749/1024, Loss: 0.3715\n",
      "Epoch 753/1024, Loss: 0.3704\n",
      "Epoch 757/1024, Loss: 0.3769\n",
      "Epoch 761/1024, Loss: 0.3681\n",
      "Epoch 765/1024, Loss: 0.3675\n",
      "Epoch 769/1024, Loss: 0.3706\n",
      "Epoch 773/1024, Loss: 0.3705\n",
      "Epoch 777/1024, Loss: 0.3702\n",
      "Epoch 781/1024, Loss: 0.3630\n",
      "Epoch 785/1024, Loss: 0.3687\n",
      "Epoch 789/1024, Loss: 0.3658\n",
      "Epoch 793/1024, Loss: 0.3646\n",
      "Epoch 797/1024, Loss: 0.3688\n",
      "Epoch 801/1024, Loss: 0.3632\n",
      "Epoch 805/1024, Loss: 0.3632\n",
      "Epoch 809/1024, Loss: 0.3658\n",
      "Epoch 813/1024, Loss: 0.3659\n",
      "Epoch 817/1024, Loss: 0.3657\n",
      "Epoch 821/1024, Loss: 0.3615\n",
      "Epoch 825/1024, Loss: 0.3635\n",
      "Epoch 829/1024, Loss: 0.3607\n",
      "Epoch 833/1024, Loss: 0.3603\n",
      "Epoch 837/1024, Loss: 0.3667\n",
      "Epoch 841/1024, Loss: 0.3698\n",
      "Epoch 845/1024, Loss: 0.3651\n",
      "Epoch 849/1024, Loss: 0.3557\n",
      "Epoch 853/1024, Loss: 0.3582\n",
      "Epoch 857/1024, Loss: 0.3600\n",
      "Epoch 861/1024, Loss: 0.3530\n",
      "Epoch 865/1024, Loss: 0.3575\n",
      "Epoch 869/1024, Loss: 0.3556\n",
      "Epoch 873/1024, Loss: 0.3607\n",
      "Epoch 877/1024, Loss: 0.3581\n",
      "Epoch 881/1024, Loss: 0.3606\n",
      "Epoch 885/1024, Loss: 0.3625\n",
      "Epoch 889/1024, Loss: 0.3560\n",
      "Epoch 893/1024, Loss: 0.3564\n",
      "Epoch 897/1024, Loss: 0.3543\n",
      "Epoch 901/1024, Loss: 0.3519\n",
      "Epoch 905/1024, Loss: 0.3528\n",
      "Epoch 909/1024, Loss: 0.3549\n",
      "Epoch 913/1024, Loss: 0.3517\n",
      "Epoch 917/1024, Loss: 0.3517\n",
      "Epoch 921/1024, Loss: 0.3535\n",
      "Epoch 925/1024, Loss: 0.3581\n",
      "Epoch 929/1024, Loss: 0.3536\n",
      "Epoch 933/1024, Loss: 0.3496\n",
      "Epoch 937/1024, Loss: 0.3510\n",
      "Epoch 941/1024, Loss: 0.3505\n",
      "Epoch 945/1024, Loss: 0.3471\n",
      "Epoch 949/1024, Loss: 0.3498\n",
      "Epoch 953/1024, Loss: 0.3477\n",
      "Epoch 957/1024, Loss: 0.3473\n",
      "Epoch 961/1024, Loss: 0.3456\n",
      "Epoch 965/1024, Loss: 0.3454\n",
      "Epoch 969/1024, Loss: 0.3442\n",
      "Epoch 973/1024, Loss: 0.3443\n",
      "Epoch 977/1024, Loss: 0.3447\n",
      "Epoch 981/1024, Loss: 0.3477\n",
      "Epoch 985/1024, Loss: 0.3435\n",
      "Epoch 989/1024, Loss: 0.3466\n",
      "Epoch 993/1024, Loss: 0.3440\n",
      "Epoch 997/1024, Loss: 0.3473\n",
      "Epoch 1001/1024, Loss: 0.3456\n",
      "Epoch 1005/1024, Loss: 0.3463\n",
      "Epoch 1009/1024, Loss: 0.3446\n",
      "Epoch 1013/1024, Loss: 0.3480\n",
      "Epoch 1017/1024, Loss: 0.3483\n",
      "Epoch 1021/1024, Loss: 0.3497\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:11:25.484512Z",
     "start_time": "2025-10-21T05:10:11.213596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-1024\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "32c79c97700354d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53e7e4a8660b458fb39477a477d0a8b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024, Loss: 2.1461\n",
      "Epoch 5/1024, Loss: 1.2341\n",
      "Epoch 9/1024, Loss: 1.0258\n",
      "Epoch 13/1024, Loss: 0.9360\n",
      "Epoch 17/1024, Loss: 0.8617\n",
      "Epoch 21/1024, Loss: 0.8203\n",
      "Epoch 25/1024, Loss: 0.7847\n",
      "Epoch 29/1024, Loss: 0.7650\n",
      "Epoch 33/1024, Loss: 0.7402\n",
      "Epoch 37/1024, Loss: 0.7158\n",
      "Epoch 41/1024, Loss: 0.7127\n",
      "Epoch 45/1024, Loss: 0.6901\n",
      "Epoch 49/1024, Loss: 0.6843\n",
      "Epoch 53/1024, Loss: 0.6731\n",
      "Epoch 57/1024, Loss: 0.6580\n",
      "Epoch 61/1024, Loss: 0.6570\n",
      "Epoch 65/1024, Loss: 0.6560\n",
      "Epoch 69/1024, Loss: 0.6395\n",
      "Epoch 73/1024, Loss: 0.6324\n",
      "Epoch 77/1024, Loss: 0.6278\n",
      "Epoch 81/1024, Loss: 0.6196\n",
      "Epoch 85/1024, Loss: 0.6147\n",
      "Epoch 89/1024, Loss: 0.6053\n",
      "Epoch 93/1024, Loss: 0.6089\n",
      "Epoch 97/1024, Loss: 0.6067\n",
      "Epoch 101/1024, Loss: 0.5930\n",
      "Epoch 105/1024, Loss: 0.5835\n",
      "Epoch 109/1024, Loss: 0.5875\n",
      "Epoch 113/1024, Loss: 0.5751\n",
      "Epoch 117/1024, Loss: 0.5775\n",
      "Epoch 121/1024, Loss: 0.5727\n",
      "Epoch 125/1024, Loss: 0.5671\n",
      "Epoch 129/1024, Loss: 0.5628\n",
      "Epoch 133/1024, Loss: 0.5617\n",
      "Epoch 137/1024, Loss: 0.5576\n",
      "Epoch 141/1024, Loss: 0.5521\n",
      "Epoch 145/1024, Loss: 0.5514\n",
      "Epoch 149/1024, Loss: 0.5500\n",
      "Epoch 153/1024, Loss: 0.5522\n",
      "Epoch 157/1024, Loss: 0.5454\n",
      "Epoch 161/1024, Loss: 0.5389\n",
      "Epoch 165/1024, Loss: 0.5365\n",
      "Epoch 169/1024, Loss: 0.5338\n",
      "Epoch 173/1024, Loss: 0.5325\n",
      "Epoch 177/1024, Loss: 0.5316\n",
      "Epoch 181/1024, Loss: 0.5270\n",
      "Epoch 185/1024, Loss: 0.5258\n",
      "Epoch 189/1024, Loss: 0.5264\n",
      "Epoch 193/1024, Loss: 0.5189\n",
      "Epoch 197/1024, Loss: 0.5198\n",
      "Epoch 201/1024, Loss: 0.5147\n",
      "Epoch 205/1024, Loss: 0.5193\n",
      "Epoch 209/1024, Loss: 0.5128\n",
      "Epoch 213/1024, Loss: 0.5122\n",
      "Epoch 217/1024, Loss: 0.5077\n",
      "Epoch 221/1024, Loss: 0.5040\n",
      "Epoch 225/1024, Loss: 0.5075\n",
      "Epoch 229/1024, Loss: 0.5092\n",
      "Epoch 233/1024, Loss: 0.5037\n",
      "Epoch 237/1024, Loss: 0.4988\n",
      "Epoch 241/1024, Loss: 0.4977\n",
      "Epoch 245/1024, Loss: 0.4935\n",
      "Epoch 249/1024, Loss: 0.4961\n",
      "Epoch 253/1024, Loss: 0.4906\n",
      "Epoch 257/1024, Loss: 0.4890\n",
      "Epoch 261/1024, Loss: 0.4924\n",
      "Epoch 265/1024, Loss: 0.4890\n",
      "Epoch 269/1024, Loss: 0.4947\n",
      "Epoch 273/1024, Loss: 0.4834\n",
      "Epoch 277/1024, Loss: 0.4800\n",
      "Epoch 281/1024, Loss: 0.4851\n",
      "Epoch 285/1024, Loss: 0.4751\n",
      "Epoch 289/1024, Loss: 0.4844\n",
      "Epoch 293/1024, Loss: 0.4768\n",
      "Epoch 297/1024, Loss: 0.4735\n",
      "Epoch 301/1024, Loss: 0.4761\n",
      "Epoch 305/1024, Loss: 0.4777\n",
      "Epoch 309/1024, Loss: 0.4727\n",
      "Epoch 313/1024, Loss: 0.4735\n",
      "Epoch 317/1024, Loss: 0.4700\n",
      "Epoch 321/1024, Loss: 0.4641\n",
      "Epoch 325/1024, Loss: 0.4688\n",
      "Epoch 329/1024, Loss: 0.4693\n",
      "Epoch 333/1024, Loss: 0.4599\n",
      "Epoch 337/1024, Loss: 0.4588\n",
      "Epoch 341/1024, Loss: 0.4627\n",
      "Epoch 345/1024, Loss: 0.4635\n",
      "Epoch 349/1024, Loss: 0.4579\n",
      "Epoch 353/1024, Loss: 0.4613\n",
      "Epoch 357/1024, Loss: 0.4582\n",
      "Epoch 361/1024, Loss: 0.4569\n",
      "Epoch 365/1024, Loss: 0.4639\n",
      "Epoch 369/1024, Loss: 0.4517\n",
      "Epoch 373/1024, Loss: 0.4533\n",
      "Epoch 377/1024, Loss: 0.4465\n",
      "Epoch 381/1024, Loss: 0.4526\n",
      "Epoch 385/1024, Loss: 0.4451\n",
      "Epoch 389/1024, Loss: 0.4461\n",
      "Epoch 393/1024, Loss: 0.4545\n",
      "Epoch 397/1024, Loss: 0.4429\n",
      "Epoch 401/1024, Loss: 0.4456\n",
      "Epoch 405/1024, Loss: 0.4429\n",
      "Epoch 409/1024, Loss: 0.4442\n",
      "Epoch 413/1024, Loss: 0.4416\n",
      "Epoch 417/1024, Loss: 0.4378\n",
      "Epoch 421/1024, Loss: 0.4422\n",
      "Epoch 425/1024, Loss: 0.4341\n",
      "Epoch 429/1024, Loss: 0.4450\n",
      "Epoch 433/1024, Loss: 0.4430\n",
      "Epoch 437/1024, Loss: 0.4360\n",
      "Epoch 441/1024, Loss: 0.4389\n",
      "Epoch 445/1024, Loss: 0.4363\n",
      "Epoch 449/1024, Loss: 0.4329\n",
      "Epoch 453/1024, Loss: 0.4328\n",
      "Epoch 457/1024, Loss: 0.4334\n",
      "Epoch 461/1024, Loss: 0.4367\n",
      "Epoch 465/1024, Loss: 0.4335\n",
      "Epoch 469/1024, Loss: 0.4332\n",
      "Epoch 473/1024, Loss: 0.4328\n",
      "Epoch 477/1024, Loss: 0.4288\n",
      "Epoch 481/1024, Loss: 0.4305\n",
      "Epoch 485/1024, Loss: 0.4293\n",
      "Epoch 489/1024, Loss: 0.4246\n",
      "Epoch 493/1024, Loss: 0.4224\n",
      "Epoch 497/1024, Loss: 0.4263\n",
      "Epoch 501/1024, Loss: 0.4241\n",
      "Epoch 505/1024, Loss: 0.4231\n",
      "Epoch 509/1024, Loss: 0.4221\n",
      "Epoch 513/1024, Loss: 0.4173\n",
      "Epoch 517/1024, Loss: 0.4247\n",
      "Epoch 521/1024, Loss: 0.4199\n",
      "Epoch 525/1024, Loss: 0.4216\n",
      "Epoch 529/1024, Loss: 0.4201\n",
      "Epoch 533/1024, Loss: 0.4165\n",
      "Epoch 537/1024, Loss: 0.4143\n",
      "Epoch 541/1024, Loss: 0.4164\n",
      "Epoch 545/1024, Loss: 0.4147\n",
      "Epoch 549/1024, Loss: 0.4159\n",
      "Epoch 553/1024, Loss: 0.4190\n",
      "Epoch 557/1024, Loss: 0.4092\n",
      "Epoch 561/1024, Loss: 0.4128\n",
      "Epoch 565/1024, Loss: 0.4151\n",
      "Epoch 569/1024, Loss: 0.4152\n",
      "Epoch 573/1024, Loss: 0.4088\n",
      "Epoch 577/1024, Loss: 0.4104\n",
      "Epoch 581/1024, Loss: 0.4089\n",
      "Epoch 585/1024, Loss: 0.4147\n",
      "Epoch 589/1024, Loss: 0.4045\n",
      "Epoch 593/1024, Loss: 0.4044\n",
      "Epoch 597/1024, Loss: 0.4049\n",
      "Epoch 601/1024, Loss: 0.4087\n",
      "Epoch 605/1024, Loss: 0.4017\n",
      "Epoch 609/1024, Loss: 0.4093\n",
      "Epoch 613/1024, Loss: 0.4025\n",
      "Epoch 617/1024, Loss: 0.4016\n",
      "Epoch 621/1024, Loss: 0.3990\n",
      "Epoch 625/1024, Loss: 0.4031\n",
      "Epoch 629/1024, Loss: 0.3981\n",
      "Epoch 633/1024, Loss: 0.4016\n",
      "Epoch 637/1024, Loss: 0.3994\n",
      "Epoch 641/1024, Loss: 0.3986\n",
      "Epoch 645/1024, Loss: 0.3977\n",
      "Epoch 649/1024, Loss: 0.3994\n",
      "Epoch 653/1024, Loss: 0.3993\n",
      "Epoch 657/1024, Loss: 0.4037\n",
      "Epoch 661/1024, Loss: 0.3938\n",
      "Epoch 665/1024, Loss: 0.3974\n",
      "Epoch 669/1024, Loss: 0.3982\n",
      "Epoch 673/1024, Loss: 0.3935\n",
      "Epoch 677/1024, Loss: 0.3951\n",
      "Epoch 681/1024, Loss: 0.3961\n",
      "Epoch 685/1024, Loss: 0.3925\n",
      "Epoch 689/1024, Loss: 0.3919\n",
      "Epoch 693/1024, Loss: 0.3966\n",
      "Epoch 697/1024, Loss: 0.3891\n",
      "Epoch 701/1024, Loss: 0.3903\n",
      "Epoch 705/1024, Loss: 0.3902\n",
      "Epoch 709/1024, Loss: 0.3895\n",
      "Epoch 713/1024, Loss: 0.3889\n",
      "Epoch 717/1024, Loss: 0.3872\n",
      "Epoch 721/1024, Loss: 0.3898\n",
      "Epoch 725/1024, Loss: 0.3879\n",
      "Epoch 729/1024, Loss: 0.3894\n",
      "Epoch 733/1024, Loss: 0.3909\n",
      "Epoch 737/1024, Loss: 0.3821\n",
      "Epoch 741/1024, Loss: 0.3820\n",
      "Epoch 745/1024, Loss: 0.3800\n",
      "Epoch 749/1024, Loss: 0.3884\n",
      "Epoch 753/1024, Loss: 0.3847\n",
      "Epoch 757/1024, Loss: 0.3799\n",
      "Epoch 761/1024, Loss: 0.3854\n",
      "Epoch 765/1024, Loss: 0.3809\n",
      "Epoch 769/1024, Loss: 0.3845\n",
      "Epoch 773/1024, Loss: 0.3798\n",
      "Epoch 777/1024, Loss: 0.3798\n",
      "Epoch 781/1024, Loss: 0.3812\n",
      "Epoch 785/1024, Loss: 0.3765\n",
      "Epoch 789/1024, Loss: 0.3763\n",
      "Epoch 793/1024, Loss: 0.3809\n",
      "Epoch 797/1024, Loss: 0.3806\n",
      "Epoch 801/1024, Loss: 0.3795\n",
      "Epoch 805/1024, Loss: 0.3762\n",
      "Epoch 809/1024, Loss: 0.3754\n",
      "Epoch 813/1024, Loss: 0.3735\n",
      "Epoch 817/1024, Loss: 0.3780\n",
      "Epoch 821/1024, Loss: 0.3741\n",
      "Epoch 825/1024, Loss: 0.3751\n",
      "Epoch 829/1024, Loss: 0.3721\n",
      "Epoch 833/1024, Loss: 0.3766\n",
      "Epoch 837/1024, Loss: 0.3755\n",
      "Epoch 841/1024, Loss: 0.3697\n",
      "Epoch 845/1024, Loss: 0.3718\n",
      "Epoch 849/1024, Loss: 0.3726\n",
      "Epoch 853/1024, Loss: 0.3670\n",
      "Epoch 857/1024, Loss: 0.3733\n",
      "Epoch 861/1024, Loss: 0.3704\n",
      "Epoch 865/1024, Loss: 0.3723\n",
      "Epoch 869/1024, Loss: 0.3734\n",
      "Epoch 873/1024, Loss: 0.3691\n",
      "Epoch 877/1024, Loss: 0.3649\n",
      "Epoch 881/1024, Loss: 0.3654\n",
      "Epoch 885/1024, Loss: 0.3649\n",
      "Epoch 889/1024, Loss: 0.3641\n",
      "Epoch 893/1024, Loss: 0.3650\n",
      "Epoch 897/1024, Loss: 0.3635\n",
      "Epoch 901/1024, Loss: 0.3638\n",
      "Epoch 905/1024, Loss: 0.3687\n",
      "Epoch 909/1024, Loss: 0.3628\n",
      "Epoch 913/1024, Loss: 0.3636\n",
      "Epoch 917/1024, Loss: 0.3663\n",
      "Epoch 921/1024, Loss: 0.3636\n",
      "Epoch 925/1024, Loss: 0.3640\n",
      "Epoch 929/1024, Loss: 0.3615\n",
      "Epoch 933/1024, Loss: 0.3585\n",
      "Epoch 937/1024, Loss: 0.3623\n",
      "Epoch 941/1024, Loss: 0.3603\n",
      "Epoch 945/1024, Loss: 0.3559\n",
      "Epoch 949/1024, Loss: 0.3595\n",
      "Epoch 953/1024, Loss: 0.3586\n",
      "Epoch 957/1024, Loss: 0.3589\n",
      "Epoch 961/1024, Loss: 0.3571\n",
      "Epoch 965/1024, Loss: 0.3568\n",
      "Epoch 969/1024, Loss: 0.3579\n",
      "Epoch 973/1024, Loss: 0.3606\n",
      "Epoch 977/1024, Loss: 0.3548\n",
      "Epoch 981/1024, Loss: 0.3558\n",
      "Epoch 985/1024, Loss: 0.3546\n",
      "Epoch 989/1024, Loss: 0.3558\n",
      "Epoch 993/1024, Loss: 0.3533\n",
      "Epoch 997/1024, Loss: 0.3552\n",
      "Epoch 1001/1024, Loss: 0.3581\n",
      "Epoch 1005/1024, Loss: 0.3569\n",
      "Epoch 1009/1024, Loss: 0.3536\n",
      "Epoch 1013/1024, Loss: 0.3553\n",
      "Epoch 1017/1024, Loss: 0.3532\n",
      "Epoch 1021/1024, Loss: 0.3525\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T05:12:44.718340Z",
     "start_time": "2025-10-21T05:11:25.510518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-2048\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "85cc0b9a7e5a7891",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba1853f326454e93ad358e4a95501792"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024, Loss: 2.1385\n",
      "Epoch 5/1024, Loss: 1.2490\n",
      "Epoch 9/1024, Loss: 1.0531\n",
      "Epoch 13/1024, Loss: 0.9465\n",
      "Epoch 17/1024, Loss: 0.8758\n",
      "Epoch 21/1024, Loss: 0.8328\n",
      "Epoch 25/1024, Loss: 0.7997\n",
      "Epoch 29/1024, Loss: 0.7723\n",
      "Epoch 33/1024, Loss: 0.7491\n",
      "Epoch 37/1024, Loss: 0.7361\n",
      "Epoch 41/1024, Loss: 0.7128\n",
      "Epoch 45/1024, Loss: 0.6995\n",
      "Epoch 49/1024, Loss: 0.6850\n",
      "Epoch 53/1024, Loss: 0.6770\n",
      "Epoch 57/1024, Loss: 0.6688\n",
      "Epoch 61/1024, Loss: 0.6589\n",
      "Epoch 65/1024, Loss: 0.6504\n",
      "Epoch 69/1024, Loss: 0.6502\n",
      "Epoch 73/1024, Loss: 0.6334\n",
      "Epoch 77/1024, Loss: 0.6307\n",
      "Epoch 81/1024, Loss: 0.6232\n",
      "Epoch 85/1024, Loss: 0.6247\n",
      "Epoch 89/1024, Loss: 0.6160\n",
      "Epoch 93/1024, Loss: 0.6094\n",
      "Epoch 97/1024, Loss: 0.6075\n",
      "Epoch 101/1024, Loss: 0.5949\n",
      "Epoch 105/1024, Loss: 0.5917\n",
      "Epoch 109/1024, Loss: 0.5875\n",
      "Epoch 113/1024, Loss: 0.5886\n",
      "Epoch 117/1024, Loss: 0.5859\n",
      "Epoch 121/1024, Loss: 0.5812\n",
      "Epoch 125/1024, Loss: 0.5663\n",
      "Epoch 129/1024, Loss: 0.5684\n",
      "Epoch 133/1024, Loss: 0.5614\n",
      "Epoch 137/1024, Loss: 0.5644\n",
      "Epoch 141/1024, Loss: 0.5558\n",
      "Epoch 145/1024, Loss: 0.5550\n",
      "Epoch 149/1024, Loss: 0.5484\n",
      "Epoch 153/1024, Loss: 0.5476\n",
      "Epoch 157/1024, Loss: 0.5474\n",
      "Epoch 161/1024, Loss: 0.5423\n",
      "Epoch 165/1024, Loss: 0.5415\n",
      "Epoch 169/1024, Loss: 0.5420\n",
      "Epoch 173/1024, Loss: 0.5359\n",
      "Epoch 177/1024, Loss: 0.5366\n",
      "Epoch 181/1024, Loss: 0.5394\n",
      "Epoch 185/1024, Loss: 0.5305\n",
      "Epoch 189/1024, Loss: 0.5249\n",
      "Epoch 193/1024, Loss: 0.5263\n",
      "Epoch 197/1024, Loss: 0.5195\n",
      "Epoch 201/1024, Loss: 0.5200\n",
      "Epoch 205/1024, Loss: 0.5211\n",
      "Epoch 209/1024, Loss: 0.5201\n",
      "Epoch 213/1024, Loss: 0.5133\n",
      "Epoch 217/1024, Loss: 0.5168\n",
      "Epoch 221/1024, Loss: 0.5078\n",
      "Epoch 225/1024, Loss: 0.5083\n",
      "Epoch 229/1024, Loss: 0.5054\n",
      "Epoch 233/1024, Loss: 0.5059\n",
      "Epoch 237/1024, Loss: 0.5007\n",
      "Epoch 241/1024, Loss: 0.5027\n",
      "Epoch 245/1024, Loss: 0.5018\n",
      "Epoch 249/1024, Loss: 0.4975\n",
      "Epoch 253/1024, Loss: 0.4958\n",
      "Epoch 257/1024, Loss: 0.4943\n",
      "Epoch 261/1024, Loss: 0.4909\n",
      "Epoch 265/1024, Loss: 0.4888\n",
      "Epoch 269/1024, Loss: 0.4869\n",
      "Epoch 273/1024, Loss: 0.4870\n",
      "Epoch 277/1024, Loss: 0.4839\n",
      "Epoch 281/1024, Loss: 0.4858\n",
      "Epoch 285/1024, Loss: 0.4830\n",
      "Epoch 289/1024, Loss: 0.4796\n",
      "Epoch 293/1024, Loss: 0.4772\n",
      "Epoch 297/1024, Loss: 0.4761\n",
      "Epoch 301/1024, Loss: 0.4773\n",
      "Epoch 305/1024, Loss: 0.4822\n",
      "Epoch 309/1024, Loss: 0.4738\n",
      "Epoch 313/1024, Loss: 0.4718\n",
      "Epoch 317/1024, Loss: 0.4701\n",
      "Epoch 321/1024, Loss: 0.4729\n",
      "Epoch 325/1024, Loss: 0.4689\n",
      "Epoch 329/1024, Loss: 0.4674\n",
      "Epoch 333/1024, Loss: 0.4720\n",
      "Epoch 337/1024, Loss: 0.4710\n",
      "Epoch 341/1024, Loss: 0.4601\n",
      "Epoch 345/1024, Loss: 0.4548\n",
      "Epoch 349/1024, Loss: 0.4662\n",
      "Epoch 353/1024, Loss: 0.4656\n",
      "Epoch 357/1024, Loss: 0.4574\n",
      "Epoch 361/1024, Loss: 0.4629\n",
      "Epoch 365/1024, Loss: 0.4548\n",
      "Epoch 369/1024, Loss: 0.4558\n",
      "Epoch 373/1024, Loss: 0.4566\n",
      "Epoch 377/1024, Loss: 0.4505\n",
      "Epoch 381/1024, Loss: 0.4504\n",
      "Epoch 385/1024, Loss: 0.4497\n",
      "Epoch 389/1024, Loss: 0.4477\n",
      "Epoch 393/1024, Loss: 0.4524\n",
      "Epoch 397/1024, Loss: 0.4450\n",
      "Epoch 401/1024, Loss: 0.4440\n",
      "Epoch 405/1024, Loss: 0.4452\n",
      "Epoch 409/1024, Loss: 0.4427\n",
      "Epoch 413/1024, Loss: 0.4389\n",
      "Epoch 417/1024, Loss: 0.4434\n",
      "Epoch 421/1024, Loss: 0.4502\n",
      "Epoch 425/1024, Loss: 0.4409\n",
      "Epoch 429/1024, Loss: 0.4407\n",
      "Epoch 433/1024, Loss: 0.4391\n",
      "Epoch 437/1024, Loss: 0.4373\n",
      "Epoch 441/1024, Loss: 0.4367\n",
      "Epoch 445/1024, Loss: 0.4401\n",
      "Epoch 449/1024, Loss: 0.4363\n",
      "Epoch 453/1024, Loss: 0.4368\n",
      "Epoch 457/1024, Loss: 0.4345\n",
      "Epoch 461/1024, Loss: 0.4335\n",
      "Epoch 465/1024, Loss: 0.4412\n",
      "Epoch 469/1024, Loss: 0.4354\n",
      "Epoch 473/1024, Loss: 0.4271\n",
      "Epoch 477/1024, Loss: 0.4268\n",
      "Epoch 481/1024, Loss: 0.4261\n",
      "Epoch 485/1024, Loss: 0.4247\n",
      "Epoch 489/1024, Loss: 0.4254\n",
      "Epoch 493/1024, Loss: 0.4267\n",
      "Epoch 497/1024, Loss: 0.4227\n",
      "Epoch 501/1024, Loss: 0.4257\n",
      "Epoch 505/1024, Loss: 0.4195\n",
      "Epoch 509/1024, Loss: 0.4228\n",
      "Epoch 513/1024, Loss: 0.4202\n",
      "Epoch 517/1024, Loss: 0.4214\n",
      "Epoch 521/1024, Loss: 0.4154\n",
      "Epoch 525/1024, Loss: 0.4228\n",
      "Epoch 529/1024, Loss: 0.4167\n",
      "Epoch 533/1024, Loss: 0.4267\n",
      "Epoch 537/1024, Loss: 0.4207\n",
      "Epoch 541/1024, Loss: 0.4168\n",
      "Epoch 545/1024, Loss: 0.4188\n",
      "Epoch 549/1024, Loss: 0.4171\n",
      "Epoch 553/1024, Loss: 0.4125\n",
      "Epoch 557/1024, Loss: 0.4194\n",
      "Epoch 561/1024, Loss: 0.4123\n",
      "Epoch 565/1024, Loss: 0.4119\n",
      "Epoch 569/1024, Loss: 0.4109\n",
      "Epoch 573/1024, Loss: 0.4081\n",
      "Epoch 577/1024, Loss: 0.4093\n",
      "Epoch 581/1024, Loss: 0.4096\n",
      "Epoch 585/1024, Loss: 0.4047\n",
      "Epoch 589/1024, Loss: 0.4157\n",
      "Epoch 593/1024, Loss: 0.4055\n",
      "Epoch 597/1024, Loss: 0.4104\n",
      "Epoch 601/1024, Loss: 0.4047\n",
      "Epoch 605/1024, Loss: 0.4045\n",
      "Epoch 609/1024, Loss: 0.4047\n",
      "Epoch 613/1024, Loss: 0.4090\n",
      "Epoch 617/1024, Loss: 0.3987\n",
      "Epoch 621/1024, Loss: 0.4017\n",
      "Epoch 625/1024, Loss: 0.3980\n",
      "Epoch 629/1024, Loss: 0.4024\n",
      "Epoch 633/1024, Loss: 0.4002\n",
      "Epoch 637/1024, Loss: 0.3988\n",
      "Epoch 641/1024, Loss: 0.4022\n",
      "Epoch 645/1024, Loss: 0.4019\n",
      "Epoch 649/1024, Loss: 0.3974\n",
      "Epoch 653/1024, Loss: 0.3930\n",
      "Epoch 657/1024, Loss: 0.3976\n",
      "Epoch 661/1024, Loss: 0.3937\n",
      "Epoch 665/1024, Loss: 0.3949\n",
      "Epoch 669/1024, Loss: 0.3925\n",
      "Epoch 673/1024, Loss: 0.3926\n",
      "Epoch 677/1024, Loss: 0.3939\n",
      "Epoch 681/1024, Loss: 0.3921\n",
      "Epoch 685/1024, Loss: 0.3965\n",
      "Epoch 689/1024, Loss: 0.3990\n",
      "Epoch 693/1024, Loss: 0.3910\n",
      "Epoch 697/1024, Loss: 0.3884\n",
      "Epoch 701/1024, Loss: 0.3906\n",
      "Epoch 705/1024, Loss: 0.3903\n",
      "Epoch 709/1024, Loss: 0.3920\n",
      "Epoch 713/1024, Loss: 0.3879\n",
      "Epoch 717/1024, Loss: 0.3873\n",
      "Epoch 721/1024, Loss: 0.3882\n",
      "Epoch 725/1024, Loss: 0.3834\n",
      "Epoch 729/1024, Loss: 0.3866\n",
      "Epoch 733/1024, Loss: 0.3820\n",
      "Epoch 737/1024, Loss: 0.3832\n",
      "Epoch 741/1024, Loss: 0.3834\n",
      "Epoch 745/1024, Loss: 0.3819\n",
      "Epoch 749/1024, Loss: 0.3919\n",
      "Epoch 753/1024, Loss: 0.3812\n",
      "Epoch 757/1024, Loss: 0.3872\n",
      "Epoch 761/1024, Loss: 0.3829\n",
      "Epoch 765/1024, Loss: 0.3815\n",
      "Epoch 769/1024, Loss: 0.3803\n",
      "Epoch 773/1024, Loss: 0.3767\n",
      "Epoch 777/1024, Loss: 0.3807\n",
      "Epoch 781/1024, Loss: 0.3816\n",
      "Epoch 785/1024, Loss: 0.3788\n",
      "Epoch 789/1024, Loss: 0.3776\n",
      "Epoch 793/1024, Loss: 0.3807\n",
      "Epoch 797/1024, Loss: 0.3742\n",
      "Epoch 801/1024, Loss: 0.3787\n",
      "Epoch 805/1024, Loss: 0.3766\n",
      "Epoch 809/1024, Loss: 0.3762\n",
      "Epoch 813/1024, Loss: 0.3765\n",
      "Epoch 817/1024, Loss: 0.3756\n",
      "Epoch 821/1024, Loss: 0.3748\n",
      "Epoch 825/1024, Loss: 0.3710\n",
      "Epoch 829/1024, Loss: 0.3783\n",
      "Epoch 833/1024, Loss: 0.3701\n",
      "Epoch 837/1024, Loss: 0.3742\n",
      "Epoch 841/1024, Loss: 0.3683\n",
      "Epoch 845/1024, Loss: 0.3684\n",
      "Epoch 849/1024, Loss: 0.3669\n",
      "Epoch 853/1024, Loss: 0.3737\n",
      "Epoch 857/1024, Loss: 0.3763\n",
      "Epoch 861/1024, Loss: 0.3652\n",
      "Epoch 865/1024, Loss: 0.3680\n",
      "Epoch 869/1024, Loss: 0.3641\n",
      "Epoch 873/1024, Loss: 0.3663\n",
      "Epoch 877/1024, Loss: 0.3656\n",
      "Epoch 881/1024, Loss: 0.3682\n",
      "Epoch 885/1024, Loss: 0.3666\n",
      "Epoch 889/1024, Loss: 0.3645\n",
      "Epoch 893/1024, Loss: 0.3676\n",
      "Epoch 897/1024, Loss: 0.3670\n",
      "Epoch 901/1024, Loss: 0.3608\n",
      "Epoch 905/1024, Loss: 0.3676\n",
      "Epoch 909/1024, Loss: 0.3648\n",
      "Epoch 913/1024, Loss: 0.3668\n",
      "Epoch 917/1024, Loss: 0.3624\n",
      "Epoch 921/1024, Loss: 0.3653\n",
      "Epoch 925/1024, Loss: 0.3609\n",
      "Epoch 929/1024, Loss: 0.3601\n",
      "Epoch 933/1024, Loss: 0.3615\n",
      "Epoch 937/1024, Loss: 0.3666\n",
      "Epoch 941/1024, Loss: 0.3594\n",
      "Epoch 945/1024, Loss: 0.3588\n",
      "Epoch 949/1024, Loss: 0.3552\n",
      "Epoch 953/1024, Loss: 0.3567\n",
      "Epoch 957/1024, Loss: 0.3567\n",
      "Epoch 961/1024, Loss: 0.3596\n",
      "Epoch 965/1024, Loss: 0.3569\n",
      "Epoch 969/1024, Loss: 0.3599\n",
      "Epoch 973/1024, Loss: 0.3563\n",
      "Epoch 977/1024, Loss: 0.3535\n",
      "Epoch 981/1024, Loss: 0.3556\n",
      "Epoch 985/1024, Loss: 0.3588\n",
      "Epoch 989/1024, Loss: 0.3547\n",
      "Epoch 993/1024, Loss: 0.3595\n",
      "Epoch 997/1024, Loss: 0.3515\n",
      "Epoch 1001/1024, Loss: 0.3535\n",
      "Epoch 1005/1024, Loss: 0.3527\n",
      "Epoch 1009/1024, Loss: 0.3493\n",
      "Epoch 1013/1024, Loss: 0.3491\n",
      "Epoch 1017/1024, Loss: 0.3562\n",
      "Epoch 1021/1024, Loss: 0.3519\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
