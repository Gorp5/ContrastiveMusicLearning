{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:35:14.267589Z",
     "start_time": "2025-10-22T04:34:16.538677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import torch.nn\n",
    "\n",
    "from training.inference import get_latents\n",
    "\n",
    "def generate_evaluation_dataset(model, dataloader, name, chunking=True, averaging=False, chunk_size=256):\n",
    "    model.mask_ratio = 0.0\n",
    "    latents, labels = get_latents(dataloader, model, chunking=chunking, averaging=averaging, chunk_size=chunk_size)\n",
    "    print(\"Saving...\")\n",
    "\n",
    "    index = 0\n",
    "    r = random.Random()\n",
    "\n",
    "    counts = [0] * 10\n",
    "    for latent, label in zip(latents, labels):\n",
    "        directory = f\"D:\\\\SongsDataset\\\\GTZAN\\\\latent_datasets\\\\{name}\\\\\"\n",
    "\n",
    "        if r.random() < 0.1 and counts[label] < 10:\n",
    "            directory += \"test\\\\\"\n",
    "            counts[0] += 1\n",
    "        else:\n",
    "            directory += \"train\\\\\"\n",
    "\n",
    "        os.makedirs(os.path.dirname(directory), exist_ok=True)\n",
    "\n",
    "        if not averaging and chunking:\n",
    "            for x in range(label.shape[0]):\n",
    "                label_name = directory + f\"{index}_label\"\n",
    "                latent_name = directory + f\"{index}_data\"\n",
    "                torch.save(latent, latent_name + \".pt\")\n",
    "                torch.save(label, label_name + \".pt\")\n",
    "                index += 1\n",
    "        else:\n",
    "            label_name = directory + f\"{index}_label\"\n",
    "            latent_name = directory + f\"{index}_data\"\n",
    "            torch.save(latent, latent_name + \".pt\")\n",
    "            torch.save(label, label_name + \".pt\")\n",
    "            index += 1"
   ],
   "id": "26862cea14b0f113",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-22T04:35:17.578229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.data_utils import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn\n",
    "\n",
    "directory = \"D:\\\\SongsDataset\\\\GTZAN\\\\genres_original\\\\\"\n",
    "GTZAN_dataset = GTZAN(directory)\n",
    "GTZAN_dataloader = DataLoader(\n",
    "    GTZAN_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=1\n",
    ")"
   ],
   "id": "5fe6cc6865b7454c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d437b6aafcf491b8ae42bc1778935df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:34:11.012424Z",
     "start_time": "2025-10-21T18:27:14.068215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-Sinusoidal\\\\Epoch-79.pt\", weights_only=False)\n",
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"Sinusoidal-Chunking-256\", chunking=True, chunk_size=256, averaging=True)"
   ],
   "id": "78096b22d19bab28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [02:28<00:00,  6.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:34:11.012424Z",
     "start_time": "2025-10-21T18:29:47.691166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\trained_models\\\\Myna-CLS-ALIBI\\\\Epoch-103.pt\", weights_only=False)\n",
    "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-256\", chunking=True, chunk_size=256, averaging=True)"
   ],
   "id": "b03abf156baf1432",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [01:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Alibi2DBias' object has no attribute 'only_x'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mCoding\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mSongAnalyzer\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mAnalyzer\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtrained_models\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mMyna-CLS-ALIBI\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mEpoch-103.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m, weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mgenerate_evaluation_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGTZAN_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mALIBI-Chunking-256\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maveraging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[17], line 12\u001B[0m, in \u001B[0;36mgenerate_evaluation_dataset\u001B[1;34m(model, dataloader, name, chunking, averaging, chunk_size)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_evaluation_dataset\u001B[39m(model, dataloader, name, chunking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, averaging\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m):\n\u001B[0;32m     11\u001B[0m     model\u001B[38;5;241m.\u001B[39mmask_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 12\u001B[0m     latents, labels \u001B[38;5;241m=\u001B[39m \u001B[43mget_latents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maveraging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maveraging\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:33\u001B[0m, in \u001B[0;36mget_latents\u001B[1;34m(dataloader, model, chunking, averaging, chunk_size)\u001B[0m\n\u001B[0;32m     30\u001B[0m     data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(data)\n\u001B[0;32m     31\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m latent \u001B[38;5;241m=\u001B[39m \u001B[43mrun_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maveraging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maveraging\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m all_latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     35\u001B[0m all_labels\u001B[38;5;241m.\u001B[39mappend(label)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:51\u001B[0m, in \u001B[0;36mrun_batch\u001B[1;34m(model, batch, averaging)\u001B[0m\n\u001B[0;32m     49\u001B[0m latents \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[1;32m---> 51\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m     latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     54\u001B[0m latents \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(latents, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:163\u001B[0m, in \u001B[0;36mMyna.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    160\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((cls_tokens, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malibi:\n\u001B[1;32m--> 163\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoordinates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:86\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, coords, cls)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, coords\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m coords \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\PositionalEmbeddings.py:49\u001B[0m, in \u001B[0;36mAlibi2DBias.forward\u001B[1;34m(self, coords)\u001B[0m\n\u001B[0;32m     45\u001B[0m x, y \u001B[38;5;241m=\u001B[39m coords[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mfloat(), coords[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     47\u001B[0m dx \u001B[38;5;241m=\u001B[39m (x[:, :, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m-\u001B[39m x[:, \u001B[38;5;28;01mNone\u001B[39;00m, :])\u001B[38;5;241m.\u001B[39mabs()\n\u001B[1;32m---> 49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monly_x\u001B[49m:\n\u001B[0;32m     50\u001B[0m     dist \u001B[38;5;241m=\u001B[39m dx\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1929\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1930\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1931\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m   1932\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1933\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Alibi2DBias' object has no attribute 'only_x'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:34:11.036421700Z",
     "start_time": "2025-10-21T05:00:43.974118Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-512\", chunking=True, chunk_size=512, averaging=True)",
   "id": "2d7c45006464f818",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:35<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:34:11.047421300Z",
     "start_time": "2025-10-21T05:01:20.692305Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-1024\", chunking=True, chunk_size=1024, averaging=True)",
   "id": "a532740122b261b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:29<00:00, 34.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:34:11.048421700Z",
     "start_time": "2025-10-21T05:01:50.910074Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-2048\", chunking=True, chunk_size=2048, averaging=True)",
   "id": "f74f278dbbc18557",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:38<00:00, 26.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:34:11.055422200Z",
     "start_time": "2025-10-21T03:52:34.162876Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-4096\", chunking=True, chunk_size=4096)",
   "id": "abf4a9871a802333",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgenerate_evaluation_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGTZAN_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mALIBI-Chunking-4096\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4096\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 9\u001B[0m, in \u001B[0;36mgenerate_evaluation_dataset\u001B[1;34m(model, dataloader, name, chunking, chunk_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_evaluation_dataset\u001B[39m(model, dataloader, name, chunking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m):\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m.\u001B[39mmask_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m----> 9\u001B[0m     latents, labels \u001B[38;5;241m=\u001B[39m \u001B[43mget_latents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m index, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(latents, labels)):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:33\u001B[0m, in \u001B[0;36mget_latents\u001B[1;34m(dataloader, model, chunking, chunk_size)\u001B[0m\n\u001B[0;32m     30\u001B[0m     data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(data)\n\u001B[0;32m     31\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m latent \u001B[38;5;241m=\u001B[39m \u001B[43mrun_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maveraging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m all_latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     35\u001B[0m all_labels\u001B[38;5;241m.\u001B[39mappend(label)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:51\u001B[0m, in \u001B[0;36mrun_batch\u001B[1;34m(model, batch, averaging)\u001B[0m\n\u001B[0;32m     49\u001B[0m latents \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[1;32m---> 51\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m     latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     54\u001B[0m latents \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(latents, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:163\u001B[0m, in \u001B[0;36mMyna.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    160\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((cls_tokens, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malibi:\n\u001B[1;32m--> 163\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoordinates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:86\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, coords, cls)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, coords\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m coords \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\PositionalEmbeddings.py:51\u001B[0m, in \u001B[0;36mAlibi2DBias.forward\u001B[1;34m(self, coords)\u001B[0m\n\u001B[0;32m     48\u001B[0m dist \u001B[38;5;241m=\u001B[39m dx \u001B[38;5;241m+\u001B[39m dy\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# raster order\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m flat \u001B[38;5;241m=\u001B[39m y \u001B[38;5;241m*\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m     52\u001B[0m le_mask \u001B[38;5;241m=\u001B[39m (flat[:, \u001B[38;5;28;01mNone\u001B[39;00m, :] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m flat[:, :, \u001B[38;5;28;01mNone\u001B[39;00m])\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     54\u001B[0m slopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslopes\u001B[38;5;241m.\u001B[39mto(coords\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:34:11.056422200Z",
     "start_time": "2025-10-21T03:51:50.599583Z"
    }
   },
   "cell_type": "code",
   "source": "generate_evaluation_dataset(model, GTZAN_dataloader, \"ALIBI-Chunking-No-Chunking\", chunking=False, chunk_size=16384)",
   "id": "3b4516ba90a5ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgenerate_evaluation_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGTZAN_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mALIBI-Chunking-No-Chunking\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16384\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 9\u001B[0m, in \u001B[0;36mgenerate_evaluation_dataset\u001B[1;34m(model, dataloader, name, chunking, chunk_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_evaluation_dataset\u001B[39m(model, dataloader, name, chunking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m):\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m.\u001B[39mmask_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m----> 9\u001B[0m     latents, labels \u001B[38;5;241m=\u001B[39m \u001B[43mget_latents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m index, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(latents, labels)):\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:33\u001B[0m, in \u001B[0;36mget_latents\u001B[1;34m(dataloader, model, chunking, chunk_size)\u001B[0m\n\u001B[0;32m     30\u001B[0m     data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(data)\n\u001B[0;32m     31\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m latent \u001B[38;5;241m=\u001B[39m \u001B[43mrun_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maveraging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m all_latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     35\u001B[0m all_labels\u001B[38;5;241m.\u001B[39mappend(label)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\training\\inference.py:51\u001B[0m, in \u001B[0;36mrun_batch\u001B[1;34m(model, batch, averaging)\u001B[0m\n\u001B[0;32m     49\u001B[0m latents \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m mini_batches:\n\u001B[1;32m---> 51\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m     latents\u001B[38;5;241m.\u001B[39mappend(latent)\n\u001B[0;32m     54\u001B[0m latents \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(latents, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:163\u001B[0m, in \u001B[0;36mMyna.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    160\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((cls_tokens, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malibi:\n\u001B[1;32m--> 163\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoordinates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x)\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\Myna.py:86\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, coords, cls)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, coords\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m coords \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malibi_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m         alibi_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_alibi_with_cls(alibi_bias, has_cls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv-flash\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\src\\models\\PositionalEmbeddings.py:51\u001B[0m, in \u001B[0;36mAlibi2DBias.forward\u001B[1;34m(self, coords)\u001B[0m\n\u001B[0;32m     48\u001B[0m dist \u001B[38;5;241m=\u001B[39m dx \u001B[38;5;241m+\u001B[39m dy\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# raster order\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m flat \u001B[38;5;241m=\u001B[39m y \u001B[38;5;241m*\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m     52\u001B[0m le_mask \u001B[38;5;241m=\u001B[39m (flat[:, \u001B[38;5;28;01mNone\u001B[39;00m, :] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m flat[:, :, \u001B[38;5;28;01mNone\u001B[39;00m])\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     54\u001B[0m slopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslopes\u001B[38;5;241m.\u001B[39mto(coords\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-21T18:05:10.858685Z",
     "start_time": "2025-10-21T18:05:10.851541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.data_utils import LatentDataset\n",
    "import torch\n",
    "from datasets import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----- Training Loop -----\n",
    "def train_model(model, train_dataloader, test_dataloader, config, device=\"cuda\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(config.num_epochs)):\n",
    "        total_loss = 0\n",
    "        for labels, data in train_dataloader:\n",
    "            # if dataset returns one-hot, convert back to integer for CrossEntropy\n",
    "            if labels.ndim > 1:\n",
    "                labels = labels.argmax(dim=1)\n",
    "\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 64 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config.num_epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "            evaluate_model(model, test_dataloader, config, device)\n",
    "            model.train()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, dataloader, config, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for labels, data in tqdm(dataloader):\n",
    "        # if dataset returns one-hot, convert back to integer for CrossEntropy\n",
    "        if labels.ndim > 1:\n",
    "            labels = labels.argmax(dim=1)\n",
    "\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    scores = f1_score(all_labels, all_predictions, average=\"macro\")\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"F1: {scores}\\nAccuracy: {accuracy:.4f}\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:05:12.058011Z",
     "start_time": "2025-10-21T18:05:12.052013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "from torch import nn, optim\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input=128, output=10, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512, output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input=128, output=10):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(input, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "96cc0615fb6248ef",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:05:13.808822Z",
     "start_time": "2025-10-21T18:05:13.801972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training.evaluation import local_coherence\n",
    "from utils.Config import Config\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def test_latent_dataaset(dataset):\n",
    "    directory = f\"D:\\\\SongsDataset\\\\GTZAN\\\\latent_datasets\\\\{dataset}\\\\\"\n",
    "\n",
    "    num_classes = 10\n",
    "    train_latent_dataset = LatentDataset(directory + \"train\\\\\", num_classes=num_classes)\n",
    "    train_latent_dataloader = DataLoader(\n",
    "        train_latent_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    test_latent_dataset = LatentDataset(directory + \"test\\\\\", num_classes=num_classes)\n",
    "    test_latent_dataloader = DataLoader(\n",
    "        test_latent_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    lgc = local_coherence(np.array(train_latent_dataloader.latents), np.array(train_latent_dataloader.labels), k=50)\n",
    "    print(f\"Local Coherence: {lgc}\")\n",
    "\n",
    "    model_name = f\"LinearClassifier-{dataset}\"\n",
    "    config = Config(\n",
    "            save_path=f\"trained_models\\\\{model_name}\\\\\",\n",
    "            num_epochs=512,\n",
    "            learning_rate=1e-3,\n",
    "            weight_decay=1e-3,\n",
    "            num_workers=2,\n",
    "            batch_size= 64,\n",
    "            eval_batch_size=64,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    device = \"cuda\"\n",
    "    model = MLP(128, num_classes).to(device)\n",
    "    train_model(model, train_latent_dataloader, config, device=device)\n",
    "    evaluate_model(model, test_latent_dataloader, config, device=device)"
   ],
   "id": "e0918c9fc1788c45",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:06:21.897193Z",
     "start_time": "2025-10-21T18:05:14.866950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"Sinusoidal-Chunking-256\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "de1b0ebaf9e0603b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Coherence: 0.4187387387387388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc77accaa4fe452081baf8ecd8a35226"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512, Loss: 1.7567\n",
      "Epoch 65/512, Loss: 0.0227\n",
      "Epoch 129/512, Loss: 0.0099\n",
      "Epoch 193/512, Loss: 0.0043\n",
      "Epoch 257/512, Loss: 0.0052\n",
      "Epoch 321/512, Loss: 0.0034\n",
      "Epoch 385/512, Loss: 0.0013\n",
      "Epoch 449/512, Loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61398bef730e4e7b8b474ff6051cc6c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.998999974999375\n",
      "Accuracy: 0.9990\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:07:21.915211Z",
     "start_time": "2025-10-21T18:06:22.036532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-256\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "e87d4a5466e02ab6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Coherence: 0.4216416416416417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d368d400c5f14897a9388b60c3cd70f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512, Loss: 1.7324\n",
      "Epoch 65/512, Loss: 0.0262\n",
      "Epoch 129/512, Loss: 0.0080\n",
      "Epoch 193/512, Loss: 0.0049\n",
      "Epoch 257/512, Loss: 0.0070\n",
      "Epoch 321/512, Loss: 0.0064\n",
      "Epoch 385/512, Loss: 0.0030\n",
      "Epoch 449/512, Loss: 0.0036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "636dfc175dbd454086b856017bf94dd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.998999974999375\n",
      "Accuracy: 0.9990\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:08:19.513671Z",
     "start_time": "2025-10-21T18:07:29.897811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-512\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "c7ae6d0abaaf8504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Coherence: 0.4152552552552553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e4b0c9a81474bc499c9b755afa0c166"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512, Loss: 1.6886\n",
      "Epoch 65/512, Loss: 0.0343\n",
      "Epoch 129/512, Loss: 0.0085\n",
      "Epoch 193/512, Loss: 0.0085\n",
      "Epoch 257/512, Loss: 0.0036\n",
      "Epoch 321/512, Loss: 0.0419\n",
      "Epoch 385/512, Loss: 0.0014\n",
      "Epoch 449/512, Loss: 0.0380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e19ea650d7eb490fa63f4a049212b319"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.998999974999375\n",
      "Accuracy: 0.9990\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:09:04.612853Z",
     "start_time": "2025-10-21T18:08:19.545853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-1024\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "32c79c97700354d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Coherence: 0.4057457457457458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ad0db44a20f4a52a52ed81df2dd1f1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512, Loss: 1.7307\n",
      "Epoch 65/512, Loss: 0.0362\n",
      "Epoch 129/512, Loss: 0.0082\n",
      "Epoch 193/512, Loss: 0.0043\n",
      "Epoch 257/512, Loss: 0.0042\n",
      "Epoch 321/512, Loss: 0.0019\n",
      "Epoch 385/512, Loss: 0.0035\n",
      "Epoch 449/512, Loss: 0.0029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "627387ad32ef44af9ffbf1c54cbd584f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.998999974999375\n",
      "Accuracy: 0.9990\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:10:01.541916Z",
     "start_time": "2025-10-21T18:09:04.643186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"ALIBI-Chunking-2048\"\n",
    "test_latent_dataaset(dataset)"
   ],
   "id": "85cc0b9a7e5a7891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Coherence: 0.4044644644644645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b396f83496044df94836db79f514bee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512, Loss: 1.7127\n",
      "Epoch 65/512, Loss: 0.0315\n",
      "Epoch 129/512, Loss: 0.0095\n",
      "Epoch 193/512, Loss: 0.0052\n",
      "Epoch 257/512, Loss: 0.0076\n",
      "Epoch 321/512, Loss: 0.0021\n",
      "Epoch 385/512, Loss: 0.0019\n",
      "Epoch 449/512, Loss: 0.0093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15e4bf3add7a4c899e682f3ddab1658f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.998999974999375\n",
      "Accuracy: 0.9990\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T15:09:35.097594Z",
     "start_time": "2025-10-21T15:09:35.094593Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "719c2154d0dfd396",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9748416e6eb8d243"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
