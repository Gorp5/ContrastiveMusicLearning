{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_distance(x, centroids, eps=1e-8):\n",
    "    # Normalize to unit vectors\n",
    "    x_norm = x / (x.norm(dim=1, keepdim=True) + eps)\n",
    "    c_norm = centroids / (centroids.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    # Compute cosine similarity (dot product)\n",
    "    sim = torch.mm(x_norm, c_norm.t())  # [N, K]\n",
    "\n",
    "    # Convert to distance (1 - similarity)\n",
    "    distances = 1.0 - sim\n",
    "\n",
    "    return distances\n",
    "\n",
    "def weighted_cosine_distance(x, centroids, weights=None, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute cosine distance between points and centroids with per-dimension weights.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Data points [N, D]\n",
    "        centroids (torch.Tensor): Cluster centers [K, D]\n",
    "        weights (torch.Tensor): Feature weights [D], larger = more important\n",
    "        eps (float): Small constant for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        distances (torch.Tensor): [N, K] cosine distance (1 - weighted cosine similarity)\n",
    "    \"\"\"\n",
    "    # Normalize weights so they’re non-negative and scale appropriately\n",
    "    weights = weights.to(x.device)\n",
    "    weights = weights / (weights.norm() + eps)\n",
    "\n",
    "    # Apply weights\n",
    "    xw = x * weights\n",
    "    cw = centroids * weights\n",
    "\n",
    "    # Normalize weighted vectors to unit length\n",
    "    xw_norm = xw / (xw.norm(dim=1, keepdim=True) + eps)\n",
    "    cw_norm = cw / (cw.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    # Compute weighted cosine similarity\n",
    "    sim = torch.mm(xw_norm, cw_norm.t())  # [N, K]\n",
    "\n",
    "    # Convert to distance\n",
    "    distances = 1.0 - sim\n",
    "\n",
    "    return distances\n",
    "\n",
    "# === Weighted cosine distance per cluster ===\n",
    "def cluster_weighted_cosine_distance(x, centroids, weights, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Weighted cosine distance where each cluster has its own weight vector.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Data points [N, D]\n",
    "        centroids (torch.Tensor): Cluster centers [K, D]\n",
    "        weights (torch.Tensor): Per-cluster weights [K, D]\n",
    "        eps (float): Numerical stability\n",
    "    Returns:\n",
    "        distances [N, K]\n",
    "    \"\"\"\n",
    "    N, D = x.shape\n",
    "    K = centroids.shape[0]\n",
    "\n",
    "    # Normalize both x and centroids per cluster (with weights)\n",
    "    x_exp = x.unsqueeze(1).expand(N, K, D)       # [N, K, D]\n",
    "    c_exp = centroids.unsqueeze(0).expand(N, K, D)  # [N, K, D]\n",
    "    w_exp = weights.unsqueeze(0).expand(N, K, D)    # [N, K, D]\n",
    "\n",
    "    # Apply weights before cosine similarity\n",
    "    x_w = x_exp * w_exp\n",
    "    c_w = c_exp * w_exp\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    dot = (x_w * c_w).sum(dim=-1)\n",
    "    x_norm = x_w.norm(dim=-1) + eps\n",
    "    c_norm = c_w.norm(dim=-1) + eps\n",
    "    sim = dot / (x_norm * c_norm)\n",
    "\n",
    "    distances = 1.0 - sim  # cosine distance\n",
    "    return distances"
   ],
   "id": "46f559a8ce0b7f5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# === Basic K-Means using custom distance ===\n",
    "def kmeans(x, num_clusters, distance_fn, num_iters=100, tol=1e-4, verbose=False):\n",
    "    device = x.device\n",
    "    n, d = x.shape\n",
    "\n",
    "    # Random initialization of centroids\n",
    "    indices = torch.randperm(n, device=device)[:num_clusters]\n",
    "    centroids = x[indices]\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        distances = distance_fn(x, centroids)\n",
    "        cluster_assignments = torch.argmin(distances, dim=1)\n",
    "\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        for k in range(num_clusters):\n",
    "            mask = cluster_assignments == k\n",
    "            if mask.any():\n",
    "                new_centroids[k] = x[mask].mean(dim=0)\n",
    "            else:\n",
    "                new_centroids[k] = x[torch.randint(0, n, (1,), device=device)]\n",
    "\n",
    "        shift = torch.norm(centroids - new_centroids, dim=1).sum()\n",
    "        if verbose:\n",
    "            print(f\"Iteration {i+1}/{num_iters} | centroid shift = {shift.item():.6f}\")\n",
    "        if shift < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_assignments, centroids"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Compute per-cluster per-dimension variance ===\n",
    "def cluster_variances(x, cluster_assignments, num_clusters):\n",
    "    n, d = x.shape\n",
    "    device = x.device\n",
    "    variances = torch.zeros(num_clusters, d, device=device)\n",
    "\n",
    "    for k in range(num_clusters):\n",
    "        mask = (cluster_assignments == k)\n",
    "        if mask.any():\n",
    "            cluster_points = x[mask]\n",
    "            variances[k] = cluster_points.var(dim=0, unbiased=False)\n",
    "        else:\n",
    "            variances[k] = torch.ones(d, device=device)  # fallback\n",
    "\n",
    "    # Step 2: mean variance across clusters per dimension\n",
    "    mean_variance = variances.mean(dim=0)  # [D]\n",
    "\n",
    "    # Step 3: deviation of each cluster variance from mean variance\n",
    "    deviations = torch.abs(variances - mean_variance)  # [K, D]\n",
    "\n",
    "    return variances"
   ],
   "id": "ef484c954d66fdd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def weighted_kmeans(x, num_clusters, weights, centroids=None, distance_fn=cluster_weighted_cosine_distance, max_iters=100, tol=1e-4, verbose=True):\n",
    "    device = x.device\n",
    "    N, D = x.shape\n",
    "    K = num_clusters\n",
    "\n",
    "    # Initialize centroids randomly\\\n",
    "    if centroids is None:\n",
    "        indices = torch.randperm(N, device=device)[:K]\n",
    "        centroids = x[indices].clone()\n",
    "\n",
    "    prev_assignments = None\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        # Compute weighted cosine distances\n",
    "        distances = distance_fn(x, centroids, weights=weights)\n",
    "        assignments = distances.argmin(dim=1)\n",
    "\n",
    "        # Check for convergence\n",
    "        if prev_assignments is not None and torch.equal(assignments, prev_assignments):\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "        prev_assignments = assignments.clone()\n",
    "\n",
    "        # Update centroids\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        for k in range(K):\n",
    "            mask = assignments == k\n",
    "            if mask.any():\n",
    "                new_centroids[k] = x[mask].mean(dim=0)\n",
    "            else:\n",
    "                new_centroids[k] = x[torch.randint(0, N, (1,), device=device)]\n",
    "\n",
    "        shift = torch.norm(centroids - new_centroids, dim=1).sum()\n",
    "        if verbose:\n",
    "            print(f\"Iteration {iteration+1}/{max_iters} | centroid shift = {shift.item():.6f}\")\n",
    "\n",
    "        if shift < tol:\n",
    "            if iteration > 2:\n",
    "                shift = 1\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "\n",
    "    return assignments, centroids, shift\n",
    "\n",
    "# === Adaptive Weighted K-Means loop ===\n",
    "def weighted_cosine_kmeans(x, num_clusters, distance_fn=cluster_weighted_cosine_distance, max_iters_inner=128, max_iters_outer=8, tol=1e-4, verbose=True, rate=1e-1):\n",
    "    \"\"\"\n",
    "    Runs iterative K-Means with weighted cosine distance and\n",
    "    moving-average variance-based weight updates.\n",
    "    \"\"\"\n",
    "    # Compute new variance-based weights\n",
    "    weights = torch.ones((num_clusters, x.shape[-1]), device=x.device)\n",
    "    centroids = None\n",
    "\n",
    "    for iteration in range(max_iters_outer):\n",
    "        assignments, centroids, shift = weighted_kmeans(x, num_clusters, weights, centroids=centroids, distance_fn=distance_fn, max_iters=max_iters_inner, tol=tol, verbose=verbose)\n",
    "\n",
    "        if shift < 1e-8:\n",
    "            break\n",
    "\n",
    "        variances = cluster_variances(x, assignments, num_clusters)\n",
    "        new_weights = 1 / (variances + 1e-8)\n",
    "\n",
    "        # Normalize weights per cluster (optional but stabilizing)\n",
    "        row_max, _ = new_weights.max(dim=1, keepdim=True)\n",
    "        row_max = row_max + 1e-8\n",
    "        new_weights = new_weights / row_max\n",
    "\n",
    "        # EMA update for stability\n",
    "        difference = weights - new_weights\n",
    "        weights = weights - rate * difference\n",
    "\n",
    "        if verbose:\n",
    "            avg_change = torch.abs(weights - new_weights).mean().item()\n",
    "            print(f\"Iter {iteration:02d} | Avg weight Δ: {avg_change:.4f}\")\n",
    "\n",
    "        # Convergence by weight change\n",
    "        if avg_change < tol:\n",
    "            if verbose:\n",
    "                print(f\"Weight changes below tol ({tol}); stopping.\")\n",
    "            break\n",
    "\n",
    "    return assignments, centroids, weights"
   ],
   "id": "247275e9031c4de6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_tsne_clusters(data, assignments, title=\"t-SNE Clusters\", perplexity=30, random_state=42):\n",
    "    \"\"\"\n",
    "    Generates a 2D t-SNE plot of high-dimensional data colored by cluster assignments.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): [N, D] input data\n",
    "        assignments (torch.Tensor or np.array): [N] cluster labels\n",
    "        title (str): Plot title\n",
    "        perplexity (float): t-SNE perplexity\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    # Move to CPU and convert to numpy\n",
    "    data_np = data.cpu().numpy()\n",
    "    labels_np = assignments.cpu().long().numpy()\n",
    "\n",
    "    # Compute t-SNE embedding\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n",
    "    data_2d = tsne.fit_transform(data_np)\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = plt.scatter(data_2d[:,0], data_2d[:,1], c=labels_np, cmap=\"tab10\", alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.colorbar(scatter, label=\"Cluster\")\n",
    "    plt.show()"
   ],
   "id": "9bd2660c44501e97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def euclidean_distance(clusters, eps=1e-8):\n",
    "    x = [x.get_position for x in clusters]\n",
    "\n",
    "    diff = x.unsqueeze(1) - x.unsqueeze(0)\n",
    "    distances = torch.sqrt((diff ** 2).sum(dim=2) + eps)  # [N, K]\n",
    "    return distances\n",
    "\n",
    "def cosine_distance(x, y=None, eps=1e-8):\n",
    "    if y is None:\n",
    "        y = x\n",
    "\n",
    "    # Normalize both tensors along feature dimension\n",
    "    x_norm = x / (x.norm(dim=1, keepdim=True) + eps)\n",
    "    y_norm = y / (y.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    # Cosine similarity matrix\n",
    "    sim = torch.mm(x_norm, y_norm.T)\n",
    "\n",
    "    # Convert to distance (1 - cosine similarity)\n",
    "    dist = 1.0 - sim\n",
    "    return dist\n",
    "\n",
    "def variance_increase_distance(clusters, eps=1e-8):\n",
    "    N = len(clusters)\n",
    "    device = clusters[0].points.device\n",
    "\n",
    "    # Precompute stats\n",
    "    means = torch.stack([c.points.mean(dim=0) for c in clusters])  # [N, D]\n",
    "    variances = torch.stack([c.points.var(dim=0, unbiased=False).mean() for c in clusters])  # [N]\n",
    "    sizes = torch.tensor([c.points.shape[0] for c in clusters], dtype=torch.float32, device=device)  # [N]\n",
    "\n",
    "    delta_var = torch.zeros((N, N), device=device)\n",
    "\n",
    "    for i in range(N):\n",
    "        n_i = sizes[i]\n",
    "        μ_i = means[i]\n",
    "        var_i = variances[i]\n",
    "\n",
    "        # j > i\n",
    "        j_indices = torch.arange(i + 1, N, device=device)\n",
    "        n_j = sizes[j_indices]                      # [M]\n",
    "        μ_j = means[j_indices]                      # [M, D]\n",
    "        var_j = variances[j_indices]                # [M]\n",
    "\n",
    "        n_total = n_i + n_j                          # [M]\n",
    "        # Weighted within-cluster variance\n",
    "        var_within = (n_i * var_i + n_j * var_j) / n_total  # [M]\n",
    "        # Between-cluster variance\n",
    "        var_between = (n_i * n_j / (n_total ** 2)) * ((μ_i - μ_j).pow(2).sum(dim=1))  # [M]\n",
    "        # Total merged variance\n",
    "        merged_var = var_within + var_between\n",
    "        # Δ variance relative to average\n",
    "        delta = merged_var - 0.5 * (var_i + var_j)  # [M]\n",
    "\n",
    "        delta_var[i, j_indices] = delta\n",
    "        delta_var[j_indices, i] = delta  # symmetric\n",
    "\n",
    "    return delta_var\n",
    "\n",
    "def variance_increase_cosine_distance(clusters, eps=1e-8):\n",
    "    sizes = []\n",
    "\n",
    "    sizes = torch.tensor([c.shape[0] for c in clusters], dtype=torch.float32, device=clusters.device)  # [K]\n",
    "\n",
    "    # --- Normalize centroids for cosine similarity ---\n",
    "    means_norm = clusters / (clusters.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    # --- Compute pairwise cosine similarity ---\n",
    "    sim = torch.mm(means_norm, means_norm.T).clamp(-1.0, 1.0)\n",
    "\n",
    "    # --- Convert similarity to distance ---\n",
    "    cosine_dist = 1.0 - sim  # [K, K]\n",
    "\n",
    "    # --- Compute Ward-like Δ variance (weighted by cluster sizes) ---\n",
    "    n_i = sizes.view(-1, 1)\n",
    "    n_j = sizes.view(1, -1)\n",
    "\n",
    "    delta_var = (n_i * n_j) / (n_i + n_j + eps) * cosine_dist\n",
    "    delta_var.fill_diagonal_(0)\n",
    "\n",
    "    return delta_var"
   ],
   "id": "940e26616307d5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, points: torch.Tensor, indices=None):\n",
    "        \"\"\"\n",
    "        points: [N, D] tensor of member points\n",
    "        indices: optional list of original indices\n",
    "        \"\"\"\n",
    "        self.points = points\n",
    "        self.indices = indices if indices is not None else list(range(points.shape[0]))\n",
    "        self.update_stats()\n",
    "\n",
    "    def update_stats(self):\n",
    "        \"\"\"Recompute centroid and variance.\"\"\"\n",
    "        self.centroid = self.points.mean(dim=0)\n",
    "        self.variance = self.points.var(dim=0, unbiased=False)\n",
    "\n",
    "    def get_position(self):\n",
    "        \"\"\"Return centroid (for distance computations).\"\"\"\n",
    "        return self.centroid\n",
    "\n",
    "    def get_variance(self):\n",
    "        \"\"\"Return per-dimension variance.\"\"\"\n",
    "        return self.variance\n",
    "\n",
    "    def merge(self, other):\n",
    "        \"\"\"Merge this cluster with another cluster or single point tensor.\"\"\"\n",
    "        if isinstance(other, Cluster):\n",
    "            new_points = torch.cat([self.points, other.points], dim=0)\n",
    "            new_indices = self.indices + other.indices\n",
    "        else:\n",
    "            # Assume `other` is a single point tensor [D]\n",
    "            new_points = torch.cat([self.points, other.unsqueeze(0)], dim=0)\n",
    "            new_indices = self.indices + [-1]  # placeholder\n",
    "        return Cluster(new_points, new_indices)\n",
    "\n",
    "\n",
    "def hierarchical_clustering(data, K, distance_fn):\n",
    "    \"\"\"\n",
    "    Custom hierarchical clustering with distance_fn.\n",
    "\n",
    "    Args:\n",
    "        data: [N, D] tensor\n",
    "        K: target number of clusters\n",
    "        distance_fn: callable (a, b) -> distance scalar\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = data.shape\n",
    "    clusters = [Cluster(data[i].unsqueeze(0), [i]) for i in range(N)]\n",
    "\n",
    "    # Iteratively merge until K remain\n",
    "    pbar = tqdm(total=N - K)\n",
    "    while len(clusters) > K:\n",
    "        # Collect all current centroids\n",
    "        centroids = torch.stack([c.get_position() for c in clusters])  # [M, D]\n",
    "        M = centroids.shape[0]\n",
    "\n",
    "        # Compute full pairwise distance matrix\n",
    "        dist_matrix = distance_fn(clusters)  # [M, M]\n",
    "        dist_matrix = dist_matrix + torch.eye(M, device=dist_matrix.device) * 1e9  # mask self-distances\n",
    "\n",
    "        # Find minimum distance pair (single argmin over upper triangle)\n",
    "        min_idx = torch.argmin(dist_matrix)\n",
    "        i, j = divmod(min_idx.item(), M)\n",
    "\n",
    "        # Merge those two clusters\n",
    "        new_cluster = clusters[i].merge(clusters[j])\n",
    "\n",
    "        # Remove the old ones and append the new one\n",
    "        new_clusters = []\n",
    "        for idx, c in enumerate(clusters):\n",
    "            if idx not in (i, j):\n",
    "                new_clusters.append(c)\n",
    "        new_clusters.append(new_cluster)\n",
    "        clusters = new_clusters\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def get_cluster_assignments(clusters, n_points):\n",
    "    assignments = torch.full((n_points,), -1, dtype=torch.long)\n",
    "\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for point_idx in cluster.indices:\n",
    "            if point_idx >= 0:  # ignore synthetic/placeholder indices (e.g. -1)\n",
    "                assignments[point_idx] = cluster_idx\n",
    "\n",
    "    return assignments"
   ],
   "id": "97d1be9bdcc7e9b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputfile = \"E:\\Coding\\SongAnalyzer\\Analyzer\\src\\output_analysis\\output-Myna-CLS-ALIBI-Chunking-256.csv\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ids = []\n",
    "data = []\n",
    "\n",
    "with open(inputfile, \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split(\" \")\n",
    "        numbers = [float(x) for x in parts[:128]]\n",
    "        id = \" \".join(parts[128:])\n",
    "        data.append(torch.tensor(numbers, device=device))\n",
    "        ids.append(id)\n",
    "\n",
    "# Generate synthetic data\n",
    "# data = []\n",
    "# num_clusters = 32\n",
    "# nums = [x * 2 + 64 for x in range(num_clusters)]\n",
    "#\n",
    "# for i in range(num_clusters):\n",
    "#     x = torch.randn(nums[i], 32, device=device) * torch.randn(32, device=device)\n",
    "#     data.append(x)"
   ],
   "id": "43fe80a64ce251cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Hi Brad,\n",
    "I’ve been following the MIR/representation-learning work from people now at Suno — especially [paper], which aligns closely with a project I built: transformer-based audio embeddings + a contrastive VAE for music similarity. Demo/code: [github.com/Gorp5/ContrastiveMusicLearning]. Write-up with method/results (WIP PDF): [PDF link].\n",
    "Would love to contribute to Suno’s ML/MIR engineering efforts."
   ],
   "id": "f221cbe8db456254"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "data = torch.stack(data, dim=0)\n",
    "\n",
    "clusters = hierarchical_clustering(data, K=256, distance_fn=variance_increase_distance)\n",
    "assignments = get_cluster_assignments(clusters, data.shape[0])\n",
    "\n",
    "# Display results\n",
    "for idx, c in enumerate(clusters):\n",
    "    print(f\"Cluster {idx}:\")\n",
    "    print(\" Centroid:\", c.get_position())\n",
    "    print(\" Variance:\", c.get_variance())\n",
    "    print(\" Points:\", c.points.shape[0])\n",
    "\n",
    "plot_tsne_clusters(data, assignments, title=\"t-SNE of Weighted Cosine K-Means\", perplexity=30)"
   ],
   "id": "f2f9885344ea050e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for cluster_index in range(16):\n",
    "    print(f\"Cluster {cluster_index}:\")\n",
    "    assign = [i for i, a in enumerate(assignments) if a == cluster_index]\n",
    "    for assign_index in assign:\n",
    "        print(ids[assign_index])"
   ],
   "id": "3407e0283f0ae39f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ids",
   "id": "d64f050e1ced4bfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d56d3beecb9ec811",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
