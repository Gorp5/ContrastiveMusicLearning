{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def cosine_distance(x, centroids, eps=1e-8):\\n\",\n",
    "    \"    # Normalize to unit vectors\\n\",\n",
    "    \"    x_norm = x / (x.norm(dim=1, keepdim=True) + eps)\\n\",\n",
    "    \"    c_norm = centroids / (centroids.norm(dim=1, keepdim=True) + eps)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute cosine similarity (dot product)\\n\",\n",
    "    \"    sim = torch.mm(x_norm, c_norm.t())  # [N, K]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert to distance (1 - similarity)\\n\",\n",
    "    \"    distances = 1.0 - sim\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return distances\\n\",\n",
    "    \"\\n\",\n",
    "    \"def weighted_cosine_distance(x, centroids, weights=None, eps=1e-8):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Compute cosine distance between points and centroids with per-dimension weights.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        x (torch.Tensor): Data points [N, D]\\n\",\n",
    "    \"        centroids (torch.Tensor): Cluster centers [K, D]\\n\",\n",
    "    \"        weights (torch.Tensor): Feature weights [D], larger = more important\\n\",\n",
    "    \"        eps (float): Small constant for numerical stability\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        distances (torch.Tensor): [N, K] cosine distance (1 - weighted cosine similarity)\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Normalize weights so they’re non-negative and scale appropriately\\n\",\n",
    "    \"    weights = weights.to(x.device)\\n\",\n",
    "    \"    weights = weights / (weights.norm() + eps)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Apply weights\\n\",\n",
    "    \"    xw = x * weights\\n\",\n",
    "    \"    cw = centroids * weights\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Normalize weighted vectors to unit length\\n\",\n",
    "    \"    xw_norm = xw / (xw.norm(dim=1, keepdim=True) + eps)\\n\",\n",
    "    \"    cw_norm = cw / (cw.norm(dim=1, keepdim=True) + eps)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute weighted cosine similarity\\n\",\n",
    "    \"    sim = torch.mm(xw_norm, cw_norm.t())  # [N, K]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert to distance\\n\",\n",
    "    \"    distances = 1.0 - sim\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return distances\\n\",\n",
    "    \"\\n\",\n",
    "    \"# === Weighted cosine distance per cluster ===\\n\",\n",
    "    \"def cluster_weighted_cosine_distance(x, centroids, weights, eps=1e-8):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Weighted cosine distance where each cluster has its own weight vector.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        x (torch.Tensor): Data points [N, D]\\n\",\n",
    "    \"        centroids (torch.Tensor): Cluster centers [K, D]\\n\",\n",
    "    \"        weights (torch.Tensor): Per-cluster weights [K, D]\\n\",\n",
    "    \"        eps (float): Numerical stability\\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        distances [N, K]\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    N, D = x.shape\\n\",\n",
    "    \"    K = centroids.shape[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Normalize both x and centroids per cluster (with weights)\\n\",\n",
    "    \"    x_exp = x.unsqueeze(1).expand(N, K, D)       # [N, K, D]\\n\",\n",
    "    \"    c_exp = centroids.unsqueeze(0).expand(N, K, D)  # [N, K, D]\\n\",\n",
    "    \"    w_exp = weights.unsqueeze(0).expand(N, K, D)    # [N, K, D]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Apply weights before cosine similarity\\n\",\n",
    "    \"    x_w = x_exp * w_exp\\n\",\n",
    "    \"    c_w = c_exp * w_exp\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute cosine similarity\\n\",\n",
    "    \"    dot = (x_w * c_w).sum(dim=-1)\\n\",\n",
    "    \"    x_norm = x_w.norm(dim=-1) + eps\\n\",\n",
    "    \"    c_norm = c_w.norm(dim=-1) + eps\\n\",\n",
    "    \"    sim = dot / (x_norm * c_norm)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    distances = 1.0 - sim  # cosine distance\\n\",\n",
    "    \"    return distances\"\n",
    "   ],\n",
    "   \"id\": \"46f559a8ce0b7f5c\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"\\n\",\n",
    "    \"# === Basic K-Means using custom distance ===\\n\",\n",
    "    \"def kmeans(x, num_clusters, distance_fn, num_iters=100, tol=1e-4, verbose=False):\\n\",\n",
    "    \"    device = x.device\\n\",\n",
    "    \"    n, d = x.shape\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Random initialization of centroids\\n\",\n",
    "    \"    indices = torch.randperm(n, device=device)[:num_clusters]\\n\",\n",
    "    \"    centroids = x[indices]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for i in range(num_iters):\\n\",\n",
    "    \"        distances = distance_fn(x, centroids)\\n\",\n",
    "    \"        cluster_assignments = torch.argmin(distances, dim=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        new_centroids = torch.zeros_like(centroids)\\n\",\n",
    "    \"        for k in range(num_clusters):\\n\",\n",
    "    \"            mask = cluster_assignments == k\\n\",\n",
    "    \"            if mask.any():\\n\",\n",
    "    \"                new_centroids[k] = x[mask].mean(dim=0)\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                new_centroids[k] = x[torch.randint(0, n, (1,), device=device)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        shift = torch.norm(centroids - new_centroids, dim=1).sum()\\n\",\n",
    "    \"        if verbose:\\n\",\n",
    "    \"            print(f\\\"Iteration {i+1}/{num_iters} | centroid shift = {shift.item():.6f}\\\")\\n\",\n",
    "    \"        if shift < tol:\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"        centroids = new_centroids\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return cluster_assignments, centroids\"\n",
    "   ],\n",
    "   \"id\": \"initial_id\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# === Compute per-cluster per-dimension variance ===\\n\",\n",
    "    \"def cluster_variances(x, cluster_assignments, num_clusters):\\n\",\n",
    "    \"    n, d = x.shape\\n\",\n",
    "    \"    device = x.device\\n\",\n",
    "    \"    variances = torch.zeros(num_clusters, d, device=device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for k in range(num_clusters):\\n\",\n",
    "    \"        mask = (cluster_assignments == k)\\n\",\n",
    "    \"        if mask.any():\\n\",\n",
    "    \"            cluster_points = x[mask]\\n\",\n",
    "    \"            variances[k] = cluster_points.var(dim=0, unbiased=False)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            variances[k] = torch.ones(d, device=device)  # fallback\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Step 2: mean variance across clusters per dimension\\n\",\n",
    "    \"    mean_variance = variances.mean(dim=0)  # [D]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Step 3: deviation of each cluster variance from mean variance\\n\",\n",
    "    \"    deviations = torch.abs(variances - mean_variance)  # [K, D]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return variances\"\n",
    "   ],\n",
    "   \"id\": \"ef484c954d66fdd5\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def weighted_kmeans(x, num_clusters, weights, centroids=None, distance_fn=cluster_weighted_cosine_distance, max_iters=100, tol=1e-4, verbose=True):\\n\",\n",
    "    \"    device = x.device\\n\",\n",
    "    \"    N, D = x.shape\\n\",\n",
    "    \"    K = num_clusters\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Initialize centroids randomly\\\\\\n\",\n",
    "    \"    if centroids is None:\\n\",\n",
    "    \"        indices = torch.randperm(N, device=device)[:K]\\n\",\n",
    "    \"        centroids = x[indices].clone()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    prev_assignments = None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for iteration in range(max_iters):\\n\",\n",
    "    \"        # Compute weighted cosine distances\\n\",\n",
    "    \"        distances = distance_fn(x, centroids, weights=weights)\\n\",\n",
    "    \"        assignments = distances.argmin(dim=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Check for convergence\\n\",\n",
    "    \"        if prev_assignments is not None and torch.equal(assignments, prev_assignments):\\n\",\n",
    "    \"            if verbose:\\n\",\n",
    "    \"                print(f\\\"Converged at iteration {iteration}\\\")\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"        prev_assignments = assignments.clone()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Update centroids\\n\",\n",
    "    \"        new_centroids = torch.zeros_like(centroids)\\n\",\n",
    "    \"        for k in range(K):\\n\",\n",
    "    \"            mask = assignments == k\\n\",\n",
    "    \"            if mask.any():\\n\",\n",
    "    \"                new_centroids[k] = x[mask].mean(dim=0)\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                new_centroids[k] = x[torch.randint(0, N, (1,), device=device)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        shift = torch.norm(centroids - new_centroids, dim=1).sum()\\n\",\n",
    "    \"        if verbose:\\n\",\n",
    "    \"            print(f\\\"Iteration {iteration+1}/{max_iters} | centroid shift = {shift.item():.6f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if shift < tol:\\n\",\n",
    "    \"            if iteration > 2:\\n\",\n",
    "    \"                shift = 1\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"\\n\",\n",
    "    \"        centroids = new_centroids\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return assignments, centroids, shift\\n\",\n",
    "    \"\\n\",\n",
    "    \"# === Adaptive Weighted K-Means loop ===\\n\",\n",
    "    \"def weighted_cosine_kmeans(x, num_clusters, distance_fn=cluster_weighted_cosine_distance, max_iters_inner=128, max_iters_outer=8, tol=1e-4, verbose=True, rate=1e-1):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Runs iterative K-Means with weighted cosine distance and\\n\",\n",
    "    \"    moving-average variance-based weight updates.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Compute new variance-based weights\\n\",\n",
    "    \"    weights = torch.ones((num_clusters, x.shape[-1]), device=x.device)\\n\",\n",
    "    \"    centroids = None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for iteration in range(max_iters_outer):\\n\",\n",
    "    \"        assignments, centroids, shift = weighted_kmeans(x, num_clusters, weights, centroids=centroids, distance_fn=distance_fn, max_iters=max_iters_inner, tol=tol, verbose=verbose)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if shift < 1e-8:\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"\\n\",\n",
    "    \"        variances = cluster_variances(x, assignments, num_clusters)\\n\",\n",
    "    \"        new_weights = 1 / (variances + 1e-8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Normalize weights per cluster (optional but stabilizing)\\n\",\n",
    "    \"        row_max, _ = new_weights.max(dim=1, keepdim=True)\\n\",\n",
    "    \"        row_max = row_max + 1e-8\\n\",\n",
    "    \"        new_weights = new_weights / row_max\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # EMA update for stability\\n\",\n",
    "    \"        difference = weights - new_weights\\n\",\n",
    "    \"        weights = weights - rate * difference\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if verbose:\\n\",\n",
    "    \"            avg_change = torch.abs(weights - new_weights).mean().item()\\n\",\n",
    "    \"            print(f\\\"Iter {iteration:02d} | Avg weight Δ: {avg_change:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Convergence by weight change\\n\",\n",
    "    \"        if avg_change < tol:\\n\",\n",
    "    \"            if verbose:\\n\",\n",
    "    \"                print(f\\\"Weight changes below tol ({tol}); stopping.\\\")\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return assignments, centroids, weights\"\n",
    "   ],\n",
    "   \"id\": \"247275e9031c4de6\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"\\n\",\n",
    "    \"def plot_tsne_clusters(data, assignments, title=\\\"t-SNE Clusters\\\", perplexity=30, random_state=42):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Generates a 2D t-SNE plot of high-dimensional data colored by cluster assignments.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        data (torch.Tensor): [N, D] input data\\n\",\n",
    "    \"        assignments (torch.Tensor or np.array): [N] cluster labels\\n\",\n",
    "    \"        title (str): Plot title\\n\",\n",
    "    \"        perplexity (float): t-SNE perplexity\\n\",\n",
    "    \"        random_state (int): Random seed for reproducibility\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Move to CPU and convert to numpy\\n\",\n",
    "    \"    data_np = data.cpu().numpy()\\n\",\n",
    "    \"    labels_np = assignments.cpu().long().numpy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute t-SNE embedding\\n\",\n",
    "    \"    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\\n\",\n",
    "    \"    data_2d = tsne.fit_transform(data_np)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Scatter plot\\n\",\n",
    "    \"    plt.figure(figsize=(8,6))\\n\",\n",
    "    \"    scatter = plt.scatter(data_2d[:,0], data_2d[:,1], c=labels_np, cmap=\\\"tab10\\\", alpha=0.7)\\n\",\n",
    "    \"    plt.title(title)\\n\",\n",
    "    \"    plt.xlabel(\\\"t-SNE 1\\\")\\n\",\n",
    "    \"    plt.ylabel(\\\"t-SNE 2\\\")\\n\",\n",
    "    \"    plt.colorbar(scatter, label=\\\"Cluster\\\")\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ],\n",
    "   \"id\": \"9bd2660c44501e97\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def euclidean_distance(clusters, eps=1e-8):\\n\",\n",
    "    \"    x = [x.get_position for x in clusters]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    diff = x.unsqueeze(1) - x.unsqueeze(0)\\n\",\n",
    "    \"    distances = torch.sqrt((diff ** 2).sum(dim=2) + eps)  # [N, K]\\n\",\n",
    "    \"    return distances\\n\",\n",
    "    \"\\n\",\n",
    "    \"def cosine_distance(x, y=None, eps=1e-8):\\n\",\n",
    "    \"    if y is None:\\n\",\n",
    "    \"        y = x\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Normalize both tensors along feature dimension\\n\",\n",
    "    \"    x_norm = x / (x.norm(dim=1, keepdim=True) + eps)\\n\",\n",
    "    \"    y_norm = y / (y.norm(dim=1, keepdim=True) + eps)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Cosine similarity matrix\\n\",\n",
    "    \"    sim = torch.mm(x_norm, y_norm.T)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert to distance (1 - cosine similarity)\\n\",\n",
    "    \"    dist = 1.0 - sim\\n\",\n",
    "    \"    return dist\\n\",\n",
    "    \"\\n\",\n",
    "    \"def variance_increase_distance(clusters, eps=1e-8):\\n\",\n",
    "    \"    N = len(clusters)\\n\",\n",
    "    \"    device = clusters[0].points.device\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Precompute stats\\n\",\n",
    "    \"    means = torch.stack([c.points.mean(dim=0) for c in clusters])  # [N, D]\\n\",\n",
    "    \"    variances = torch.stack([c.points.var(dim=0, unbiased=False).mean() for c in clusters])  # [N]\\n\",\n",
    "    \"    sizes = torch.tensor([c.points.shape[0] for c in clusters], dtype=torch.float32, device=device)  # [N]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    delta_var = torch.zeros((N, N), device=device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for i in range(N):\\n\",\n",
    "    \"        n_i = sizes[i]\\n\",\n",
    "    \"        μ_i = means[i]\\n\",\n",
    "    \"        var_i = variances[i]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # j > i\\n\",\n",
    "    \"        j_indices = torch.arange(i + 1, N, device=device)\\n\",\n",
    "    \"        n_j = sizes[j_indices]                      # [M]\\n\",\n",
    "    \"        μ_j = means[j_indices]                      # [M, D]\\n\",\n",
    "    \"        var_j = variances[j_indices]                # [M]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        n_total = n_i + n_j                          # [M]\\n\",\n",
    "    \"        # Weighted within-cluster variance\\n\",\n",
    "    \"        var_within = (n_i * var_i + n_j * var_j) / n_total  # [M]\\n\",\n",
    "    \"        # Between-cluster variance\\n\",\n",
    "    \"        var_between = (n_i * n_j / (n_total ** 2)) * ((μ_i - μ_j).pow(2).sum(dim=1))  # [M]\\n\",\n",
    "    \"        # Total merged variance\\n\",\n",
    "    \"        merged_var = var_within + var_between\\n\",\n",
    "    \"        # Δ variance relative to average\\n\",\n",
    "    \"        delta = merged_var - 0.5 * (var_i + var_j)  # [M]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        delta_var[i, j_indices] = delta\\n\",\n",
    "    \"        delta_var[j_indices, i] = delta  # symmetric\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return delta_var\\n\",\n",
    "    \"\\n\",\n",
    "    \"def variance_increase_cosine_distance(clusters, eps=1e-8):\\n\",\n",
    "    \"    sizes = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"    sizes = torch.tensor([c.shape[0] for c in clusters], dtype=torch.float32, device=clusters.device)  # [K]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Normalize centroids for cosine similarity ---\\n\",\n",
    "    \"    means_norm = clusters / (clusters.norm(dim=1, keepdim=True) + eps)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Compute pairwise cosine similarity ---\\n\",\n",
    "    \"    sim = torch.mm(means_norm, means_norm.T).clamp(-1.0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Convert similarity to distance ---\\n\",\n",
    "    \"    cosine_dist = 1.0 - sim  # [K, K]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Compute Ward-like Δ variance (weighted by cluster sizes) ---\\n\",\n",
    "    \"    n_i = sizes.view(-1, 1)\\n\",\n",
    "    \"    n_j = sizes.view(1, -1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    delta_var = (n_i * n_j) / (n_i + n_j + eps) * cosine_dist\\n\",\n",
    "    \"    delta_var.fill_diagonal_(0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return delta_var\"\n",
    "   ],\n",
    "   \"id\": \"940e26616307d5a\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"\\n\",\n",
    "    \"class Cluster:\\n\",\n",
    "    \"    def __init__(self, points: torch.Tensor, indices=None):\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        points: [N, D] tensor of member points\\n\",\n",
    "    \"        indices: optional list of original indices\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        self.points = points\\n\",\n",
    "    \"        self.indices = indices if indices is not None else list(range(points.shape[0]))\\n\",\n",
    "    \"        self.update_stats()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def update_stats(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"Recompute centroid and variance.\\\"\\\"\\\"\\n\",\n",
    "    \"        self.centroid = self.points.mean(dim=0)\\n\",\n",
    "    \"        self.variance = self.points.var(dim=0, unbiased=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def get_position(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"Return centroid (for distance computations).\\\"\\\"\\\"\\n\",\n",
    "    \"        return self.centroid\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def get_variance(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"Return per-dimension variance.\\\"\\\"\\\"\\n\",\n",
    "    \"        return self.variance\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def merge(self, other):\\n\",\n",
    "    \"        \\\"\\\"\\\"Merge this cluster with another cluster or single point tensor.\\\"\\\"\\\"\\n\",\n",
    "    \"        if isinstance(other, Cluster):\\n\",\n",
    "    \"            new_points = torch.cat([self.points, other.points], dim=0)\\n\",\n",
    "    \"            new_indices = self.indices + other.indices\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            # Assume `other` is a single point tensor [D]\\n\",\n",
    "    \"            new_points = torch.cat([self.points, other.unsqueeze(0)], dim=0)\\n\",\n",
    "    \"            new_indices = self.indices + [-1]  # placeholder\\n\",\n",
    "    \"        return Cluster(new_points, new_indices)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def hierarchical_clustering(data, K, distance_fn):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Custom hierarchical clustering with distance_fn.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        data: [N, D] tensor\\n\",\n",
    "    \"        K: target number of clusters\\n\",\n",
    "    \"        distance_fn: callable (a, b) -> distance scalar\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    N, D = data.shape\\n\",\n",
    "    \"    clusters = [Cluster(data[i].unsqueeze(0), [i]) for i in range(N)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Iteratively merge until K remain\\n\",\n",
    "    \"    pbar = tqdm(total=N - K)\\n\",\n",
    "    \"    while len(clusters) > K:\\n\",\n",
    "    \"        # Collect all current centroids\\n\",\n",
    "    \"        centroids = torch.stack([c.get_position() for c in clusters])  # [M, D]\\n\",\n",
    "    \"        M = centroids.shape[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Compute full pairwise distance matrix\\n\",\n",
    "    \"        dist_matrix = distance_fn(clusters)  # [M, M]\\n\",\n",
    "    \"        dist_matrix = dist_matrix + torch.eye(M, device=dist_matrix.device) * 1e9  # mask self-distances\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Find minimum distance pair (single argmin over upper triangle)\\n\",\n",
    "    \"        min_idx = torch.argmin(dist_matrix)\\n\",\n",
    "    \"        i, j = divmod(min_idx.item(), M)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Merge those two clusters\\n\",\n",
    "    \"        new_cluster = clusters[i].merge(clusters[j])\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Remove the old ones and append the new one\\n\",\n",
    "    \"        new_clusters = []\\n\",\n",
    "    \"        for idx, c in enumerate(clusters):\\n\",\n",
    "    \"            if idx not in (i, j):\\n\",\n",
    "    \"                new_clusters.append(c)\\n\",\n",
    "    \"        new_clusters.append(new_cluster)\\n\",\n",
    "    \"        clusters = new_clusters\\n\",\n",
    "    \"\\n\",\n",
    "    \"        pbar.update(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return clusters\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_cluster_assignments(clusters, n_points):\\n\",\n",
    "    \"    assignments = torch.full((n_points,), -1, dtype=torch.long)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for cluster_idx, cluster in enumerate(clusters):\\n\",\n",
    "    \"        for point_idx in cluster.indices:\\n\",\n",
    "    \"            if point_idx >= 0:  # ignore synthetic/placeholder indices (e.g. -1)\\n\",\n",
    "    \"                assignments[point_idx] = cluster_idx\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return assignments\"\n",
    "   ],\n",
    "   \"id\": \"97d1be9bdcc7e9b2\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"inputfile = \\\"E:\\\\Coding\\\\SongAnalyzer\\\\Analyzer\\\\src\\\\output_analysis\\\\output-Myna-CLS-ALIBI-Chunking-256.csv\\\"\\n\",\n",
    "    \"device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"ids = []\\n\",\n",
    "    \"data = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(inputfile, \\\"r\\\", encoding='utf-8') as f:\\n\",\n",
    "    \"    for line in f:\\n\",\n",
    "    \"        parts = line.split(\\\" \\\")\\n\",\n",
    "    \"        numbers = [float(x) for x in parts[:128]]\\n\",\n",
    "    \"        id = \\\" \\\".join(parts[128:])\\n\",\n",
    "    \"        data.append(torch.tensor(numbers, device=device))\\n\",\n",
    "    \"        ids.append(id)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate synthetic data\\n\",\n",
    "    \"# data = []\\n\",\n",
    "    \"# num_clusters = 32\\n\",\n",
    "    \"# nums = [x * 2 + 64 for x in range(num_clusters)]\\n\",\n",
    "    \"#\\n\",\n",
    "    \"# for i in range(num_clusters):\\n\",\n",
    "    \"#     x = torch.randn(nums[i], 32, device=device) * torch.randn(32, device=device)\\n\",\n",
    "    \"#     data.append(x)\"\n",
    "   ],\n",
    "   \"id\": \"43fe80a64ce251cf\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"Hi Brad,\\n\",\n",
    "    \"I’ve been following the MIR/representation-learning work from people now at Suno — especially [paper], which aligns closely with a project I built: transformer-based audio embeddings + a contrastive VAE for music similarity. Demo/code: [github.com/Gorp5/ContrastiveMusicLearning]. Write-up with method/results (WIP PDF): [PDF link].\\n\",\n",
    "    \"Would love to contribute to Suno’s ML/MIR engineering efforts.\"\n",
    "   ],\n",
    "   \"id\": \"f221cbe8db456254\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"torch.manual_seed(0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = torch.stack(data, dim=0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"clusters = hierarchical_clustering(data, K=256, distance_fn=variance_increase_distance)\\n\",\n",
    "    \"assignments = get_cluster_assignments(clusters, data.shape[0])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display results\\n\",\n",
    "    \"for idx, c in enumerate(clusters):\\n\",\n",
    "    \"    print(f\\\"Cluster {idx}:\\\")\\n\",\n",
    "    \"    print(\\\" Centroid:\\\", c.get_position())\\n\",\n",
    "    \"    print(\\\" Variance:\\\", c.get_variance())\\n\",\n",
    "    \"    print(\\\" Points:\\\", c.points.shape[0])\\n\",\n",
    "    \"\\n\",\n",
    "    \"plot_tsne_clusters(data, assignments, title=\\\"t-SNE of Weighted Cosine K-Means\\\", perplexity=30)\"\n",
    "   ],\n",
    "   \"id\": \"f2f9885344ea050e\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"for cluster_index in range(16):\\n\",\n",
    "    \"    print(f\\\"Cluster {cluster_index}:\\\")\\n\",\n",
    "    \"    assign = [i for i, a in enumerate(assignments) if a == cluster_index]\\n\",\n",
    "    \"    for assign_index in assign:\\n\",\n",
    "    \"        print(ids[assign_index])\"\n",
    "   ],\n",
    "   \"id\": \"3407e0283f0ae39f\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"ids\",\n",
    "   \"id\": \"d64f050e1ced4bfa\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"\",\n",
    "   \"id\": \"d56d3beecb9ec811\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "f0c4382d4362cfbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
