{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data & Training",
   "id": "ebf89474ab777d38"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "source": [
    "from Analyzer.Webscraper.AudioCNNTransformerReverse import AudioTransformerReverse\n",
    "from Analyzer.Webscraper.AudioConformer import AudioConformer\n",
    "from Analyzer.Webscraper.AudioTransformerCNNReconstruction import AudioTransformerCNNReconstruction\n",
    "from Analyzer.Webscraper.AudioTransformerDeepCNN import AudioTransformerDeepCNN\n",
    "from Analyzer.Webscraper.AudioTransformerDeeperCNN import AudioTransformerDeeperCNN\n",
    "from Analyzer.Webscraper.AudioTransformerSingleLinearEncoderOnly import AudioTransformerSingleLinearEncoderOnly\n",
    "from Analyzer.Webscraper.AutioTransformerSingleLinearReconstruction import AudioTransformerSingleLinearReconstruction\n",
    "\n",
    "\n",
    "def parse_dataset(length=256):\n",
    "    directory = \"\"#\"latents\\\\\" #\"reconstruction_test_latents\\\\\" #mtg-jamendo\n",
    "    num_per = 1000\n",
    "    count = 1\n",
    "\n",
    "    for start in tqdm(range(0, 16150, num_per)):\n",
    "        mtg_dataset = retrieve_data(\"E:\\SongsDataset\\\\mtg-jamendo\\\\\", directory, start=start, count=num_per, sample_length=length)\n",
    "        torch.save(mtg_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}.pt\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    for start in tqdm(range(0, 3975, num_per)):\n",
    "        spotify_dataset = retrieve_data(\"E:\\SongsDataset\\\\latents\\\\\", directory, start=start, count=num_per, sample_length=length)\n",
    "        torch.save(spotify_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{count}.pt\")\n",
    "        count += 1\n",
    "\n",
    "    full_dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset1.pt\")\n",
    "    for start in tqdm(range(2, 21)):\n",
    "        new_data = torch.load(f\"E:\\\\SongsDataset\\\\length_{length}\\\\dataset{start}.pt\")\n",
    "        full_dataset = torch.cat((full_dataset, new_data))\n",
    "\n",
    "    torch.save(full_dataset, f\"E:\\\\SongsDataset\\\\length_{length}\\\\full_dataset.pt\")"
   ],
   "id": "b8e1130f3fe4d3cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parse_dataset(256)",
   "id": "5c59e7733cec7e6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T05:44:33.404418Z",
     "start_time": "2025-05-06T05:44:24.820588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from Training import train\n",
    "from Data import AudioDataset, retrieve_data\n",
    "from Analyzer.Webscraper.Training import evaluate"
   ],
   "id": "6afff8a0a232dd33",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T05:44:33.420419Z",
     "start_time": "2025-05-06T05:44:33.410419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==== Model & Optimizer ====\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "d_model = 256\n",
    "latent_space = 512\n",
    "dim_feedforward = 1024\n",
    "sample_length = 1024\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "device = \"cuda\""
   ],
   "id": "1112734e300ad7bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T05:46:08.878515Z",
     "start_time": "2025-05-06T05:44:33.896384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#full_dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\full_dataset.pt\")\n",
    "full_dataset = torch.load(f\"E:\\\\SongsDataset\\\\length_{sample_length}\\\\dataset1.pt\")"
   ],
   "id": "932e5667035a22a1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T05:46:09.354562Z",
     "start_time": "2025-05-06T05:46:09.307800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "num_samples, seq_length, embed_dim = full_dataset.shape\n",
    "\n",
    "train_len = int(len(full_dataset) * 0.9)\n",
    "train_set, test_set = random_split(full_dataset, [train_len, len(full_dataset) - train_len])\n",
    "\n",
    "train_dataset = AudioDataset(train_set)\n",
    "test_dataset = AudioDataset(test_set)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "d91d65a8dceff284",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:41:10.389045Z",
     "start_time": "2025-05-06T00:41:05.704118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Analyzer.Webscraper.Loss import combined_loss\n",
    "from AudioTransformerDeepCNN import AudioTransformerDeepCNN\n",
    "\n",
    "model = AudioTransformerDeepCNN(d_model=d_model, num_heads=num_heads, transformer_layers=num_layers, dim_feedforward=dim_feedforward, latent_space=latent_space, length=sample_length, dropout=0.1, name_extension=\"-fft-cos-only_RoPE\", use_rope=True, use_alibi=False)"
   ],
   "id": "c2dbff9d4271f59c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288455104\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ],
   "id": "b838be6a44280c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:41:13.760882Z",
     "start_time": "2025-05-06T00:41:10.493802Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0)"
   ],
   "id": "fb94e024564875f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, device=device, loss_func=combined_loss)\n",
    "evaluate(model, test_dataloader)"
   ],
   "id": "2344619c52ef2185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Output Stuff",
   "id": "be18100accf986cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "reconstruction_examples = retrieve_data(\"E:\\SongsDataset\\\\\",  \"reconstruction_test_latents\\\\\", sample_length=256)"
   ],
   "id": "cf4253aa15817256"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "song_set, _ = random_split(reconstruction_examples, [len(reconstruction_examples), 0])\n",
    "song_dataset = AudioDataset(song_set)\n",
    "song_dataloader = DataLoader(song_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "fbc2621ca2ea37d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\n",
    "    f\"AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft-cos\\\\-Epoch-14.pt\",\n",
    "        weights_only=False)\n",
    "evaluate(model, song_dataset)"
   ],
   "id": "7a8ae7c9d6b12217",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "455f9eaa3c09be8b",
   "metadata": {},
   "source": [
    "from Loss import combined_loss\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "new_song = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(song_dataloader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed = model(batch)\n",
    "\n",
    "        new_song.extend(reconstructed.to(\"cpu\"))\n",
    "\n",
    "        loss = combined_loss(reconstructed, batch)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "l = np.array(np.stack(new_song)).reshape(64, -1)\n",
    "np.save(\"reconstructed_song-2D-256-Campfire.npy\", l)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from Loss import combined_loss\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "latent_space = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(song_dataloader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        latent = model.to_latent(batch)\n",
    "\n",
    "        latent_space.extend(latent.to(\"cpu\"))\n",
    "\n",
    "        num_batches += 1"
   ],
   "id": "500f58f1e962f7d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(new_song))\n",
    "print(len(new_song[0]))\n",
    "print(len(new_song[0][0]))"
   ],
   "id": "b136dab70b1d939c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99e97d55b0fe4e2b",
   "metadata": {},
   "source": [
    "import IPython\n",
    "import torch\n",
    "from music2latent import EncoderDecoder\n",
    "import numpy as np\n",
    "\n",
    "encdec = EncoderDecoder()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "l = np.load(\"reconstructed_song-2D-256-Banana.npy\")",
   "id": "c551566086e9ad0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compressed_song = np.load(\"reconstructed_song-256-FFT.npy\")\n",
    "wv_rec = encdec.decode(l)"
   ],
   "id": "37adcbe9ec1ff8a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IPython.display.display(IPython.display.Audio(wv_rec, rate=44100))",
   "id": "3bf378f8f7bd1e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IPython.display.display(IPython.display.Audio(wv_rec, rate=44100))",
   "id": "829e14203c6c68c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:00:00.329815Z",
     "start_time": "2025-05-06T03:59:57.564398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from Training import train\n",
    "from Data import AudioDataset, retrieve_data\n",
    "from Analyzer.Webscraper.Training import evaluate"
   ],
   "id": "ea5e81db5410c8c0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Output",
   "id": "fb377fc03ba185eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T03:49:31.334169Z",
     "start_time": "2025-05-06T03:49:20.359933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models/AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft/-Epoch-10.pt\", weights_only=False)\n",
    "model.to(\"cuda\")\n",
    "model.eval()  # Set model to evaluation mode"
   ],
   "id": "347ec462d939c27a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioTransformerDeepCNN(\n",
       "  (projection): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (projection_gelu): GELU(approximate='none')\n",
       "  (encoder): RoPEALiBiTransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x RoPEALiBiTransformerEncoderLayer(\n",
       "        (self_attn): RoPEALiBiMultiheadAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv_shrink_1): Sequential(\n",
       "    (0): Conv1d(256, 64, kernel_size=(11,), stride=(4,), padding=(5,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): AvgPool1d(kernel_size=(11,), stride=(2,), padding=(5,))\n",
       "  )\n",
       "  (conv_shrink_2): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "  )\n",
       "  (conv_shrink_3): Sequential(\n",
       "    (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       "  (conv_shrink_4): Sequential(\n",
       "    (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       "  (conv_shrink_5): Sequential(\n",
       "    (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): AvgPool1d(kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "  )\n",
       "  (latent_skip_in): Linear(in_features=65536, out_features=2048, bias=True)\n",
       "  (to_latent1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (to_latent1_gelu): GELU(approximate='none')\n",
       "  (from_latent1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (from_latent1_gelu): GELU(approximate='none')\n",
       "  (latent_skip_out): Linear(in_features=2048, out_features=65536, bias=True)\n",
       "  (conv_grow_1): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ConvTranspose1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (conv_grow_2): Sequential(\n",
       "    (0): GELU(approximate='none')\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ConvTranspose1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (conv_grow_3): Sequential(\n",
       "    (0): GELU(approximate='none')\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (conv_grow_4): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ConvTranspose1d(128, 64, kernel_size=(5,), stride=(2,), padding=(2,), output_padding=(1,))\n",
       "  )\n",
       "  (conv_grow_5): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ConvTranspose1d(64, 256, kernel_size=(11,), stride=(4,), padding=(4,), output_padding=(1,))\n",
       "  )\n",
       "  (decoder): RoPEALiBiTransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x RoPEALiBiTransformerDecoderLayer(\n",
       "        (self_attn): RoPEALiBiMultiheadAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attn): RoPEALiBiMultiheadAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc_gelu): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T03:54:33.822595Z",
     "start_time": "2025-05-06T03:54:23.701451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "directory = \"\"\n",
    "spotify_dataset = retrieve_data(\"E:\\SongsDataset\\\\latents\\\\\", directory, sample_length=256, keep_song_data_option=True)"
   ],
   "id": "d74b57bfb1d69555",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Coding\\SongAnalyzer\\Analyzer\\Webscraper\\Data.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  dataset.append(torch.tensor(np.split(padded_data, padded_data.shape[0] / sample_length)))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 2\u001B[0m spotify_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mretrieve_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mE:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mSongsDataset\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mlatents\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_song_data_option\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\Webscraper\\Data.py:63\u001B[0m, in \u001B[0;36mretrieve_data\u001B[1;34m(path_stub, song_path_stub, start, count, sample_length, keep_song_data_option)\u001B[0m\n\u001B[0;32m     59\u001B[0m dataset_saved_path \u001B[38;5;241m=\u001B[39m path_stub \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msong-dataset.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Read and Memoize Training and Test Datasets\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# if not os.path.exists(dataset_saved_path):\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mread_data_from_folder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_stub\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msong_path_stub\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_song_data_option\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_song_data_option\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;66;03m# np.save(dataset_saved_path, dataset)\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# else:\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m#     dataset = torch.tensor(np.load(dataset_saved_path))\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# dataset = dataset.to(torch.float32)\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\Webscraper\\Data.py:45\u001B[0m, in \u001B[0;36mread_data_from_folder\u001B[1;34m(folder_stub, start, count, keep_song_data_option, sample_length)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m each_song \u001B[38;5;129;01min\u001B[39;00m all_folders:\n\u001B[0;32m     43\u001B[0m     path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(folder_stub, each_song)\n\u001B[1;32m---> 45\u001B[0m     padded_data \u001B[38;5;241m=\u001B[39m \u001B[43mchunk_song\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m keep_song_data_option:\n\u001B[0;32m     48\u001B[0m         dataset\u001B[38;5;241m.\u001B[39mextend(np\u001B[38;5;241m.\u001B[39msplit(padded_data, padded_data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m/\u001B[39m sample_length))\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\Analyzer\\Webscraper\\Data.py:24\u001B[0m, in \u001B[0;36mchunk_song\u001B[1;34m(path, sample_length)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mchunk_song\u001B[39m(path, sample_length):\n\u001B[1;32m---> 24\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m8192\u001B[39m:\n\u001B[0;32m     27\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Coding\\SongAnalyzer\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    403\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 405\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    406\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:03:05.786165Z",
     "start_time": "2025-05-06T04:03:05.774167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compress_song_average(song, model):\n",
    "    with torch.no_grad():\n",
    "        latent_space = model.to_latent(song)\n",
    "        #sum = sum + latent_space.to('cpu')\n",
    "        return torch.sum(latent_space.to('cpu'), dim=0) / len(song)"
   ],
   "id": "7b90b9c27234c4e8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:03:06.038797Z",
     "start_time": "2025-05-06T04:03:06.033798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Analyzer.Webscraper.Data import chunk_song\n",
    "import os\n",
    "\n",
    "def compute(model, name):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    path = \"E:\\SongsDataset\\\\latents\\\\\"\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    file = open(f\"output_analysis/output-{name}.csv\", \"w\", encoding='utf-8')\n",
    "\n",
    "    for each_song in tqdm(all_folders):\n",
    "        song_path = os.path.join(path, each_song)\n",
    "\n",
    "        padded_data = chunk_song(song_path, 256)\n",
    "        input_tensor = torch.Tensor(padded_data).reshape(-1, 256, 64).to(\"cuda\")\n",
    "\n",
    "        latent = compress_song_average(input_tensor, model)\n",
    "\n",
    "        output = \"\"\n",
    "        for value in latent:\n",
    "            output += f\"{str(value.item())} \"\n",
    "\n",
    "        file.write(output + f\"\\\"{each_song}\\\"\\n\")"
   ],
   "id": "11349c5b539020d6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T03:57:21.919293Z",
     "start_time": "2025-05-06T03:56:18.124592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models/AudioTransformerDeepCNN-LatentSpace512-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft/-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"CNN-FINAL\")"
   ],
   "id": "ebf06fb8b58faa6d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [01:01<00:00, 64.13it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:05:58.773214Z",
     "start_time": "2025-05-06T04:03:15.609260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torch.load(\"final-models/AudioTransformerSingleLinearReconstruction-LatentSpace512-Heads16-TrasformerLayers16-DModel256-Dropout0.1-fft/-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"Linear-FINAL\")"
   ],
   "id": "6acce80a99de4c67",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [02:41<00:00, 24.56it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.load(\"AudioTransformerCNNReconstruction-LatentSpace64-Heads8-TrasformerLayers8-DModel256-Dropout0.1-fft-cos\\\\-Epoch-10.pt\", weights_only=False)\n",
    "compute(model, \"CNN-FFT-COS\")"
   ],
   "id": "709dec0c65f8c805",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Analysis",
   "id": "b415f5779e1b4264"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
